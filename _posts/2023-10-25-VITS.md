---
title: VITS: Conditional Variational Autoencoder with Adversarial Learning for
End-to-End Text-to-Speech
date: 2023-10-25 16:24:00 +0300
description: VITS 논문 리딩
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [] # add tag
use_math: true
categories:
  - Paper Reading
sidebar:
    nav: "blog"
---


<aside>
🗣 **overall**
표현력 향상을 위해 conditional prior 분포, 적대적 학습 과정에 normalizing flow 추가
text에 대한 다양한 one-to-many 관계를 표현할 수 있는 stochastic duration predictor 사용
Latent 변수에 대한 uncertainty 모델링과 stochastic predictor를 통해 텍스트로는 표현되지 않는 음성 변화를 캡쳐

</aside>

<aside>
🗣 **process**
VITS는 Text encoder가 prior를 학습하고 Wave Encoder가 posterior를 학습
둘 사이의 변환을 normalizing flow를 통해 학습
이 때 prior과 posterior간에 sequence length가 안맞는 문제가 발생하는데, 이 때 Stochastic Duration Predictor와 Monotonic Alignment Search를 적용해 둘 사이의 Alignment를 학습
Waveform Generation은 HiFi-GAN 구조를 그대로 들고 와서 사

</aside>

# ***Abstract***

최근 여러 단일 스테이지 훈련과 parallel sampling을 가능하게 하는 end-to-end text-to-speech(TTS) 모델이 제안되어왔으나, 그들의 샘플 퀄리티는 2-stage TTS 시스템과 같지 않았음

- 이 논문에서는, 현재의 2-stage 모델보다 더 자연스러운 소리의 오디오를 생성하는 parallel end-to-end TTS 방법을 소개
- 해당 방법은 정규화 플로우를 도입한 변분 추론과 적대적 훈련 프로세스를 채택하여, 생성 모델링의 표현 능력을 향상시킴
- 또한 input text로부터 speech를 다양한 리듬과 합성하기 위해 stochastic duration predictor를 제안
- 잠재변수에 대한 불확실성 모델링과 확률적 길이 예측기(stochastic duration predictor)를 사용함으로써, 해당 방법은 자연스러운 one-to-many 관계를 표현, 텍스트 input이 다양한 음조(pitch)와 리듬으로 여러 방법으로 발음될 수 있음
    - one-to-many: 하나의 텍스트 인풋이 다양한 방식으로 발음될 수 있다는 것을 뜻함
- 주관적인 인간 평가(mean opinion score, or MOS)결과, 단일 화자 데이터셋인 LJ speech에서 해당 방법이 최고의 공개적으로 이용 가능한 텍스트 음성 합성 시스템을 능가하며 실제 음성과 유사한 MOS를 달성함

# ***1. Introduction***

TTS 시스템은 주어진 텍스트를 여러 컴포넌트를 사용해 raw speech waveform을 합성하는 것

심층신경망 dnn(deep neural networks)의 급속한 발전과 함께, TTS 파이프라인은 텍스트 전처리를 제외하고(텍스트 정규화 및 음소화 등) 이제 2단계 생성 모델링으로 단순화됨

- 첫 번째 단계는 전처리된 텍스트로부터 mel-spectrogram 혹은 linguistic features와 같은 중간 speech 표현을 만드는 것
- 두 번째 단계는 중간 표현에 따라 조정된 raw waveform을 생성하는 것
- 두 단계의 파이프라인 각각에는 모델이 독립적으로 개발됨

- 신경망 기반의 자귀회귀형 TTS 시스템은 실제같은 음성을 합성하는 능력을 보여주었으나, 순차적인 생성 프로세스 때문에 현대의 parallel(병렬) 프로세서를 완전히 활용하기가 어려움
- 이러한 한계를 극복하고 합성 속도를 높이기 위해, 여러 non-자기회귀적 방법들이 제안됨
- text-to-spectrogram 생성 단계에서, pre-train된 자귀회귀형 teacher 네트워크에서 attention maps를 추출하는 것이 시도됨, 이는 텍스트와 spectrogram 간의 learning alignments를 학습하는 어려움을 줄이기 위한 시도였음
- 더 최근에는, 확률(likelyhood) 기반의 방법들이 외부 aligner의 의존성을 더욱 줄이기 위해 사용됨
    - external aligner: 주로 두 시퀀스간의 대응 관계를 찾아내기 위한 별도의 도구나 알고리즘을 의미
    - 이러한 aligner을 사용하지 않고, 모델 자체에서 데이터의 대응 관계를 학습하려는 것이 likelihood-methods의 목표
    - 즉, 외부 도구를 사용하지 않고도 모델이 입력 텍스트와 출력 멜-스펙트로그램간의 대응 관계를 직접 학습할 수 있게 해줌
    - 이러한 방법들은 목표 mel-spectrogram의 확률을 최대화하기 위한 alignments를 추정하거나 학습
- 그러는동안, GAN이 2단계 모델에서 탐색됨
    - GAN-based feed-forward 네트워크는 다양한 판별자를 사용하여 각각 다른 스케일이나 주기에서 각 샘플을 구별하여 고품질의 raw waveform 합성을 달성

병렬 TTS 시스템의 발전에도 불구하고,  2단계 파이프라인에서 그들이 순차적인 학습 혹은 fine-tuning을 필요로 한다는 점에서 문제가 남아있음

- 특히나 고품질 제작 과정에서 2단계 모델들이 1단계 모델들에게서 생성한 샘플들로 학습되어야 하기 때문
- 추가적으로, 미리 정의된 중간 특성에 대한 의존성은 학습된 hidden 표현을 적용하여 성능을 더 개선하는 것을 불가능하게 함
    - 특정 모델이 미리 정의된 중간 특성을 사용하고 있어서, 추가적인 성능 향상을 위해 학습된 표현을 활용하는 것이 어려워진다는 의미

- 최근에는, 여러 연구(ex. FastSpeech 2s)와 EATS가 효율적인 end-to-end 훈련 방법을 제안
    - 전체 웨이브폼을 학습하는 것이 아닌 짧은 오디오 클립들을 기반으로 학습하기 (FastSpeech 2)
    - 멜-스펙트로그램 디코더를 text representation learning에 활용하기 (EATS)
    - 목표 스펙트로그램과 생성된 음성간의 길이 불일치를 완화하기 위한 특수한 스펙트로그램 loss 설계 (FastSpeech 2, EATS)
- 그러나, 학습된 표현을 활용하여 잠재적인 성능을 향상시킬 수 있음에도 불구하고, 합성 품질은 2단계 시스템보다 뒤쳐짐

이 논문에서는, 현재 2단계 모델들보다 더 자연스러운 오디오 사운드를 합성하는 parallel end-to-end TTS 방법을 사용

- variational autoencoder를 사용하여, TTS 시스템의 두 모듈을 잠재 변수를 통해 연결하여 효율적인 end-to-end 학습을 가능하게 함
- 표현력을 높여 고품질의 음성을 합성하기 위해, 조건부 사전 분포에 normalizing flow를 적용하고 waveform 도메인에서 적대적 훈련(adversarial training)을 진행
    - 특정 조건 하에서의 확률 분포를 변환하는 과정에서 정규화 흐름 기술을 활용한다는 의미, 이는 확률 분포를 더 복잡하게 모델링하고 다양한 형태의 데이터를 생성하는데 도움
- 세밀한 오디오를 생성하는 것 외에도, TTS 시스템에서는 텍스트 인풋을 다양한 변형(ex. pitch and duration)으로 여러 방식으로 말할 수 있는 일대다 관계를 표현하는 것이 중요
- 일대다 문제를 해결하기 위해, input 텍스트로부터 다양한 리듬으로 음성을 합성하기 위한 stochastic duration predictor 또한 제안
- 잠재 변수에 대한 불확실성 모델링과 stochastic duration predictor로, 이 방법은 텍스트로 표현될 수 없는 speech variation을 포착함

- 이 방법은 HiFi-GAN을 사용한 최고의 공개 사용 가능 TTS 시스템인 Glow-TTS보다 더 자연스러운 사운드의 음성과 높은 샘플링 효율성을 얻을 수 있음
- 데모 페이지와 소스코드를 공개적으로 사용 가능하도록 만들어두었음

<aside>
🗣 많은 TTS는 두 단계로 구성됨

- 첫 번째 단계에서는 중간 표현을 생성 (멜 스펙트로그램이나 linguistinc representations)
- 두 번째 단계에서 생성된 중간 표현을 이용해 raw-waveform을 생성

이러한 단계들을 좀 간단히 만들 수 없을까? 해서 다양한 시도가 있었으나 아직 2단계 모델보다는 완성도가 떨어짐

그러다가 VAE를 써서 두 모델을 연결하고 conditional prior distribution에 normalizing flow를 써보고 훈련을 adversarial하게 해보면 어떨까, 그리고 여기에 음성 리듬을 다양하게 하기 위한 stochastic duration predictor도 적용해보면 어떨까 하는 idea에서 나온 모델이 바로 VITS

</aside>

# 2. ***Method***

이 섹션에서는, 제안된 방법과 그 아키텍처를 설명

- 제안된 방법은 처음 3 서브 섹션에서 거의 설명됨:
    - 조건부 VAE formulation
    - variational inference로부터 기인한 alignment 추정
    - 합성 품질을 향상시키기 위한 adversarial training
- 전체적인 아키텍처는 이 섹션의 마지막 부분에 설명됨
- *Figure 1a*와 *1b*는 이 방법의 훈련과 추론 과정을 각각 보여줌
- 이제부터, 이 방법을 Variational Inference with adversarial learning for end-to-end Text-to-Speech라고 명명함 (VITS)

![*Figure 1*. 시스템 다이어그램 묘사(a) 훈련과정과 (b) 추론과정. 제안된 모델은 conditional VAE로 볼 수 있음; 사후 인코더, 디코더, 그리고 conditional 사전(초록 블록: normalizing flow, linear projection layer, and text encoder) with a flow-based stochastic duration predictor](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/17a0de51-cdab-4fc2-9478-4e6960d634bb/Untitled.png)

*Figure 1*. 시스템 다이어그램 묘사(a) 훈련과정과 (b) 추론과정. 제안된 모델은 conditional VAE로 볼 수 있음; 사후 인코더, 디코더, 그리고 conditional 사전(초록 블록: normalizing flow, linear projection layer, and text encoder) with a flow-based stochastic duration predictor

<aside>
🗣 VITS는 interactable marginal log-likelihood data log의 ELBO를 최대화하는 conditional VAE
Training loss는 negative ELBO
- Reconstruction loss와 KL-divergence의 합

</aside>

## ***2.1 Variational Inference***

### ***2.1.1. OVERVIEW***

VITS는 조건적 VAE라고 표현할 수 있음

- 이 때의 목적은 다루기 어려운 marginal log-likelihood(주변로그확률)의 variational lower bound, 즉 ELBO(evidence lower bound)를 최대화 하는 것
- $logp_\theta(x|c)$: 이는 주어진 조건 c에서의 데이터 c의 확률 로그 우도를 모델링하고 최대화하려는 것을 의미
- 즉 VITS는 marginal log-likelihood의 variational lower bound, 즉 evidence lower bound(ELBO)를 최대화하는 목적을 가진 conditinal VAE라고 설명될 수 있음
    - ELBO를 최대화함으로써 모델이 추론 가능한 범위를 축소하여 보다 정확한 예측을 하도록 도움

$$
(1)~~~logp_{\theta}(x|c)\geq E_{q_\phi (z|x)}[logp_\theta(x|z)-log\frac{q_\phi (z|x)}{p_\theta (z|c)}]
$$

- $p_\theta(z|c)$는 주어진 조건 $c$에 대한 잠재 변수 $z$의 사전 분포를 나타냄
- $p_\theta(x|z)$는 데이터 포인트 $x$에서의 우도 함수, 즉 주어진 z일 때 x가 발생할 확률을 나타냄
- $q_\phi(z|x)$는 근사 사후 분포, 주어진 x에 대한 z의 추정값

training loss는 negative ELBO이며, 이는 reconstruction loss -$logp_\theta(x|z)$와 KL divergence $logq_\phi(z|x)$-  $logp_\theta(z|c)$ 의 합으로 볼 수 있음

이 때 $z$는 $z$~$q_\phi(z|x)$에 따라 샘플링된 값 (주어진 입력 x가 있을 때, 모델은 근사 사후 분포 $q_\phi(z|x)$에서 z값을 샘플링함

이는 학습 목적함수를 나타내며, 이를 최소화하여 모델을 훈련

<aside>
🗣 VAE는 input과 output을 같게 만드는 것을 통해 의미있는 latent space를 만드는 autoencoder와 비슷
이 때 encoder를 통해 input으로부터 latent space를 도출하고, decoder를 통해 latent space로부터 output 생성

encoder: input x가 주어졌을 때, latent vector z의 분포, 즉 q(z|x)를 approximate하는 것을 목적으로 함
               * q(z|x)를 가장 잘 나타내는 분포로 정규 분포를 선택한다면 이 정규분포를 나타내는 평균과 표준편차를 찾는 것이 목적이 됨

decoder: latent vector z 가 주어졌을 때, x의 분포 p(x|z)를 approximate하는 것이 목적

Latent Space: autoencoder처럼 input과 output을 똑같이 만드는 것을 목적으로 하면 latent space는 항상 input과 같은 모양의 데이터만을 생성할 것
이를 방지하기 위해 noise를 sampling하여 이로부터 latent vector를 생성

- z = Mean + Varianace + noise
- ELBO: p(x)를 maximize하는 것이 목적, maximum likelihood 접근법을 택하여 전개
</aside>

### ***2.1.2 RECONSTRUCTION LOSS***

reconstruction loss에서 target 데이터 포인트는 raw waveform 대신 멜 스펙트로그램을 사용, $x_{mel}$

디코더를 사용해 잠재변수 $z$를 waveform 도메인 $\hat y$ 로 업샘플링하고 $\hat y$를 멜 스펙트로그램 도메인 $\hat x_{mel}$으로 변형

그러고나서 예측된 것과 타겟 멜-스펙트로그램 사이의 $L_1$ loss를 계산 ⇒ reconstruction loss

$$
L_{recon} = ||x_{mel}-\hat x_{mel}||_1
$$

데이터 분포를 Laplace 분포로 가정하고, 상수항들을 무시한 상태에서 최대 우도 추정을 수행

- 모델 학습을 단순화하고 수학적으로 다루기 쉽게 만들기 위함

인간 청각 시스템의 반응에 근접 멜 스케일을 사용해 지각 품질을 개선시키기 위해 멜-스펙트로그램 도메인에서 reconstruction loss를 정의

raw waveform의 멜 스펙트로그램 추정은 학습 가능한 parameters를 요구하지 않음. 오직 STFT와 mel-scale에 대한 linear projection만 사용

또한 이 estimation은 훈련중에만 사용되고 추론에는 사용되지 않음

실제로 디코더의 input으로 전체 잠재 변수 $z$를 사용하지 않고 부분적인 시퀀스를 사용, 이는 효율적인 end-to-end 학습에 사용되는 windowed generator 학습임

<aside>
🗣 Reconstruction loss를 사용하는 이유는 perceptual 품질을 개선시키기 위함

따라서 멜 스펙트로그램을 타겟으로 사용, 잠재변수를 업샘플링하고 해당 waveform을 멜 스펙트로그램 도메인으로 다시 변형, 이 아웃풋과 인풋 멜 스펙트로그램 사이의 $L_1$ loss가 바로 Reconstruction Loss

</aside>

### ***2.1.3. KL-DIVERGENCE***

prior 인코더의 input condition $c$는 텍스트로부터 추출된 음소인 $c_{text}$, 음소와 latent vector간의 alignment $A$로 구성됨

alignment는 크기가 $|c_{text}|~*~|z|$인 hard monotonic attention 행렬

- 각 음소가 target speech에 얼마나 길게 나타나는지를 표현
- 행렬의 크기는 입력 음소들의 수와 잠재변수의 수에 따라 결정되며, 각 요소는 해당 음소가 목표 음성과 얼마나 시간적으로 일치해야 하는지를 나타냄
- 입력 음소마다 얼마나 길게 확장되어 목표 음성과 시간적으로 정렬되는지를 나타내는 monotonic hard attention layer
- 음소의 확장을 시간상으로 정렬하는 데 얼마나 많은 프레임이 필요한지를 나타냄
- 각 input 음소가 time-aligned된 타겟 음성에 맞춰 얼마나 확장되어야 하는지 표현
- alignment에 대한 ground truth 라벨이 없기 때문에, alignment를 각 학습 iteration마다 추정해야 함, 섹션 $2.2.1$에서 논의

문제 설정에서, posterior 인코더를 위해 보다 높은 해상도의 정보를 제공하는 것을 목적으로 함

그래서, 멜 스펙트로그램 대신 target speech의 linear-scale 스펙트로그램 $x_{lin}$ 사용

- 선형 스펙트로그램은 멜 스펙트로그램보다 더 많은 주파수 세부 정보를 포함하고 있기 때문
- 고주파 영역에서 더 세밀한 정보를 제공하므로, 선형 스펙트로그램을 사용함으로써 후방 인코더에 더 많은 고해상도 정보를 제공할 수 있게 됨

수정된 input은 variational inference의 속성을 위반하지 않음

- KL divergence는 두 확률분포간의 차이를 나타내는 지표, 하나의 분포가 다른 분포와 얼마나 다른지를 나타냄
- VITS 모델에서는 KL 다이버전스를 사용하여 사전 인코더와 사후 인코더간의 분포의 차이를 측정하고자 함
    - 모델이 variance inference를 통해 잠재변수를 학습하는 과정에서 얼마나 잘 학습되고 있는지를 확인하기 위함
    - ⇒ KL 다이버전스는 두 확률분포간의 차이를 측정하여, 사전 인코더의 출력과 사후 인코더의 출력간의 유사성을 평가, 이는 모델이 원하는대로 정보를 잘 학습하고 있는지를 확인하는 중요한 지표 중 하나

variational inference에서의 목표는 사후 분포를 학습하여 잠재 변수의 분포를 approximation하는 것

- 사전 인코더는 입력 조건을 기반으로 사전 분포를 추정하고, 사후 인코더는 관측 데이터를 기반으로 사후 분포를 추정
- 사전 분포와 사후 분포가 가능한 한 비슷해지도록 모델을 학습하게 됨

따라서 KL divergence는:

$$
L_{kl} = logq_\phi(z|x_{lin}) - logp_\theta(z|c_{text},A), \\z\sim q_\phi(z|x_{lin}) = N(z;\mu_\phi(x_{lin}),\sigma_\phi(x_{lin}))
$$

인수분해 정규분포는 사전 인코더와 사후 인코더를 파라미터화 하는 데 사용

사전 분포의 표현력을 높이는 것은 현실적인 sample을 만드는데 중요한 요소임

⇒ normalizing flow $f_0$ 사용, 이는 간단한 분포를 더 복잡한 분포로 역변환할 수 있는 기능 제공, 이는 change-of-variable 룰을 따름, factorized normal prior distribution에다가 normalizing flow를 적용:

$$
p_\theta(z|c) = N(f_\theta(z);\mu_\theta(c),\sigma_\theta(c))\left\vert det\frac{\partial f_\theta(z)}{\partial z} \right \vert,\\c = [c_{text},A]
$$

<aside>
🗣 condition으로 사용하는 데이터는 텍스트로부터 나온 $c_{text}$와 음소와 잠재 변수간의 alignment임
alignment: hard monotonic attention matrix, 각 음소가 음성에 어떻게 매칭되는지를 나타내는 행렬
또한 사후 인코더에 더 고해상도의 정보를 제공하기 위해 타겟 음성의 line-spectrogram 을 사용(멜 스펙트로그램 X)

사전 분포의 표현력을 높이는 것이 현실적인 샘플을 만드는 데 중요한 요소라는 것을 알아냄
사전 인코더와 사후 인코더에서는 normalizing flow 사용, 단순한 normal distribution을 이리꼬고 저리꼬고 해서 더 복잡한 모양의 distribution으로 만들어 사용한다는 의미 ⇒ 표현력이 증가함

</aside>

<aside>
🗣 **Kullback-Leibler divergence**
두 확률분포의 차이를 계산하는 함수로, 어떤 이상적인 분포에 대해 그 분포를 근사하는 다른 분포를 사용해 샘플링한다면 발생할 수 있는 정보 엔트로피 차이를 계산
즉, 정보량 차이 (p와 q의 cross entropy에서 p의 엔트로피를 뺀 값, 결국 두 분포의 차이를 나타냄)

전체를 알 수 없는 분포 p(x)에서 추출되는 데이터를 모델링하고 싶다고 가정
이 분포에 대한 어떤 학습 가능한 파라미터의 parametric distribution q(x|$\theta$)를 이용해 근사시킨다고 가정
이 세타를 결정하는 법 중 하나는 바로 p(x) 와 q(x|) 사이의 KL-divergence를 최소화시키는 세타를 찾는 것
p(x)를 모르기 때문에 이를 직접 해내기는 불가능, 그러나 몇 개의 샘플데이터는 알고 있으므로 p(x)에 대한 기댓값은 그 샘플들의 평균을 통해 구할 수 있음
KL-divergence를 minimize 하는것이 결국 likelihood를 maximize하는 것과 같음

cross entropy는 negative log likelihood와 같음, cross entropy를 minimize하는 것이 log likelihood를 maximize 하는 것과 같음
확률분포 p, q에 대한 cross entropy는 H(p) + KL(p|q)이므로 KL-divergence를 minimize하는 것 또한 log likelihood를 maximize하는 것과 같음

</aside>

## ***2.2 Alignment Estimation***

### ***2.2.1 MONOTONIC ALIGNMENT SEARCH***

input text와 목표 음성 사이의 alignment $A$를 추정하기 위해, Monotonic Alignment Search(MAS) 적용

- normalizing flow $f$에 의해 파라미터화된 데이터의 likelihood를 최대화하는 alignment를 찾음:

$$
(5)~~~A = arg ~max_{\hat A}~logp(x|c_{text}, \hat A)\\= arg~max_{\hat A}~logN(f(x);\mu(c_{text}, \hat A), \sigma(c_{text}, \hat A))
$$

후보 alignment는 단조적이어야(monotonic)하며, 건너뛰지 않아야(non-skipping)함 ⇒ 제약사항

- 사람들이 텍스트를 읽을 때 일반적으로 문장을 빠짐없이 순서대로 읽는다는 사실을 반영

최적 alignment 를 찾기 위해, Kim et al.은 dynamic programming 사용

그러나 MAS를 직접적으로 적용하는 것은 어려움, 논문 모델 목적이 ELBO를 최대화하는 것이고 정확한 log-likelihood가 아니기 때문

그래서 ELBO를 최대화하는 alignment를 찾기 위해 MAS를 재정의, 이는 잠재변수 $z$를 최대화하는 alignment를 찾는 것으로 축소됨

$$
(6) ~~~ arg~max_{\hat A} ~ logp_\theta(x_{mel}|z)-log\frac{q_\phi(z|x_{lin})}{p_\theta(z|c_{text}, \hat A)}\\=arg~max_{\hat A}~logp_\theta(z|c_{text},\hat A)\\=logN(f_\theta(z);\mu_\theta(c_{text},\hat A),\sigma_\theta(c_{text},\hat A))
$$

*Equation 5*와 *Equation 6*의 유사성 때문에, original MAS 수행을 변경없이 사용 가능

Appendix A가 MAS의 수도 코드를 포함함

<aside>
🗣 alignment를 찾기 위해 MAS(Monotonic Alignment Search)를 사용
근데 VAE에서 지금 log-likelihood를 사용하는 것이 아니라 ELBO를 사용하기 때문에 MAS에서처럼 dynamic programming을 바로 사용하기는 어려움

⇒ ELBO를 최대화 하는 alignment를 찾도록 수정해야 함 ⇒ latent 변수 z의 log-likelihood를 최대화하는 것으로 축소시켜 MAS를 재정의

</aside>

### ***2.2.2 DURATION PREDICTION FROM TEXT***

추정된 alignment(정렬)의 각 행의 모든 컬럼을 더함으로써 각 input token $d_i$의 지속시간을 계산 가능  $\sum_jA_{i,j}$ 

duration은 deterministic duration predictor를 훈련시키는 데 쓰일 수 있음, 이전 연구 Kim et al에서 제안되었던 것처럼

그러나 이는 사람이 각각 다른 발화 속도로 말할 때의 방식을 표현할 수 없음

- 즉, 고정된 발화 길이로는 사람들이 각자 다르게 발음하는 것을 표현할 수 없다는 것

사람처럼 **자연스러운 발화의 리듬을 생성하기 위해서, stochastic duration predictor**를 설계 ⇒ 확률론적인 duration predictor

- 이렇게 설계된 모델은 주어진 음소의 duration 분포를 따르는 샘플을 생성하도록 함
- 즉, 발화의 리듬을 예측하고 이를 따라 자연스러운 음성을 생성할 수 있는 모델을 만들었다는 것

stochastic duration predictor는 일반적으로 Maximum likelihood estimation(MLE)을 통해 학습되는 flow-based 생성 모델

그러나 MLE의 직접적인 적용은 어려움, 각 input 음소의 duration이

1. 이산적인 정수임, 따라서 연속적인 normalizing flow를 사용하기 위해서는 역양자화(변량화?)(dequantized) 되어야함
2. 스칼라값임, 역함수에 의한 고차원의 변환을 막음 ⇒ 하나의 숫자로 이루어진 값으로, 차원이 없거나 1차원인 값, 이러한 값은 고차원으로 변환하기에는 한계가 있음

⇒ 이러한 문제들을 해결하기 위해 **variational dequantization과 variational data augmentation을 적용**함

⇒ 두 개의 변수 $u$와 $\nu$를 도입, duration sequence $d$와 동일한 resolution과 dimension을 가지는 랜덤 변수

- 이 변수들은 각각 dequantization와 데이터 확장(data augmentation)에 사용됨

- $u$의 범위를 $[0,1)$로 제한, $d-u$가 양의 실수가 되도록 함
- $\nu$와 $d$를 채널별로 연결하여 고차원의 latent representation 생성
- 이 두 변수를 $q_{\phi}(u,\nu|d,c_{text})$ posterior 분포 근사를 통해 샘플링함
- 결과로 목적함수는 음소 duration의 log-likelihood의 variational lower bound가 됨
- 즉, 음소 duration의 로그 우도의 변량 하한이 됨

 

$$
(7)~~~log~p_{\phi}(d|c_{text})\geq E_{q_\phi}(u,\nu|d, c_{text} \left [log\frac{p_\theta(d-u,\nu|c_{text})}{q_\phi(u,\nu|d,c_{text})}\right ]
$$

**즉, stochastic duration predictor의 training loss $L_{dur}$은 negative variational lower bound 가 됨** 

- 원래 목적함수는 로그 우도를 최대화하는 것이지만, 훈련 과정에서 일반적으로 손실함수를 최소화하는 것이 일반적
- 따라서 로그 우도의 음의 값을 최소화하는 것으로 바뀌게 되는데, 이를 나타내기 위해 negative가 붙은 것

input 조건에는 입력 그래디언트의 back propagation을 방지하는 stop gradient operator를 적용

이를 통해 duration predictor의 훈련이 다른 모듈에 영향을 미치지 않도록 함

- duration predictor는 음성의 길이나 리듬과 관련된 정보를 추정하는 역할을 하며, 이 정보는 다른 모듈의 훈련에 직접적으로 영향을 주지 않아도 됨, 이 정보는 텍스트에서 추출한 특징과 연관이 있을 수 있지만, 다른 모듈들과의 연결을 통해 직접적인 영향을 주지 않음
- 이의 목적은 음성의 길이를 추정하는 것, 텍스트와 관련된 작업 중 하나이며 다른 모듈들이 텍스트의 다른 측면을 다루는 동안 독립적으로 진행될 수 있도록 해야 함
- 역정파를 통해 모든 모듈에 그래디언트가 전달될 경우, 예상치 못한 결과가 나타날 수 있음

sampling 과정은 상대적으로 간단함; 음소 duration이 stochastic duration predictor의 inverse transformation에 의해 랜덤한 노이즈로부터 샘플링된 다음 정수로 변환됨

<aside>
🗣 Alignment가 추정되었다면 각 input token들의 길이는 그냥 매트릭스에서 row마다 각 컬럼들의 합을 구하면 됨
근데 사람의 자연스러운 발화 리듬 요소를 추가하기 위해, stochastic duraion predictor 이용

predictor를 flow-based 모델로 만들고 maximum likelihood estimation 방법으로 훈련하면 되는데, 바로는 안됨
왜? 인풋 음소들의 길이가 정수이고 스칼라값이기 때문, 이를위해 variational dequantization과 variational data augmentation 사용

</aside>

## ***2.3. Adversarial Training***

학습 시스템에 adversarial training을 적용하기 위해, output이 decoder $G$로부터 생성된 아웃풋인지, 아니면 ground truth waveform $y$인지 구별하는 판별자 $D$를 추가

이 작업에서는, 음성 합성에 성공적으로 적용되는 두 타입의 loss 사용

adversarial training을 위한 the least-squares loss function과 generator를 학습시키기 위해 추가적으로 feature-matching loss 사용

$$
(8)~~~L_{adv}(D) = E_{(y,z)}\left [(D(y)-1)^2 + (D(G(z)))^2 \right],
$$

$$
(9)~~~L_{adv}(G) = E_z\left[(D(G(z))-1)^2\right],
$$

$$
\\(10)~~~L_{fm}(G) = E_{(y,z)}\left[\sum_{l=1}^{T}\frac{1}{N}||D^l(y)-D^l(G(z))||_1\right]
$$

$T$는 discriminator의 레이어 총 개수를 나타내며 $D^l$은 discriminator의 $l$번째 레이어의 featuremap을 나타냄, 이 레이어에는 $N_l$개의 특성이 존재함

특히, feature matching loss는 VAE의 요소별 construction loss의 대안으로 제안됨

- 이는 discriminator의 히든 레이어에서 측정되는 reconstruction loss로도 볼 수 있음
- ⇒ discriminator의 각 레이어마다 피쳐맵간의 차이를 고려하는 로스

## ***2.4. Final Loss***

VAE와 GAN training의 combination으로, conditional VAE의 training을 위한 total loss는 아래와 같이 표현됨:

$$
L_{vae} = L_{recon} + L_{kl} + L_{dur} + L_{adv}(G) + L_{fm}(G)
$$

## ***2.5. Model Architecture***

제안된 모델의 전체 아키텍처는 posterior encoder, prior encoder, 디코더, discriminator 그리고 stochastic duration predictor로 구성됨

posterior encoder과 discriminator는 training에만 사용됨 (추론에서는 사용 X)

아키텍처 디테일은 Appendix B에서 볼 수 있음

### ***2.5.1. POSTERIOR ENCODER***

posterior 인코더는, WaveGlow와 Glow-TTS에서 사용된 non-casual WaveNet residual block 사용

- WaveNet residual 블록은 확장된 합성곱(dilated convolutions) 레이어들로 이루어짐
- 각 레이어는 게이트 활성화 유닛(gated activation unit)과 스킵 연결(skip connection)을 포함
    - 이러한 구조는 WaveNet 모델에서 사용되며, 시계열 데이터를 처리하는데 효과적, WaveNet은 음성 생성에 주로 사용되는 모델로, 자연스러운 음성을 생성하는데 쓰임
- 블록들 위의 linear projection 레이어(선형 투영 레이어)는 normal posterior distribution(정규 사후 분포)의 평균과 분산을 생성
- 다중 화자의 경우, residual block에 전역 조건을 추가하여 화자 임베딩을 사용
    - 모델에서 화자 정보를 고려하여 처리한다는 말

<aside>
🗣 posterior encoder = WaveGlow + non-casual WaveNet residual block

residual block 위에 linear projection layer를 추가해 normal posterior distribution의 평균과 분산을 생성
multi-speaker의 경우에는 residual block안에 speaker embedding을 더해서 global conditioning을 이용

</aside>

### ***2.5.2 PRIOR ENCODER***

사전 인코더는 input 음소 $c_{text}$를 처리하는 텍스트 인코더와 사전 확률의 유연성을 높이는 normalizing flow $f_\theta$로 이루어짐

텍스트 인코더는 transformer encoder, 이는 절대적인 위치 인코딩 대신 상대적인 위치 표현을 이용

텍스트 인코더를 통해 $c_{text}$로부터 숨겨진 표현 $h_{text}$를 얻음

텍스트 인코더 위의 linear projection layer는 $h_{text}$를 받아 사전 확률 구성에 쓰이는 평균과 분산을 생성

**normalizing flow는 여러 개의 WaveNet residual 블록으로 이루어진 afiine coupling layer의 stack**

단순화를 위해, normalizing flow를 volume-preserving transformation(볼륨 보존 변환)으로 설계

이 변환의 Jacobian determinant는 1

multi speaker setting에서는, normalizing flow의 residual block에 화자 임베딩을 전역 조건부로 추가

- normalizing flow의 일부 구성 요소에 화자 정보를 포함하여 다양한 화자의 음성을 생성할 수 있도록 함

<aside>
🗣 prior encoder = text encoder + normalizing flow
텍스트 인코더 = 트랜스포머 기반, 상대적 위치 표현 (relative positional representation) 사용
텍스트 인코더를 통해 ctext로부터 htext를 얻고, 여기에 linear projection layer로 이 htext를 받아 prior normal distribution의 평균과 분산을 생성

normalizing flow는 여러 WaveNet residual block을 쌓아서 구성된 affine coupling layer 이용
multi speaker setting에서는 normalizing flow의 residual block에 speaker embedding을 더해서 사용함

</aside>

### ***2.5.3. DECODER***

decoder는 기본적으로 HiFi-GAN V1 generator를 이용

여러개의 transposed convolution으로 이루어져있으며, 각 transposed convolution 층 다음에는 다중 수용장 퓨전 모듈(Multi-Receptive Field Fusion Module, MRF)이 따라옴

- transposed convolution은 일반적인 convolution의 반대 방향으로 동작함
    - 즉, 입력 데이터를 확대하고 공간 해상도를 높이는 역할을 함
- MRF는 입력 데이터의 다양한 특징을 합치는 역할을 수행함
- MRF의 출력은 각기 다른 receptive field size를 가진 residual block의 출력의 합임
- 다중 화자 세팅에서는 화자 임베딩을 변환하는 linear layer를 추가하고 이를 잠재 변수 $z$에 더함

1. MRF는 여러 다른 receptive field size를 가진 residual block의 출력을 합산 ⇒ 다양한 특징을 통합해 더 풍부한 정보를 얻기 위함
2. multi-speaker 설정에서는 추가적으로  speaker embedding을 변환하는 선형 레이어를 사용, 이 변환된 화자 정보는  input latent variables $z$에 더해짐

<aside>
🗣 decoder = HiFi-GAN V1 
이는 여러개의 transposed convolution과 multi-receptive field fusion(MRF) 모듈을 쌓아서 사용한 모델
multi-speaker의 경우 linear layer를 사용해 speaker embedding을 변형하여 input latent variables z에 더함

</aside>

### ***2.5.4. DISCRIMINATOR***

이 연구에서는 HiFi-GAN에서 제안된 multi-period discriminator의 아키텍처를 따름

multi-period discriminator는 input waveform의 서로 다른 periodic pattern에 작용하는 각기 다른 Markovian window-based sub-discriminators(마르코비안 윈도우 기반 하위 판별기)의 혼합체

이러한 판별기는 input waveform의 서로 다른 periodic pattern을 기반으로 동작하며, 이러한 패턴은 마르코비안 윈도우를 기반으로함

<aside>
🗣 discriminator = HiFi-GAN에서 제안한 multi-period discriminator 이용
이는 waveform의 여러 periodic pattern을 고려하는 식

</aside>

### ***2.5.5. STOCHASTIC DURATION PREDICTOR***

stochastic duration predictor는 조건부 input인 $h_{text}$로부터 음소 duration의 분포를 추정

효율적인 매개변수화를 위해, stochastic duration predictor에 dilated & depth-seperable convolution layers를 residual bloak으로 쌓음

⇒ stochastic duration predictor는 $h_{text}$를 조건으로 하여 음소의 지속시간 분포를 추정하며, 이를 위해 효율적인 파라미터화를 위해 특별한 방법을 사용

또한 neural spine flows 사용, monotonic한 rational-quadratic splines를 사용해 레이어를 결합함으로써 가역적 비선형 변환의 한 형태를 취함

Neural spline flows는 기존에 사용되던 affine coupling layer와 비교했을 때 비슷한 수의 파라미터를 사용하면서도 변환 표현력을 향상시킬 수 있음

다중 화자 설정에서는, 화자 임베딩을 변환하는 linear layer를 추가하고 이를 input $h_{text}$에 더함

<aside>
🗣 stochastic duration predictor는 효율적인 매개변수화를 위해 dilated & depth-seperable conv 레이어를 쌓아 구성
nural spine flows 적용, 이는 비슷한 숫자의 파라미터로도 표현 능력을 향상시킬 수 있음
multi-speaker의 경우, speaker embedding을 linear layer로 변환시켜 이를 input htext에 더함

</aside>

# ***3. Experiments***

## 3.1. ***Datasets***

두 가지의 다른 데이터셋에 대해 실험을 수행, LJ Speech dataset & VCTK dataset

- 다른 공개적으로 사용 가능한 모델들과의 비교를 위한 LJ Speech dataset
- 우리 모델이 다양한 말 특성을 학습하고 표현할 수 있는지를 검증하기 위한 VCTK 데이터셋

LJ speech dataset은 단일 화자의 13,100개의 짧은 오디오 클립으로 구성, 총 길이는 약 24시간

오디오 형식은 16-bit PCM, sample rate는 22kHz, 아무런 조작 없이 그대로 사용

우리는 랜덤하게 데이터셋을 split하여 training set으로 만듦 (12,500 샘플), validation set은 100 samples, test set은 500 samples

VCTK 데이터셋은 다양한 억양의 109명의 네이티브 English speaker의 약 44,000개의 짧은 오디오 클립으로 구성됨, 오디오 클립의 총 길이는 약 44시간

오디오 형식은 16-bit PCM, sample rate 44kHz, 그러나 우리는 22kHz로 줄임

랜덤하게 split하여 training set(43,470 samples), validation set(100 samples), test set(500 samples)

## ***3.2. Preprocessing***

**posterior 인코더의 입력 데이터**로 raw waveform에서 Short-time Fourier transform**(STFT)를 통해 얻어질 수 있는 선형 스펙트로그램**을 사용

transform에서 FFT 사이즈와 윈도우 사이즈, 홉 사이즈는 각각 1024, 1024, 256

**reconstruction loss를 위해 80 bands mel-scale spectrograms 사용**, 이는 linear spectrogram에 mel-filterbank를 적용함으로써 얻어짐

International Phonetic Alphabet**(IPA) 시퀀스를 prior 인코더의 입력으로 사용**

오픈 소스 소프트웨어로 text sequence를 IPA(국제 음성 기호) 음소 시퀀스로 변환하고 변환된 시퀀스들은 Glow-TTS의 구현방식을 따라 공백 토큰과 함께 삽입됨 (interpersed: 두가지 또는 그 이상의 다른 것들이 교차하거나 섞여 있는 것을 의미, 이 문장에서는 변환된 IPA 음소 시퀀스가 공백 토큰과 함께 교차되거나 섞여서 사용된다는 의미)

모델은 IPA 음소 시퀀스를 입력으로 받아 음성을 생성하게 됨

<aside>
🗣 posterior encoder input = waveform에 STFT를 적용시켜 얻어지는 linear spectrogram
reconstruction loss ⇒ 80 bands mel-scale spectrogram 사용
prior encoder input = IPA sequence 사용

</aside>

## ***3.3. Training***

네트워크는 AdamW optimizer를 통해 훈련됨

- $\beta_1=0.8,~\beta_2=0.99$
- weight decay $\lambda=0.01$
- learning rate decay는 $0.999^{1/8}$ (각 에포크가 지날 때마다 저만큼 감소시킴)
- 초기 learning rate는 $2*10^{-4}$ 로 정의

이전 연구를 따라서, **windowed generator training**을 채택, 이는 raw waveform의 일부 파트만 생성하는 방법, training동안 훈련 시간과 메모리 사용을 줄이기 위함

전체 latent representation을 입력으로 넣는 것이 아니라, latent representation에서 **32 윈도우 사이즈로 랜덤하게 세그먼트 추출, 이는 decoder의 입력으로 들어감**

그리고 또한 training target으로서 **그에 상응하는 ground truth 오디오 세그먼트도 추출**, 이는 원본 오디오 데이터에서 추출됨

4개의 NVIDIA V100 GPUs를 사용하여 혼합 정밀도 훈련(mixed precision training)수행

GPU마다 배치 사이즈는 64, 모델은 800k(800,000) step만큼 훈련됨 (step은 모델이 한 번 업데이트 되는 것을 의미, 800,000 단계동안 모델이 훈련함)

배치사이즈는 한 번에 모델에 공급되는 데이터 샘플의 수를 의미

## ***3.4 Experimental Setup for Comparison***

해당 모델을 다른 최고 성능 공개 사용 가능 모델과 비교함

- 자기회귀 모델인 Tacotron2, flow-based non-자기회귀모델인 Glow-TTS를 1단계 모델, HiFi-GAN을 2단계 모델로 사용
- 그들의 공개된 구현과 사전 훈련된 weight를 사용
- 2단계 TTS 시스템이 시퀀스 training을 통해 이론적으로 높은 합성 품질을 달성할 수 있기 때문에, 1단계에서 예측된 output을 사용해 최종적으로 fine-tuned된 HiFi-GAN 모델을 100k 스텝까지 학습시킴
- teacher-forcing 모드의 Tacotron2에서 생성된 멜스펙트로그램을 이용해 HiFi-GAN을 fine-tuning하는 것은 Glow-TTS를 통해 생성된 멜 스펙트로그램으로 fine-tuning 하는 것보다 두 모델 모두에게서더 좋은 품질을 이끌어낸다는 것이 경험적으로 증명됨
- 따라서 더 나은 fine-tuning된 HiFi-GAN을 Tacotron2와 Glow-TTS 둘 다에 추가함
- 각 모델이 샘플링 과정에서 어느 정도의 무작위성을 가지고 있기 때문에, 실험에서 각 모델의 무작위성을 제어하는 하이퍼 파라미터를 고정시킴
- Tacotron2의 pre-net의 dropout 확률은 0.5로 설정
- Glow-TTS같은 경우, 사전 확률의 표준편차는 0.333
- VITS에서는, stochastic duration predictor의 input noise의 표준 편차를 0.8로 설정
- 사전 분포의 표준 편차에 scale factor 0.667을 곱함 ⇒ 사전 분포의 변동 폭을 조정 가능

<aside>
🗣 1단계 모델: Tacotron2, Glow-TTS
2단계 모델: HiFi-GAN
실험을 해보니 Tacotron2에서 생성된 멜 스펙트로그램으로 fine-tuning한 HiFi-GAN의 성능이 더 좋아 이를 사용

</aside>

# ***4. Results***

## ***4.1. Speech Synthesis Quality***

품질 평가를 위해 crowd-sources MOS 테스트를 수행

- 평가원들은 무작위로 선택된 오디오 샘플들을 듣게 됨, 그리고 자연스러움을 1부터 5 사이의 점수로 평가
- 평가원들은 하나의 오디오 샘플을 한 번만 평가
- 모든 오디오 클립은 amplitude의 차이로 오는 영향을 피하기 위해 정규화됨
- 모든 품질 평가는 이러한 방식으로 진행됨

평가 결과는 *Table 1*에 존재

![*Table 1.* LJ Speech dataset에서 신뢰도 95% 구간에서 평가된 MOS 비교](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/4ce3721c-a821-4bf0-b05a-f37fd46bda86/Untitled.png)

*Table 1.* LJ Speech dataset에서 신뢰도 95% 구간에서 평가된 MOS 비교

- VITS는 다른 TTS 시스템을 능가, ground truth와 유사한 MOS 달성
- VITS(DDP)는 Glow-TTS에서와 동일하게 deterministic duration predictor를 사용한 모델, 두 번째로 성능이 높음을 확인 가능
- 이러한 결과는 다음을 암시함
    1. stochastic duration predictor는 deterministic duration predictor보다 더욱 현실적인 음소 duration을 생성
    2. end-to-end 훈련 방법은 더 나은 샘플을 만드는 데 효율적인 방법임, 비슷한 duration predictor 아키텍처를 유지하더라도

논문의 방법의 효과를 입증하기 위해 ablation study 진행, 사전 인코더의 normalized flow와 사후 인코더의 input인 linear-scale 스펙트로그램을 포함함

- ablation study의 모든 모델은 300k 스텝만큼 훈련됨
- 결과는 *Table 2*에 존재

![*Table 2.* ablation studies에서의 MOS 비교](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/c8a2fe96-41bf-4fc6-ab07-57d99f201f1d/Untitled.png)

*Table 2.* ablation studies에서의 MOS 비교

- 사전 인코더에서 normalizing flow를 제거하는 것에 대한 결과는 베이스라인과 비교해 1.52의 MOS 감소를 이끌어냄, 이는 사전 확률의 유연성이 합성 퀄리티에 상당한 영향을 미친다는 것을 입증
- posterior input에서 linear-scale 스펙트로그램을 멜-스펙트로그램으로 대체하는 것은 -0.19 MOS의 품질 저하를 낳음
- 이는 고해상도의 정보가 VITS의 합성 품질을 높이는 데 효과적이라는 것을 가리킴

## ***4.2. Generalization to Multi-Speaker Text-to-Speech***

논문의 모델이 다양한 음성 특징을 학습하고 표현할 수 있다는 것을 입증하기 위해, Tacotrao2, Glow-TTS, HiFi-GAN과 비교

- 모델을 다중 화자 음성 합성으로 확장할 수 있는 능력을 보여준 Tacotron2, Glow-TTS 및 HiFi-GAN
- 모델들을 VCTK 데이터셋으로 훈련시킴
- 섹션 $2.5$에서 묘사한바와 같이 모델에 화자 임베딩 추가
- Tacotron2에서는, 화자 임베딩을 broadcast하고 인코더 output과 연결
- Glow-TTS에서는, 이전 연구에 따라 전역 조건을 추가
- 평가 방법은 섹션 $4.1$에서와 동일
- *Table 3*에서 볼 수 있듯이, 모델은 다른 모델보다 더 높은 MOS 점수를 달성한 것을 알 수 있음
- 이는 논문의 모델이 end-to-end 방식에서 다양한 음성 특징을 학습하고 표현할 수 있다는 것을 입증함

![*Table 3.* VCTK 데이터셋에서 신뢰구간 95%에서의 평가된 MOS 점수 비교](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/ec842935-6dca-413a-a5f2-6a3076208d9b/Untitled.png)

*Table 3.* VCTK 데이터셋에서 신뢰구간 95%에서의 평가된 MOS 점수 비교

## ***4.3. Speech Variation***

stochastic duration predictor가 얼마나 많은 다른 길이의 음성들을 생성하는지 입증함

그리고 합성된 샘플들이 다양한 음성 특징을 얼마나 가지고 있는지 입증함

Valle et al.과 유사하게, 모든 샘플들이 How much variation is there? 이라는 문장으로부터 생성됨

*Figure 2a*는 각 모델마다 100개의 생성된 발화를 보여줌

![*Figure 2.* 초단위의 샘플 지속 길이, (a) LJ Speech dataset, (b) VCTK dataset](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/e324f04b-eb5d-4cf6-b4f4-fcd39568d54a/Untitled.png)

*Figure 2.* 초단위의 샘플 지속 길이, (a) LJ Speech dataset, (b) VCTK dataset

- Glow-TTS는 확률론적이지 않은 duration predictor(deterministic duration predictor)를 사용하기 때문에 고정된 길이의 발화만을 생성하는 반면,
- 논문의 모델에서 나온 샘플들은 Tacotron2와 비슷한 길이 분포를 따르는 것을 보임
- *Figure 2b*는 다중 화자 설정일 때 5명의 화자에게서 100개의 발화를 보여줌, 모델이 화자에 따른 음소 지속시간을 학습한다는 것을 의미
- *Figure 3*에서 YIN 알고리즘을 통해 추출된 10개 발화의 F0 contours는 논문의 모델이 다양한 pitch와 리듬으로 음성을 생성한다는 것을 보여주며, *Figure 3d*에서 각기 다른 화자에게서 생성된 다섯개의 샘플들은 논문의 모델이 각 화자의 identity마다 아주 다른 길이와 pitch를 표현한다는 것을 입증함
- Glow-TTS는 사전 분포의 표준 편차를 늘림으로써 pitch의 다양성을 증가시킬 수 있음, 그러나 대조적으로, 합성 품질을 낮춤

![*Figure 3.* 발화(”How much variation is there?”)의 pitch track. 샘플들은 각각 (a) VITS, (b) Tacotron2, (c) Glow-TTS의 단일 화자 세팅에서 생성됨, (d)는 VITS의 다중 화자 모드에서 생성된 샘플](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/b994d994-eb1f-4150-9e73-d328cbb148f0/Untitled.png)

*Figure 3.* 발화(”How much variation is there?”)의 pitch track. 샘플들은 각각 (a) VITS, (b) Tacotron2, (c) Glow-TTS의 단일 화자 세팅에서 생성됨, (d)는 VITS의 다중 화자 모드에서 생성된 샘플

## ***4.4. Synthesis Speed***

병렬 2단계 TTS 시스템과 합성 속도를 비교 (Glow-TTS, HiFi-GAN)

- 음소 시퀀스와 LJ Speech dataset의 테스트셋에서 무작위로 선택된 100개 문장으로부터 raw waveform을 생성하는 전체 과정에 대해 동기화된 경과 시간을 측정
- 단일 NVIDIA V100 GPU를 사용, batch size=1
- 결과는 *Table 4*에 존재

![*Table 4.* 합성 속도 비교. nkHz는 모델이 1초마다 n * 1000개의 오디오 샘플을 만들어낼 수 있음을 뜻함. Real-time은 실시간 합성 속도를 뜻함](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/99a5b36b-89d4-4674-831e-fc24f211f25e/Untitled.png)

*Table 4.* 합성 속도 비교. nkHz는 모델이 1초마다 n * 1000개의 오디오 샘플을 만들어낼 수 있음을 뜻함. Real-time은 실시간 합성 속도를 뜻함

우리 모델은 미리 정의된 중간 표현 생성을 위한 모듈이 필요하지 않으므로, 샘플링 효율성과 속도가 크게 향상됨

# ***5. Related Work***

## ***5.1. End-to-End Text-to-Speech***

최근들어, 2단계 신경망 TTS 모델들이 인간과 같은 음성을 합성하는 것이 가능함

그러나, 그들은 일반적으로 1단계에서의 모델 아웃풋과 함께 훈련된 혹은 fine-tune된 보코더를 필요로 함

- 이는 훈련과 배포에 비효율적임
- 또한 미리 정의된 중간 표현이 아닌 훈련된 hidden representation을 사용할 수 있는 end-to-end 접근방식의 잠재적인 이익을 얻을 수 없음
- 최근, raw waveform을 생성하는 더 어려운 과제를 해결하기 위해 단일 단계 end-to-end TTS 모델들이 제안됨
- 이들은 직접적으로 텍스트로부터 멜-스펙트로그램보다 더 풍부한 정보를 포함함(ex. 고주파 response 그리고 phase)
- FastSpeech 2s는 FastSpeech2의 확장판으로, 텍스트 표현을 학습하도록 돕는 adversarial training과 auxiliary mel-spectrogram decoder를 채택함으로써 end-to-end 병렬 생성을 가능하게함
- 그러나, one-to-many 문제를 해결하기 위해, FastSpeech 2s는 input condition으로 사용된 음성으로부터 음소의 지속시간, pitch, 그리고 에너지를추출해야 함
- EATS는 adversarial training을 적용, 또한 differentiable alignment scheme을 채택
- 타겟 음성과 생성된 음성의 길이의 불일치 문제를 해결하기 위해, EATS는 soft dynamic time warping loss를 채택
    - 이는 dynamic programming을 사용해 계산됨
- Wave Tacotron은 end-to-end 구조를 위해 normalizing flow와 Tacotron2를 결합, 그러나 여전히 자기회귀는 남아있음
- 앞서 언급된 모든 end-to-end TTS 모델의 오디오 품질은 2단계 모델보다 좀 떨어짐

- 앞서 언급된 end-to-end 모델들과는 다르게, conditional VAE를 사용함으로써, 우리 모델은
    1. 다른 input condition을 필요로 하지 않고 텍스트로부터 직접적으로 raw waveform을 합성하는 법을 학습
    2. 최적의 alignment를 찾기 위해 loss를 계산하는 대신 dynamic programming method를 사용(MAS)
    3. 병렬적으로 샘플들을 생성함
    4. 공개적으로 사용가능한 2단계 모델들을 능가

## ***5.2. Variational Autoencoders***

VAEs는 가장 널리 사용되는 likelihood 기반 deep 생성모델 중 하나

- 우리는 TTS 시스템에 conditional VAE를 채택함
- conditional VAE는 조건부 생성 모델
    - 관측된 조건(데이터의 특정한 특성이나 정보)이 출력을 생성하는데 사용되는 잠재변수의 사전 분포를 변화시킴
    - 입력된 조건에 따라 모델이 출력을 생성하는 방식을 의미
- 음성 합성에서, Hsu et al.과 Zhang et al.은 Tacotron2와 VAE를 합쳐 음성 스타일과 억양을 학습하도록 함
- BVAE-TTS는 mel-spectrogram을 병렬로 생성, bidirectional VAE를 기반으로 함
- VAE를 1단계 모델에 적용한 이전 연구들과는 다르게, 우리는 VAE를 병렬 end-to-end TTS 시스템으로 채택
- Rezende & Mohamed (2015), Chen et al. (2017) and Ziegler & Rush (2019) 는 VAE의 성능을 개선시킴
    - normalizing flow로 사전 및 사후 분포의 표현력을 증가시킴
- 사전 분포의 표현력을 증가시키기 위해, 우리의 조건부 사전 네트워크에 normalizing flow를 추가
    - 이는 좀 더 현실적인 샘플을 생성함

- 우리의 연구와 비슷하게, Ma et al.는 조건부 사전 네트워크의 normalizing flow와 conditional VAE를 제안함, FlowSeq라는 non-자기회귀 신경망 기계 번역을 위해
- 그러나, 우리 모델이 명확하게 잠재 시퀀스를 소스 시퀀스와 align 시킬 수 있다는 사실은 FlowSeq과 다른 점
- 이는 어텐션 매커니즘을 통해 암시적으로 alignment를 학습해야하는 FlowSeq와는 다름
- 우리의 모델은 잠재 시퀀스를 표준 정규 무작위 변수들로 변환하는 부담을 제거
    - MAS를 이용해 잠재 시퀀스를 time-aligned source sequence와 match 함으로써
    - 이는 normalizing flow의 더욱 단순한 아키텍처를 허용함

## ***5.3. Duration Prediction in Non-Autoregressive Text-to-Speech***

Autogregressive TTS 그들의 자기회귀 구조와 추론과 priming중 dropout 확률을 유지하는 등 여러 기술을 통해 모델들은 각기 다른 리듬의 다양한 음성을 생성

병렬 TTS 모델들은, 반면에, deterministic duration prediction에 의존해왔음

- 이는 병렬 모델들은 타겟 음소 지속시간 혹은 타겟 음성의 전체 길이를 하나의 feed-forward path에서 예측해야했기 때문
- 이는 음성 리듬의 연관된 joint distribution을 포착하기 어렵게 함
- 이 연구에서는, flow-based stochastic duraion predictor를 제안함
- 이는 추정된 음소 지속시간의 joint distribution를 학습, 이로써 다양한 음성 리듬을 병렬로 생성함

# ***6. Conclusion***

이 연구에서는, 병렬 TTS 시스템과 end-to-end 방식으로 학습 및 생성 가능한 VITS 모델을 소개함

- 또한 음성의 다양한 리듬을 표현하기 위한 stochastic duraion predictor 도입
- 결과 시스템은 자연스러운 소리의 음성 파형을 텍스트로부터 직접 합성함
- 미리 정의된 중간 표현을 얻을 필요 없이
- 우리의 실험 결과는 우리의 방법이 2단계 TTS 시스템을 능가했음을 보였으며, 인간의 품질과 가까운 결과를 달성함
- 우리는 제안된 방법들이 좋은 성능 향상을 달성하고 단순화된 훈련 과정을 즐기기 위해 2단계 TTS 시스템이 사용되었던 많은 음성 합성 과제에서 사용되길 바람
- 우리는 또한 우리의 방법이 TTS 시스템에서 두 분리된 생성 파이프라인을 통합했음에도 불구하고, 텍스트 전처리에 문제가 있음을 지적하고 싶음
- 언어 표현의 self-supervised learning을 조사하는 것은 텍스트 전처리 단계를 생략하기 위한 가능한 방향
- 우리는 우리의 소스코드와 사전 훈련된 모델들을 다양한 미래 방향에 대한 연구를 촉진하기 위해 공개할 것

VAE 입력은 텍스트, 출력은 음성, 텍스트와 음성 간의 관계를 잘 모델링하고, 잠재변수를 이용해 음성 생성, 이 과정에서 잠재변수를 학습함으로써 텍스트와 음성간의 의미적 유사성을 잘 포착하도록 모델을 학습

VITS 단점

- 어떤 대사를 읽든 똑같은 톤으로 읽는다 ⇒ 감정 임베딩이 되는 emotioanl-vits 존재
- numpy version 1.18.5

- 1-stage vs. 2-stage
    
    1단계 tts는 텍스트를 음성으로 변환하는 단일 단계 모델,텍스트를 직접 음성으로 변환
    
    2단계 tts는 텍스트에서 중간 표현(텍스트 의미를 나타내는 특정 형태)으로 변환 후, 이를 음성으로 변환하는 두 단계로 나뉨
    
- 변분추론(variational infernce)
    
    확률 분포의 모수를 추정하는 통계적 방법 중 하나, 주어진 데이터를 기반으로 모수의 확률분포를 추정하는 과정에서 불확실성을 고려
    
    이를 통해 모델이 어떤 확률분포에서 파생됐을지에 대한 추론을 수행 가능
    
    변분 추론은 주로 베이지안 추론에서 사용되며, 확률적 모델의 사후 분포를 근사하는데 사용됨
    

- stochastic duration predictor
    - 입력 텍스트의 각 부분이 얼마동안 지속될지를 확률적으로 예측하는 모델을 가리킴
    - 다양한 리듬의 음성을 생성 가능

- MOS
    
    음성 합성 시스템의 성능을 평가하기 위한 주관적인 평가 지표
    

- 순차적 생성 프로세스
    
    한 단계가 끝나야 다음 단계를 시작할 수 있어 병렬 프로세싱을 제한함
    

- learning alignments
    
    입력과 출력 사이의 상응하는 부분 또는 구조를 학습하는 것
    
    즉, 텍스트와 그에 해당하는 스펙트로그램간의 정렬을 학습하는 것을 의미
    

- feed forward
    
    신경망의 데이터 전달방식 중 하나
    
    정보가 입력에서 출력 방향으로 단방향으로 흐르는 구조를 의미
    
    GAN을 기반으로 하며, 데이터가 입력에서 출력으로 단방향으로 전달되는 신경망 구조를 의미
    
- alignment
    
    두 시퀀스 또는 데이터 세트간의 대응 관계를 나타냄
    
    예를들어, 텍스트와 음성 데이터 사이에서 alignment는 각 단어나 음소가 음성 신호에서 어느 부분에 해당하는지를 나타냄
    
    이러한 alignment 정보는 주로 시퀀스 생성 모델에서 중요한 역할을 함
    
    모델은 입력 시퀀스와 출력 시퀀스간의 대응관계를 이해하여 올바른 결과 생성
    
    텍스트에서 음성을 합성할 때, 각 단어가 어떤 부분에서 발음되는지를 알아야 올바른 음성 생성 가능
    
    이 논문에서 언급하는 alignment는 텍스트와 멜 스펙트로그램(음성데이터의 주파수 특성)간의 대응관계를 학습하거나 추정하여 더 빠른 합성을 가능하게 하는 방법들을 언급하고 있음
    

- 멜-스펙트로그램 디코더
    
    멜-스펙트로그램을 오디오 신호로 디코딩하는 모듈
    
    멜 스펙트로그램을 다시 원래의 오디오 신호로 변환
    

- VAE
    
    variational Auto Encoder
    
    생성 모델의 한 유형으로, 데이터의 잠재변수를 학습하고 이를 사용하여 새로운 데이터를 생성할 수 있는 모델
    
    VAE는 확률적인 방식으로 데이터를 재구성, 이를 통해 데이터의 분포를 학습하고 다양한 형태의 새로운 데이터 생성 가능
    
    VAE는 주로 생성모델링, 차원축소, 데이터 복원 등 다양한 응용 분야에서 사용됨
    

- normalizing flow
    
    확률분포를 변환하는 방법 중 하나, 정규화 흐름은 기본 확률 분포를 더 복잡한 분포로 변환하는 과정을 의미
    
    이를 통해 더 다양한 형태의 데이터 생성 가능
    
- variational lower bound(evidence lower bound) of marginal log-likelihood
    
    확률 모형에서 데이터의 우도를 계산하기 어려울 때, 대신 계산 가능한 다른 값들을 이용하여 그 우도의 하한을 추정하는 방법
    
    ELBO는 베이지안 추론에서 사용되는 중요한 개념 중 하나로, 모형의 파라미터를 추정하는 과정에서 활용됨
    
    ELBO를 최대화함으로써 모형 파라미터의 불확실성을 줄이고, 더 좋은 추정값을 얻을 수 있음
    
    이를 최대화하면 모델의 예측이 실제 분포와 가까워짐, 불확실성을 줄이고 보다 정확한 예측을 할 수 있음 (모델이 추론 가능한 범위 축소)
    

- 샘플링
    
    확률분포로부터 무작위로 값을 선택하는 과정을 말함
    
    특히, 확률분포가 주어졌을 때 그 분포에서 무작위로 하나의 값을 선택하는 것을 의미
    
    확률적인 요소를 포함하는 모델에서 다양한 결과를 생성하거나 예측하는 데 사용
    

- Laplace 분포
    
    확률분포 중 하나
    
    중심경향성을 가진 데이터를 모델링하는데 사용됨
    
    정규분포와 달리 뾰족한 꼬리를 가지며, 이상치에 민감하게 반응
    
    이상치 탐지, 통계적 모델링 등 다양한 분야에서 활용됨
    

- hard monotonic attention
    
    시퀀스-투-시퀀스 모델에서 어텐션 메커니즘의 한 유형
    
    이는 주어진 입력에 대한 각 출력위치에서의 관심을 특정 입력 위치에 집중시키는 방식을 기반으로 함
    
    모노토닉: attention이 일정 방향으로만 진행된다는 것을 의미
    
    하드: 각 출력 위치에서의 관심을 특정 입력위치 하나로만 집중시킴, 소프트 어텐션과 달리 이산적인 집중을 사용
    
    출력 시퀀스의 각 위치에서 어느 입력 위치에 주로 관심을 둘지를 강하게 정해주는 것
    
    soft의 경우 출력의 각 위치가 이볅의 모든 위치에 주의를 기울일 수 있음, 즉 출력의 각 위치에서 입력의 모든 위치에 대한 attention을 가질 수 있음
    
    - stop gradient operator
        
        입력의 그래디언트를 중단시키는 연산
        
        훈련 중 일부 입력이 다른 모듈들에게 영향을 미치지 않도록 함
        
        훈련 중 지정된 임력이 역전파로 그래디언트가 전파되지 않도록 막는 것
        
        모델의 일부분을 격리시키고 효과적으로 훈련을 관리하는 데 사용될 수 있음
        
    
    - affine coupling layer
        
        입력 공간을 두 부분으로 나눈 후, 한 부분을 선형변환하고 다른 부분에 추가
        
        **즉, $y$를 $y_{I_1}$과 $y_{I_2}$로 반반으로 나누어 구성해줌 ⇒ coupling layer
        한 부분을 선형변환 ⇒ affine transformation**
        
        이를 통해 레이어는 입력을 변환하고 그래디언트를 전파할 수 있도록 함 ⇒ **계산이 쉬워짐**
        
        확률 분포를 변환하는 데 사용되며, 확률분포의 유연성을 높이는 데 도움이 됨
        
        이를 통해 더 복잡한 확률 분포를 모델링할 수 있게 됨
        
    
    - HiFi-GAN V1 generator
        
        여러 개의 전치 합성곱층으로 구성되어있음
        
    
    - MRF
        
        입력 데이터의 다양한 특징을 합치는 역할을 수행
        
        - dilated convolution layer
            
            일반적인 합성곱 연산과 다르게, 입력에 구멍(또는 dilation)을 두어 더 넓은 영역의 정보를 캡처할 수 있도록 하는 연산
            
            이는 큰 수용 영역을 가지면서도 많은 파라미터를 사용하지 않아도 되는 장점 존재
            
        
        - depth-separable convolution layer
            
            일반적인 합성곱 연산을 두 단계로 나누어 수행
            
            먼저 입력의 각 채널별로 depthwise convolution을 수행하고, 그 다음에 1x1 합성곱을 통해 채널간의 선형 결합을 수행
            
            ⇒coupling layer의 구성조합을 다양하게 만들어주는 역
            
            이를 통해 파라미터의 수를 줄이면서도 효과적으로 특징 추출 가능
            
        
        - deterministic duraion predictor
            
            랜덤성이 없는 duration predictor
            
        
        - broadcast
            
            일반적으로 데이터를 복사하여 여러 위치 또는 디바이스에 전송하는 작업을 의미
            
        
        - differentiable alignment scheme
            
            기존 방법들과는 다른, 새로운 방식으로 음성 데이터의 정렬을 수행하는 방법을 의미
            
            미분가능하다는 특성을 가지고 있음
            
            역전파등의 머신러닝 학습 기법에 적용 가능
            
            화자나 발음 등 다양한 특성을 고려하여 음성 데이터를 정렬하는데 됨