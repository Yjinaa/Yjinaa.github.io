---
title: Pre-training of Deep Bidirectional Transformers for
Language Understanding
date: 2023-10-25 14:30:00 +0300
description: BERT 논문 리딩
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [] # add tag
use_math: true
categories:
  - Paper Reading
sidebar:
    nav: "blog"
---

# ***Abstract***

- BERT는 모든 레이어에서 왼쪽과 오른쪽 문맥을 공동으로 조정함으로써 unlabeled text로부터 깊은 양방향 표현을 사전학습하도록 설계
- 그 결과로, pre-trained BERT 모델은 넓은 범위의 태스크(질문답변, [language inference](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21))를 위한 sota 모델을 만들기 위해 단 하나의 추가적인 output layer로도 fine-tuning 가능
    - 실질적 작업별 아키텍처 수정 없이 가능
- BERT는 심플하고 파워풀, [GLUE](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)에서 11개 부문에서 new-state-of-the-art(sota, 가장 우수한 모델) 결과를 얻음
    - including pushing the GLUE score to 80.5% (7.7% point absolute improvement),
    MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

# ***Introduction***

- 언어 모델 pre-training은 많은 NLP 과제들에 효과적임을 보여주었음
- 전체적으로 분석해 문장 관계를 파악하는 sentence-level (such as Lnguage inference and paraphrasing), 명명된 개체 인식과 같은 token-level 과제, 모델이 token-level에서 정교한 output을 내는 것을 필요로 하는 질문에 대한 답변까지
- downstream task를 해결하는 pretrained language representation을 위한 두 가지 전략은 *feature_based and fine-tuning*

***Feature-based approach***

- ELMo - pre-trained representations as additional features를 포함하는 task-specific 아키텍처를 사용

***Fine-tuning approach***

- Generative Pre-trained Transformer(OpenAI GPT) - minimal task-specific parameters를 도입, downstream task에서 학습됨, by 간단히 전체 pre-trained 파라미터를 파인튜닝함으로써

이 두 approach는 pre-training동안 동일한 목적함수를 공유, 일반적인 언어 표현을 학습하기 위해 단방향 언어 모델을 사용

- 현 기술들은 pre-trained representations의 power를 제한하며, 특히 fine-tuning approach에서는 더욱 그러함
- 주요 한계는, 표준 언어 모델은 단방향이고, 이는 pre-training 동안 쓰일 수 있는 아키텍처의 선택지를 제한함
- 예를 들어, OpenAI의 GPT 같은 경우, 저자들은 left-to-right 아키텍처를 사용하여 모든 토큰이 트랜스포머의 self-attention 레이어에 있는 이전 토큰에만 attend 가능
- 이러한 제한 사항은 문장 수준 과제에는 최적이 아니며, 양방향에서 문맥의 통합이 중요한 질문 답변과 같은 token-level 태스크에서 fine-tuning based approach를 적용할 때 매우 위험할 수 있음

- 따라서 이 논문에서는, **BERT**를 제안함으로써 *fine-tuning based approaches*를 개선
- Bidirectctioanl Encoder Representations from Transformers
- BERT는 이전에 언급했던 단방향의 제한사항을 완화, masked lanuage model를 사용함으로써
- pretrain된 object로서, Cloze task에 영감을 받음 (cloze task = 빈칸 채우기 task)
- MLM 은 input에서 랜덤하게 몇몇 토큰을 마스킹함
- 마스킹된 언어 모델은 인풋 데이터에서 몇몇 토큰들을 랜덤하게 마스킹, 목적은 문맥에 기초하여 masked된 단어의 원래 단어 id를 예측하는 것
- left-to-right 언어 모델 pretraining과는 다르게, MLM의 목적은 left와 right의 문맥을 통합할 수 있게 해줌, 이것은 깊은 양방향 transformer를 pre-training 하도록 함
- MLM에 더해, ‘next sentene prediction’ 과제는 전체적으로 text-pair representations를 pre-train한다.

이러한 기여는 :

- 양방향의 pre-training for language representations의 중요성을 입증함.
- Radford et al. , 단방향 언어 모델 for pre-training과는 다르게, BERT는 masked 언어모델을 사용해 pre-train된 깊은 양방향 표현을 가능하게 함
- 이는 Peters et al.과도 대조적인데, 그는 독립적으로 train된 left-to-right과 right-to-left LMs의 얕은 연쇄(shallow concatenation)을 사용

- pre-trained representations는 많은 무거운 engineered task-specific 아키텍처의 필요성을 줄임
- BERT는 많은 task-specific 아키텍처를 능가하면서 대규모 제품군의 sentence-level과 token-level 과제에서 sota performance를 성취한 첫 fine-tuning based representation 모델

- BERT는 11개의 NLP 과제에서 sota를 이룩, code와 pre-trained 모델은 [여기](https://github.com/google-research/bert)서 볼 수 있음

# 2 *Related Work*

pre-training general language representations에 대한 긴 역사가 있고, 가장 널리 사용하는 접근 방식을 간단하게 리뷰

## 2.1 ***Unsupervised Feature-based Approaches***

널리 적용 가능한 단어 표현의 학습은 non-neural과 neural 방식을 포함해 수십년동안 연구의 활발한 영역이었음

Pre-trained word embeddings는 현대 NLP 시스템의 필수적인 파트, 처음부터 학습된 임베딩보다 상당한 개선 사항을 제공

- word embedding vector를 pre-training하기 위해서, left-to-right language modeling 목표들이 사용되었으며, 또한 left와 right 문맥에서 틀린 단어들로부터 맞는 것을 구별하는 목표들도 사용됨

이러한 approach들은 더 거친 세분화를 거쳐왔음

- 문장 임베딩, 문단 임베딩
- 문장 표현을 훈련시키기 위해서, 이전의 작업들은 다음 문장의 후보들을 랭크하는 목표들을 사용
    - 이전문장의 표현이 주어지면 다음 문장 단어들을 left-to-right으로 생성하거나 **auto-encoder를 denoising하는데서 파생되는 objectives를 사용**

- ELMo와 그 이전 모델은 다른 차원을 따라 전통적인 워드 임베딩 연구를 일반화시킴
    - left-to-right과 a right-to-left 언어모델에서 context-sensitive 피처들을 extract
    - 각 토큰의 이러한 문맥상의 표현은 left-to-right과 right-to-left representation의 연결(concatenation)로 이루어짐
- 존재하는 task-specific 아키텍처와 문맥상의 워드 임베딩을 통합할때, ELMo는 질의응답, 감성분석, 명명개체인식과 같은 여러 주요 NLP 벤치마크에서 sota를 거머쥠

- Melamue et al. 은 LSTM을 이용해 left와 right 문맥 동시로부터 하나의 단어를 예측하는 과제를 통한 문맥적인 표현의 학습을 제안
    - ELMo와 유사하게, 그들의 모델은 feature-based이고 깊은 양방향은 아님

- Fedus et al.은 cloze task가 텍스트 생성 모델의 강건성을 개선시킬 수 있다는 것을 보여줌

## ***2.2 Unsupervied Fine-tuning Approaches***

feature-based approach와 마찬가지로, 이러한 방향에서의 가장 첫 번째 작업들은 오직 unlabeled 텍스트에서 워드 임베딩 파라미터만 사전훈련시킴

- 좀 더 최근에는, 문장이나 문서 문맥적 토큰 표현들을 생성하는 인코더들이 downstream task를 위해 unlabeled text로부터 pre-train되고 fine-tune 되어왔음
- 이러한 접근 방식의 장점은 애초에 적은 파라미터들만이 학습될 필요가 있다는 것
- 적어도 부분적으로는 이러한 이점으로 인해, OpenAI GPT는 GLUE 벤치마크로부터 많은 문장 레벨 태스크에서 이전에 sota를 달성
- left-to-right 언어 모델링과 오토 인코더 목적 함수들이 그러한 모델들을 pretraining하는 데 사용되었음

## ***2.3 Transfer Learning from Supervised Data***

언어 추론이나 기계 번역과 같은 large dataset에서 supervised task에서 효과를 보인 transfer learning에 관한 작업은 없었음

컴퓨터 비전 연구 또한 거대 사전 훈련된 모델로부터의 transfer learning의 중요성에 대해 입증함

- ImageNet으로 pre-train된 모델을 파인튜닝 하는 것이 효과적인 방법이라는것

# ***3 BERT***

BERT와 그 디테일한 구현을 소개

프레임 워크에는 크게 두 step이 존재:

- pre-training and fine-tuning
- [pre-training동안, 모델은 각기 다른 pre-training 태스크에 대해 unlabeled 데이터를 학습](https://www.notion.so/self-supervised-Learning-3b456eb6e6fc40fbb3359a8ea97cfd61?pvs=21)
- fine-tuning을 위해서는 BERT는 pre-trained 파라미터들과 함께 초기화되고, 모든 파라미터들은 downstream 태스크의 labeled data를 이용해 fine-tuning됨
- 각 downstream 태스크는 같은 pre-train된 파라미터로 초기화되었다고 할 지라도 구분된 fine-tuned 모델이 존재

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/2fd5dd81-1aad-4eb7-844f-2efa72f3ef39/Untitled.png)

- 위 question-answering 예제는 이 섹션에 대한 실행 과정 예제로 사용됨
- BERT의 독특한 특징은 다양한 작업에 걸친 통합 아키텍처임
- pre-trained 아키텍처와 최종 downstream 아키텍처 사이에는 작은 차이점이 존재

### ***Model Architecture***

BERT의 모델 아키텍처는 Vaswani et al.에 설명된 원래 구현을 기반으로 한 다층 양방향 트랜스포머 인코더이며, tensor2tensor library에 출시됨

트랜스포머의 사용이 일반화 되었고 우리의 구현이 거의 원본과 동일하기 때문에, 모델 아키텍처의 철저한 배경 설명을 생략할 것이며 독자들에게 Vaswani et al.와 ‘The Annotated Transformer’과 같은 훌륭한 가이드를 나타냄

- 이 작업에서는 많은 레이어를 L과 같이 나타내고, hidden size를 H, self-attention heads의 개수를 A로 나타냄

우리는 주로 이 두 모델 사이즈에 대한 결과를 보고함 : 

- BERTBASE (L=12, H=768, A=12, Total Parameters=110M)
- BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)

BERTBASE는 비교 목적으로 OpenAI GPT와 같은 모델 사이즈를 가지기 위해 선택됨

그러나 비판적으로, BERT 트랜스포머는 양방향 셀프 어텐션을 사용하는 반면 GPT 트랜스포머는 constrained self-attention을 사용, 이는 모든 토큰이 오직 그의 왼쪽에 있는 문맥에만 attend할 수 있음

### ***Input/Output Representations***

- BERT가 다양한 downstream 작업을 수행하게 하도록 하기 위해서는, input representation이  하나의 토큰 시퀀스 안에서 단일 문장과 문장 쌍(질문, 답변)을 명확하게 나타낼 수 있음
- 이러한 작업 전체에서, 실제 ‘문장’은 실제 언어적 문장이 아니라 연속된 텍스트의 임의의 span이 될 수 있음
- ‘시퀀스’는 BERT의 input 토큰 시퀀스를 나타내고, 단일 문장 혹은 함께 묶인 두 문장 일 수 있음
- 30000 토큰 단어가 있는 WordPiece 임베딩 사용
- 모든 시퀀스의 첫 토큰은 항상 special classification token인 ***[CLS]*** 임
- 이 토큰을 가리키는 마지막 히든 스테이트는 분류 작업을 위한 시퀀스 표현 집계를 위해 사용됨
- 문장 쌍은 같이 묶여 하나의 시퀀스임
- 문장들에 두 방법으로 차이를 줌
    - SEP이라는 special token으로 구분함
    - 모든 토큰에 그것이 문장 A에 속하는지 B에 속하는지를 가리키는 학습된 임베딩을 추가
    - *Figure1* 그림에서 보았듯이, input embedding을 $E$로, special 토큰인 ***[CLS]***의 마지막 히든 벡터를 $C$ 로, $i^{th}$ input token의 마지막 히든 벡터를 $T_i$로 표기
- 특정 토큰에 대해, 해당 토큰의 input representation은 가리키는 토큰, 세그먼트 및 위치 임베딩을 합해 구성됨
- 이러한 구성은 *Figure2*에서 확인할 수 있음

## ***3.1 Pre-training BERT***

- Peters et al. , Radford et al. 과 다르게, 여기서는 BERT를 훈련시킬 때 전통적인 left-to-right이나 right-to-left 언어 모델을 사용하지 않음
- 대신에, 두 개의 비지도 학습을 이용해 pre-train하는데, 이 섹션에서 설명
- 이 스텝은 *Figure1*의 left 파트

### ***Task #1: Masked LM***

- 직관적으로, 깊은 양방향 모델은 다른 left-to-right모델 혹은 left-to-right과 right-to-left의 shallow concatenation 모델보다 철저하게 더 파워풀하다고 믿는것이 합리적
- 안타깝게도, 표준 조건부 언어모델은 left-to-right 혹은 right-to-left 로만 훈련 가능, 양방향 조건화를 사용하면 각 단어가 간접적으로 ‘see itself’(자기 자신을 보는 것) 하는 것을 허용하고, 다층 구조에서 대상 단어를 간단하게 예측 가능하기 때문
- 자기 자신을 마스킹하더라도 다층 구조에서는 간접적으로 자기 자신에 대한 정보를 알 수 있음
- 심층적인 양방향 표현을 훈련하기 위해, 단순히 input tokens의 일부 퍼센트를 랜덤하게 마스킹하고, 마스킹된 토큰을 예측
- 우리는 이러한 과정은 masked LM, 즉 MLM이라 부름 (문헌에서 Cloze task로 종종 불리기도 하지만)
- 이러한 상황에서, 마스킹된 토큰에 해당하는 마지막 히든 벡터는 표준 언어 모델과 같이 어휘에 대한 출력 소프트 맥스에 입력됨
- 모든 실험에서, 각 시퀀스의 모든 WordPiece 토큰 중 15%를 마스킹함
- denoising auto-encoders와는 대조적으로, 우리는 전체 input을 재구성하는 대신 masked words만을 predict함

- 이는 우리에게 양방향 pre-trained model을 얻는 것을 허용하지만, 단점은 [MASK] 토큰이 fine-tuning때는 나타나지 않기 때문에 pre-training과 fine-tuning의 mismatch
- 이를 해결하기 위해서는, masked 토큰을 항상 [MASK]로 표시하지는 않음
- training data generator가 prediction을 위한 15%의 토큰 포지션을 랜덤하게 선택
- 만약 $i^{th}$ 토큰이 선택되었다면, 80%는 (1) [MASK]를, 10%는 random token을, 10%는 unchanged  $i^{th}$로 대체
- 그러고나면 $T_i$는 cross entropy loss와 함께 원래의 토큰을 예측하는데 쓰임
- 부록에서 이 절차의 variation을 비교

### ***Task #2: Next Sentence Prediction (NSP)***

- Question & Answering, Natural Language Inference와 같은 많은 중요한 과제들은 두 문장 사이의 관계를 이해하는 것에 기초를 둠, 이는 언어 모델링으로 직접적으로 포착되지 않음
- 문장 관계를 모델이 이해할 수 있도록 훈련하기 위해서는, 모든 단일 언어 corpus에서 간단히 생성될 수 있는 이진화된 *next-sentence-prediction* 과제를 pre-train함
- 구체적으로, 각 훈련 예시로 문장 A와 B중에 고를 때, 50%는 B가 실제 A의 다음 문장이고(IsNext로 라벨링됨), 50%는 코퍼스로부터의 랜덤 문장 (NotNext로 라벨링됨)
- *Figure1*에서 보았듯이, $C$는 NSP(Next Sentence Prediction)에 사용됨
- 이렇게 단순함에도 불구하고, pre-training은 QA와 NLI 모두에게 매우 유익하다는 것을 $5.1$에서 입증

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/1fd042a1-589b-4bf3-9e8d-8923ba06faa1/Untitled.png)

*Figure2. BERT input representation, input 임베딩은 token embedding + segment embedding + position embedding의 합으로 구성됨*

- NSP 과제는 Jernite et al.과 Logeswaran and Lee에서 사용된 representation-learning objectives에 밀접한 연관 존재
- 그러나, 이전의 작업들은 오직 문장 임베딩만 downstream task로 트랜스퍼됨, BERT는 모든 파라미터를 트랜스퍼하여 end-task(최종작업) 모델 파라미터를 초기화

### ***Pre-training data***

- pre-training 프로시저는 대부분 문헌에 존재하는 언어 모델 pre-training을 따름
- pre-training 코퍼스로는 BooksCorpus(800M words)와 English Wikipedia (2500M words) 사용
- 위키피디아에서는 텍스트 문단만 추출하여 사용하고 list, table, header는 무시
- 긴 연속 시퀀스를 추출하는데는 Billiion Word Benchmark처럼 shuffled sentence-level 코퍼스보다 document 레벨 코퍼스를 사용하는 것이 중요

### ***3.2 Fine-tuning BERT***

트랜스포머의 self-attention 매커니즘이 BERT가 적절한 입력과 출력을 교체해가며 많은 downstream 작업들을 모델링하게끔 (그 작업이 단일 텍스트 혹은 텍스트 쌍을 포함하는지 아닌지는 관계 없이) 하기 때문에 fine-tuning은 간단함

- 텍스트 쌍 관련 경우의 적용에 대해, 흔한 패턴은 Parikh et al.처럼 텍스트 쌍을 각각 독립적으로 인코딩 하는 방법이었
- BERT는 대신에 self-attention 매커니즘을 이 두 단계를 통합하는데 사용, self-attention으로 연결된 텍스트 쌍을 인코딩하면 두 문장 간 bidirectional cross attention이 효과적으로 포함되기 때문

- 각각의 태스크에 대해, 간단하게 task-specific inputs과 output을 BERT에 plug in 하고 모든 파라미터를 end-to-end로 fine-tuning
- input에서 pre-training의 문장 A와 문장 B는 (1) sentence pairs in paraphrasing, (2) hypothesis-premisepaiers in entailment, (3) question-passage pairs in question answering, (4) a degenerate text pair in text classification or sequence tagging과 유사
- output에서, 토큰 representation은 시퀀스 태깅이나 질의응답과 같은 토큰 레벨 태스크를 위한 아웃풋 레이어로 들어가고, [CLS] representation은 entailment 혹은 감성분석과 같은 classification을 위한 아웃풋 레이어에 들어감
- pre-training과 비교해서, fine-tuning은 상대적으로 저비용
- paper의 모든 결과가 완전히 똑같은 pre-trained 모델 대상으로 최대 1시간 이내로 single cloud TPU, 몇 시간 내 GPU내에 복제됨
- task-specific에 대한 디테일은 섹션 4에서 설명, 더한 디테일은 어펜딕스 A5에서 설명

## ***4 Experiments***

여기서는 11개 NLP task에서의 BERT의 fine-tuning 결과를 나타냄

### ***4.1 GLUE***

- The General Language Understanding Benchmark
- 다양한 자연어 이해 과제의 집합
- GLUE의 디테일한 설명은 어펜딕스 B.1에 존재
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/18391cf4-04e4-4886-b2f0-e76926e72624/Untitled.png)
    

*Table 1*  GLUE Test 결과, evaluation server로부터 저장됨

각 태스크명 밑의 숫자는 training sample의 개수를 의미, Average 컬럼의 경우 오피셜 GLUE보다 살짝 다름, WNLI set을 제외했기 때문

BERT와 OpenAI GPT는 싱글 모델, 멀티 태스크임

F1 Score는 QQP와 MRPC를 위해 보고되었고, Spearman correlations는 STS-B를 위해, accuracy scores는 다른 나머지 태스크들을 위해 보고됨

BERT를 구성요소중 하나로 사용하는 엔트리는 제외됨

- GLUE dataset에서 fine-tuning 하기 위해서는, input 시퀀스(단일 문장 혹은 문장 쌍)를 section 3에서 묘사한 것처럼 표현 (토큰 임베딩을 사용하여 각 단어를 숫자 벡터로, segment, token, position 임베딩이 합쳐진 형태)
- 모델이 처리한 후 나온 첫 번째 input 토큰([CLS])에 해당하는 최종 히든 벡터 $C$를 aggregate representation으로 사용 ⇒ 모델이 해당 입력 시퀀스를 이해하고 종합하여 얻은 표현, 해당 시퀀스의 의미를 대표하는 정보를 담고 있음
- fine-tuning 중 도입되는 새로운 파라미터는 오직 classification layer weights 이며, K는 label의 개수
- $C$와 $W$로 표준 classification loss 계산 (ex. $log(softmax(CW^T))$
- batch size=32, fine-tune for 3epochs over the data for all GLUE tasks
- 각 태스크마다, 베스트 fine-tuning learning rate (among 5e-5, 4e-5, and 2e-5) on the Dev set
- 더불어, BERT large 같은 경우 fine-tuning이 small dataset에 대해 불안정하다는 것을 발견
- 그래서 random restarts를 여러 번 하고, 개발 세트에서 가장 베스트 모델을 선택
- random restarts를 통해, 동일한 pre-trained 체크포인트를 사용했지만 다른 fine-tuning 데이터를 섞고 (모델이 특정한 순서에 의존하지 않고 일반적인 패턴을 학습하기 위함) classifier layer initialization 진행(fine-tuning 과정에서 추가되는 classifier layer 초기화를 의미, 이 레이어는 특정 작업을 수행하기 위해 fine-tuning됨
- ⇒ random restart를 통해 같은 사전 훈련된 모델을 사용하지만, 다양한 파인튜닝 데이터를 섞고 분류기 레이어를 초기화하여 다양한 초기 조건에서 모델을 학습시킨다는 것을 나타냄

- 결과는 *Table 1에 나와있음*
- BERT base와 BERT large는 모든 시스템과 모든 태스크에서 상당한 차이로 능가, 이전 기술에 비해 각각 4.5%와 7%의 정확도 상승
- BERT base와 OpenAI GPT는 attention masking만 제외하고 보면 모델 아키텍처 측면에서 거의 동일
- 가장 크고 가장 널리 reported된 GLUE task, MNLI 의 BERT는 4.6%의 정확도 향상을 보여줌
- 공식 GLUE 리더보드를 보면, BERT large는 80.5의 스코어를 획득, OpenAI는 72.8 획득

- BERT large가  모든 태스크에서 BERT base를 능가했고, 특히 training 데이터가 굉장히 작을 경우.
- 모델 사이즈 효과는 섹션 $5.2$에서 더 자세히 다루었음

## ***4.2 SQuAD v1.1***

- The Stanford Question Answering Dataset
- 100k crowd-sourced question/answer pairs
- 질문과 답이 담긴 Wikipedia passage가 주어지고, 태스크는 passage안에서 answer text 범위를 예측
- 모델이 주어진 문맥과 질문에 대해 답변 스팬의 시작과 끝을 예측하는 방법을 나타냄
- 시작 지점과 끝 지점에 대한 예측은 각각 질문벡터와 문맥 벡터의 내적을 사용하여 계산됨
- 그런다음 가능한 스팬 중에서 점수가 가장 높은 스팬이 답변으로 선택됨

*Figure1*에서 보았듯이,  질문 답변 태스크에서는 input으로 question과 passage를 단일 패킹 시퀀스로 표현, 질문은 A 임베딩을 쓰고 passage는 B 임베딩을 사용

- fine-tuning에서 start vector $S$ 와 end vector $E$ 만을 도입
- word $i$가 답변 범위의 시작이 될 확률은 $T_i$ 와 $S$의 내적과 이후에 단락의 모든 단어에 대한 소프트맥스로 계산
- 
    - 내적 결과에 $e$의 지수함수를 적용, 이를 모든 단어에 대해 합산한 값을 사용하여 softmax 계산, 이렇게 함으로써 각 단어가 시작 지점일 확률을 얻게 됨
    - 이전 단계에서 계산한 softmax 확률값을 각각의 단어에 대해 계산하여 각 단어가 시작지점일 확률을 얻음
    - 여기서 $i$는 특정 단어를 나타내며, $j$는 모든 단어를 나타냄

$$
P_i = \frac{e^{S\cdot T_i}}{\sum_je^{S\cdot T_j}}
$$

- 유사한 식이 답변 스팬의 끝지점에도 사용됨
- position $i$ 부터 position $j$ 까지 후보 스팬이 주어진 경우, i와 j 사이의 점수를 계산

$$
S\cdot T_i + E\cdot T_j
$$

질문벡터와 시작 위치의 문맥 벡터, 그리고 답변 끝벡터와 끝위치의 문맥벡터의 내적으로 계산됨

- 모든 가능한 스팬에 대해 점수를 계산한 후, j가 i보다 크거나 같은 조건을 만족하는 스팬 중에서 가장 높은 점수를 가진 스팬을 예측값으로 선택 ⇒ 모델이 예측한 답변 스팬의 범위

- 학습 목표는 올바른 시작 지점과 끝 지점의 로그 우도의 합
- 모델이 주어진 문장에서 올바른 답변의 시작과 끝을 예측하는 능력을 향상시키는데 사용됨
- 로그 우도는 모델의 예측값이 실제값에 얼마나 가까운지를 측정하는 지표
- fine-tuning을 3 epochs와 5e-5의 learning rate, batch size 32 사용

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/2d50116b-6b45-4315-a8e4-fc3d646903a2/Untitled.png)

*Table 2: SQuAD 1.1 결과, BERT ensemble은 다른 pre-training checkpoints와 fine-tuning seed를 사용하는 7x systems*

- 총 7개의 다른 시스템이 사용되었음을 의미, 각 시스템은 독립적으로 작동하며 서로 다른 설정이나 초기화 방법을 사용
- *Table2*는 top published systems로부터의 결과와 함께 top leaderboard entries를 보여줌
- SQuAD  리더보드의 상위 결과들은 사용 가능한 최신의 공개된 시스템 설명이 없으며 그들의 시스템을 훈련시킬 시 모든 공개 데이터 사용 가능
- 따라서 처음으로 TriviaQA에서 fine-tuning 후 SQuAD 데이터셋에서 미세조정
- TriviaQA는 다양한 도메인과 주제를 다루는 데이터셋으로, 이를 먼저 학습시키면 다양한 주제에 대한 지식을 획득 가능
- 트랜스퍼 러닝을 이용해 SQuAD 데이터셋에서 미세조정함으로써 모델이 특히 질문응답 작업을 수행하는 데 더 특화된 능력을 갖게 됨

- 가장 최적의 결과를 보여준 시스템은 top leaderboard 시스템을 앙상블을 사용하여 F1스코어가 1.5 높고, 단일 시스템으로는 1.3 더 높음
- 즉 앙상블로도 뛰어난 성능을 보이며, 개별적으로도 상위 리더보드 시스템보다 우수하다는 것을 보여줌
- TriviaQA 데이터 없이는 0.1에서 0.4 저옫의 점수를 잃지만, 그렇더라도 여전히 큰 마진으로 존재하는 시스템들을 능가

## ***4.3 SQuAD v2.0***

- SQuAD v2.0 과제는 SQuAD 1.1에서의 문제의 정의를 확장하여 추가적인 가능성 고려, 즉 간결한 답변이 없을 수 있다는 것을 나타냄, 이는 문제에 좀 더 현실적인 상황을 더 반영한 것
- 간단히 SQuAD v1.1 BERT 모델을 확장시키는 접근 방법을 사용
- 질문에 대한 답이 존재하지 않는 질문에 대해서는 answer span이 [CLS]에서 시작 및 종료되는 답변범위를 갖는 것으로 처리됨
- 시작과 끝의 answer span 위치에 대한 확률 공간은 [CLS] 토큰을 포함하기 위해 확장됨
- 예측을 위해서는, 답변이 없는 범위의 점수: $S_{null} = S\cdot C + E\cdot C$    # 답변이 없는 경우의 점수를 계산
- $S_{null}$과 가장 좋은 비-빈번한(non-null) 답변 범위의 점수: $S_{\hat i,j} = max_{j>=i} S\cdot T_i + E \cdot T_j$
    - 여기서 T는 다른 답변 범위에 대한 표현을 나타냄, 여기서는 모든 가능한 답변 범위 중에서 가장 높은 점수를 찾음
- 우리는 $S_{\hat i,j} > S_{null} + \tau$ 일 때 non-null 답변이라고 예측, threshold인 $\tau$가 F1 스코어를 최대화하기 위해 개발셋에서 선정됨
- $\tau$ 는 개발셋에서 선택되며, 모델의 예측을 조정하는 역할을 함
    - 예를 들어, $\tau$가 크면 답변을 예측하는 기준이 더 엄격해지고, 작으면 더 널널하게 예측 가능 ⇒ 모델의 성능을 최적화
- TriviaQA는 이 모델에서 사용하지 않음
- 이로써 모델은 답변이 있는 경우와 없는 경우를 구별하고, 적절한 답변을 예측하도록 함
- $\tau$는 모델의 예측을 조정하는 매개변수로서, 성능 최적화 가능
- 2 epochs, learning rate 5e-5, batch size 48

## ***4.4 SWAG***

- The Situations With Adversarial Generations
- 이 데이터셋은 현실적이고 실질적인 상식 추론을 평가하는 113k개의 문장 쌍 완성 예시를 포함
- 하나의 문장이 주어지면, 과제는 네 가지 선택지 중 가장 그럴듯한 연속적인 답변을 선택하는 것

- SWAG 데이터셋에서 fine-tuning을 할 때, 네 개의 입력 시퀀스를 구성
- 각각의 시퀀스는 주어진 문장(sentence A)과 가능한 연속적 문장(sentence B)의 concatenation을 포함, 이 두 부분이 연결되어 하나의 입력으로 사용됨
- 도입된 task-specific 파라미터는 오직 [CLS] 토큰 representation과 내적되는 벡터
- 내적 연산으로 얻은 결과는 소프트맥스 레이어로 정규화되며, 이 연산의 결과는 각 선택지에 대한 score를 나타냄
    - 소프트맥스 함수는 다양한 선택지들 간의 상대적인 중요도를 나타내는 확률분포로 변환

⇒ 즉, [CLS] 관련 토큰과 관련된 벡터를 사용하여 각 선택지의 점수를 계산하고, 이 점수를 소프트맥스 함수를 통해 확률분포로 변환

![*Table 4. SWAG Dev and Test accuracies, 인간 능력은 100개의 샘플을 가지고 측정되었다.*](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/b521a6a7-0bda-4ca2-8c20-83796696ceb4/Untitled.png)

*Table 4. SWAG Dev and Test accuracies, 인간 능력은 100개의 샘플을 가지고 측정되었다.*

- 3 epochs, learning rate 2e-5, batch size 16
- BERT large가 ESIM + ELMo 시스템을 27.1%, Open AI GPT는 8.3%를 앞서며 능가

## ***5 [Abalation Studies](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)***

BERT 모델의 여러 구성 요소나 특징들을 제거하여 각각이 모델 성능에 어떤 영향을 미치는지를 실험적으로 조사

![*Table 5. BERT base를 통해 수행한 pre-training task의 abalation결과, No NSP 는 next sentence prediction task 없이 수행한 결과, LTR & No NSP 는 NSP없이 left-to-right LM과 같이 수행한 결과(OpenAI GPT와 유사하게) , “+BiLSTM”은 fine-tuning 중 “LTR + NoNSP” 모델 상단에 랜덤하게 초기화된 [BiLSTM](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)을 추가, 함께 학습됨*](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/060a93b0-f6eb-48bd-b8aa-51d8e8c702ff/Untitled.png)

*Table 5. BERT base를 통해 수행한 pre-training task의 abalation결과, No NSP 는 next sentence prediction task 없이 수행한 결과, LTR & No NSP 는 NSP없이 left-to-right LM과 같이 수행한 결과(OpenAI GPT와 유사하게) , “+BiLSTM”은 fine-tuning 중 “LTR + NoNSP” 모델 상단에 랜덤하게 초기화된 [BiLSTM](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)을 추가, 함께 학습됨*

- abalation studies는 Appendix $C$에서 자세히 볼 수 있음

## ***5.1 Effect of Pre-training Tasks***

BERT의 deep bidirectionality가 왜 중요한지 보여주기 위해 두 가지 pre-training objectives를 평가

BERT base와 동일한 pre-training data, fine-tuning scheme, hyperparameters 사용

**NoNSP:** ‘masked LM(MLM)’으로 훈련된, 그러나 NSP 작업은 포함하지 않은 양방향 모델

**LTR & No NSP:** standard Left-to-right LM을 사용해 훈련된 left-context-only 모델

- 좌측 문맥만을 고려하는 제약도 fine-tuning 과정에 적용됨
- 이 작업을 제거하는 것은 pre-train/fine-tune간의 불일치를 초래하기 때문, 이는 downstream 퍼포먼스를 저해
- 또한, 이 모델은 NSP 작업 없이 pre-train됨
- 이건 직접적으로 OpenAI GPT와 직접적으로 비교 가능, 더 큰 규모의 데이터셋을 사용하고, OpenAI와는 다른 input representation, fine-tuning scheme를 사용

**NSP 태스크로부터의 impact 조사**

- *Table 5*를 보면, NSP를 제거하는 것이 QNLI, MNLI, SQuAD 1.1에서 상당한 퍼포먼스 저하를 일으킨다는 것을 알 수 있음

**bidirectional representations의 impact 조사**

- No NSP와 LTR & NoNSP를 비교
- LTR 모델은 모든 태스크에서 MLM 모델보다 성능이 좋지 않음, 특히 MRPC와 SQuAD에서 성능이 더 떨어짐
- SQuAD 데이터셋에 관해서는 LTR 모델이 토큰 예측에서 성능이 좋지 않다는 것이 직관적으로 분명함
- 토큰 레벨 hidden state가 오른쪽 문맥을 보지 못하기 때문
- LTR 시스템을 강화하기 위해, 무작위로 초기화된 BiLSTM을 상단에 추가
- 이것은 SQuAD에서의 성능을 큰 폭으로 향상시켰으나, 결과는 bidirectional 모델에 비해서는 여전히 훨씬 떨어짐
- BiLSTM은 GLUE 태스크에서의 성능을 저해

⇒ LTR이 토큰 예측에서 우측 컨텍스트를 보지 못하기 때문에 좋은 성능을 내기 어려움, 그래서 성능을 높이기 위해 BiLSTM 추가, 성능이 올라가긴 했지만 여전히 bidirectional 모델보다는 낮고 또 BiLSTM이 GLUE 태스크에서의 성능을 저하시킴

ELMo와 비슷하게 LTR과 TRL 모델을 각각 훈련시키고 두 모델의 각 토큰을 연결하여 representation 생성도 가능하겠다, 그러나 

(a) 단일 bidirectional 모델보다 2배 더 고비용

(b) QA와 같은 태스크에는 직관적이지 않음, RTL 모델이 question을 제대 파악하지 못할 것이기 때문

(c) 이것은 deep bidirectional 모델보다 철저히 덜 파워풀할 것, 왜냐하면 bidirectional model은 left와 right context를 모든 레이어에서 사용할 수 있기 때문

## ***5.2 Effect of Model Size***

모델 사이즈에 따른 fine-tuning accuracy를 알아봄

레이어, hidden units, attention heads의 수를 다르게 하는 동시에 이전에 묘사했던 동일한 하이퍼 파라미터와 trianing procedure를 유지하면서 다양한 경우의 수를 실험

- GLUE 태스크의 결과는 *Table 6*

![*Table 6. Abalation over BERT 모델 사이즈, #L = 레이어 개수, #H = hidden size, #A = attention head 개수. LM(ppl)은 masked LM [perplexity](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21) of [held-out training data](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)*](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/1459f57b-47f5-4739-9ecc-3169de564c1a/Untitled.png)

*Table 6. Abalation over BERT 모델 사이즈, #L = 레이어 개수, #H = hidden size, #A = attention head 개수. LM(ppl)은 masked LM [perplexity](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21) of [held-out training data](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)*

- Dev set에 대한 fine-tuning에서 5 random restarts로 얻어낸 결과의 평균을 기록
- larger 모델이 모든 4개 데이터셋에서 엄격한 성능 향상을 이끄는 것을 볼 수 있음, 오직 3600개의 labeled 훈련 샘플을 가진 MRPC에서도 성능 향상됨, 그리고 해당 데이터셋은 pre-train 태스크와 실질적으로 다름
- 기존 문헌에 비해 이미 상당히 큰 모델 위해 이러한 중요한 향상을 이루어낼 수 있다는 것이 놀라운 점
    - 이미 상당히 큰 모델에 대해 추가적인 중요한 향상을 이끌어낼 수 있다는 사실이 놀라움
    - 즉, 모델이 이미 큰데도 더 큰 모델을 사용함으로써 더 좋은 성능을 얻을 수 있었음
- Vaswani et al. (L=6, H=1024, A=16) with 인코더에 100M parameters, 그리고 문헌 중 찾은 가장 큰 모델은 (L=64, H=512, A=2) with 235 parameters
- 대조적으로, BERT base는 110M의 파라미터를 가지며 BERT large는 340M의 파라미터를 가짐
- 기계 번역과 언어 모델링과 같은 large-scale task에서 모델 사이즈를 늘리는 것이 연속적인 성능 향상으로 이어진다는 것은 table 6에서 보인 것과 같이 오래동안 알려졌던 사실
- 그러나, 아주 작은 규모의 작업에서도 모델의 극적인 확장이 (모델 사전 훈련이 충분히 되었다면) 큰 성능 향상을 가져올 수 있다는 것을 이 연구가 처음으로 확신
- Peters et al.은 downstream task에서 pre-trained bi-LM 사이즈를 2에서 4 레이어로 늘리는 것에 대한 혼합된 결과를 보여줌
- Melamud et al. 에서는 200에서 600으로 늘어난 hidden dimension 사이즈를 통과하는 것이 도움이 되었으나, 1000개 이상으로 늘리는 것은 더 이상 성능 향상에 도움이 되지 않았다는 것을 보여줌
- 이러한 사전 작업들은 feature-based 접근 방법, 우리는 모델이 downstream task에서 직접적으로 fine-tuning 되고, 아주 작은 수의 무작위로 초기화된 추가적인 파라미터를 사용하면, task-specific 모델은 downstream task 데이터가 아주 작을 경우에도 더 크고, 더 표현력이 높은 pre-trained representations에서 이점을 누릴 수 있다고 가정

## 5.3 *Feature-based Approach with BERT*

현재까지 제시된 모든 BERT 결과는 전부 fine-tuning approach 사용

간단한 classification layer이 pre-trained model에 추가된 형태, 그리고 해당 레이어에서 모든 파라미터가 전체적으로 downstream task 위에서 fine-tuned

그러나, feature-based approach에서는 사전 훈련된 모델에서 고정된 feature를 추출, 이에는 특정한 장점이 존재

- 첫째로, 트랜스포머 아키텍처로 모든 태스크들이 쉽게 표현되지 않을 수 있으며, 그래서 task-specific한 아키텍처를 추가해야할 수 있음 (즉, 일부 작업은 BERT만으로 표현하기 어려울 수 있고 이를해결하기 위해 작업에 특화된 모델  아키텍처를 추가해야 할 수 있음)
- 둘째로, 훈련 데이터의 고비용의 representation을 한번 pre-compute해놔서 [계산상의 이점](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)을 가져오고, 이 사전 계산된 표현을 기반으로 다른 여러 저비용 모델에 대해 다양한 실험 가능

이 섹션에서는 CoNLL-2003, [Named Entity Recognition 태스크(NER)](https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-817258dea3b342418cf92c39b791f948?pvs=21)에 BERT를 적용하는 두 가지 접근 방식에 대해 비교

- BERT의 input에는, case-preserving WordPiece 모델을 사용
    - 이 모델은 단어를 작은 조각들로 나누어 처리
    - 데이터에서 제공된 최대 문맥 포함 (⇒ 주어진 가능한 한 많은 문서 컨텍스트를 포함한다는 의미)
- 표준 관행에 따라, 일반적으로 tagging task로 정의됨
- CRF 레이어는 출력에 사용하지 않음
    - Conditinal Random Field layer, 시퀀스 레이블링 작업에 사용되는 통계적 모델
- 첫 번째 서브 토큰의 표현을 사용하여 NER 레이블 세트에 대한 token level 분류기 형성
    - 개체명의 경계를 예측하는 작업을 의미

fine-tuning approach의 제거를 위해, BERT의 어떠한 매개변수도 fine-tuning 하지 않으면서 하나 혹은 그 이상의 레이어에서 activations를 추출해내어 feature-based approach를 적용

- 이렇게 추출된 문맥 임베딩은 무작위로 초기화된 두 개의 레이어로 이루어진 768차원의 BiLSTM의 입력으로 사용되며, 이후 classificaiton layer로 전달됨

결과는 *Table 7에 있음*

![*Table 7. CoNLL-2003 NER results. 하이퍼파라미터는 Dev set을 사용하여 선택됨. 보고된 Dev 그리고 Test score는 해당 파라미터를 사용하여 5 random restarts을 통해 평균화됨*](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/9423f12d-7835-4236-8121-9d5d3d281b47/Untitled.png)

*Table 7. CoNLL-2003 NER results. 하이퍼파라미터는 Dev set을 사용하여 선택됨. 보고된 Dev 그리고 Test score는 해당 파라미터를 사용하여 5 random restarts을 통해 평균화됨*

BERT large는 최신 기술을 적용한 방법들과 경쟁력을 갖추고 있음, 즉 유사하게 우수함

- 성능이 가장 우수한 방법은 사전 훈련된 트랜스포머의 4개의 상위 hidden layers의 토큰 representation을 concat하는 방법으로, 전체 모델을 미세 조정하는 것보다 0.3 F1 스코어만이 뒤떨어짐
- 이는 BERT가 fine-tuning과 feature-based approach 모두에서 효과적임을 보여줌

# ***Conclusion***

- 언어 모델을 사용한 transfer learning으로 인한 최근의 경험적 개선은,  풍부한 비지도 사전 학습이 많은 언어 이해 시스템에서 필수적인 부분이 되었다는 것을 입증했음
- 특히, 이 결과들은 적은 리소스의 태스크까지 심층적인 단방향 아키텍처로부터 이익을 누릴 수 있다는 것을 보여줌
- 여기서 주요 기여는 이러한 결과물을 깊은 양방향 아키텍처로 일반화하여 동일한 사전학습된 모델이 더 광범위한 NLP 태스크를 성공적으로 수행할 수 있도록 하는 것

# ***Appendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”***

3 가지의 Appendix로 나뉨

- A: BERT에 관한 수행 디테일
- B: 실험에 대한 추가적인 detail
- C: 추가적인 ablation studies for BERT
    - training steps의 효과
    - Abalation for Different Masking Procedures

# ***A Additional Details for BERT***

다음은 pre-training task의 예시

### ***Masked LM and the Masking Procedure***

unlabeled sentence가 *my dog is hairy* 라고 가정, 랜덤 마스킹 과정동안 우리는 4th 단어(hairy)를 고름, 마스킹 과정은 이렇게 묘사 가능

- 80%: [MASK] 토큰에 의해 대체 ⇒ *My dog is hairy  → My dog is [MASK]*
- 10%: random 단어에 의해 대체 ⇒ *My dog is hairy → My dog is apple*
- 10%: 그대로 놔둠 ⇒ *My dog is hairy → My dog is hairy*
    - 이것의 목적은 표현을 실제 관측된 단어 쪽으로 편향시키는 것
    - 모델이 학습 과정에서 특정 단어를 실제 문장에서 나온 대로 정확하게 파악하도록 유도한다는 의미
    - 모델이 실제로 나온 단어의 의미와 역할을 올바르게 이해할 수 있도록 하는 것
    - 만약 모든 단어를 가리게 된다면 모델은 실제로 나온 단어의 위치나 역할을 파악할 수 없게 됨

- 이러한 과정의 장점은 트랜스포머 인코더가 어떤 단어를 예측하게 될 지, 어떤 단어가 랜덤 단어로 대체되었는지 모르기 때문에 모든 입력 토큰에 대한 분포적 문맥 표현을 유지하도록 강제됨
- 즉, 모델은 모든 단어에 대한 문맥을 보존하며 예측을 수행해야 함
- 추가적으로, 랜덤 대체는 전체 토큰에서 오직 1.5%에서만 일어나기 때문에, (마스킹 비율 전체 15%, 그 중에서 10%) 모델의 언어 이해 능력을 저하시키지 않음

standard language model training과 비교해보면, masked LM은 각 배치에서 15%만 예측이 일어나기 때문에, 모델이 수렴하기 위해 더 많은 pre-training steps가 필요할 수 있음

섹션 $C.1$에서 MLM이 left-to-right 모델에 비해 수렴이 약간 느리다는 것을 봤는데(모든 토큰을 예측한다고 했을 때), MLM의 경험적인 개선이 모델의 훈련 비용 증가보다 훨씬 크다는 것을 보여줌

### ***Next Sentense Prediction***

NSP 태스크는 다음과 같은 예시로 진행됨

- Input = [CLS] the man went to [MASK] store [SEP]  |  he bought a gallon [MASK] milk [SEP]
- Label = IsNext

- input = [CLS] the man [MASK] to the store [SEP]  |  penguin [MASK] are flight ##less birds ( SP)
- Label = NOtNext

## ***A.2 Pre-training Procedure***

각 training input 시퀀스를 생성하기 위해서는, 말뭉치로부터 두 개의 텍스트 범위를 샘플링 하는데, 일반적으로 단일 문장보다는 길지만 sentences라고 함( 물론 더 짧을 수도 있음)

- 첫 번째 문장은 A 임베딩을 받고 두 번째 문장은 B 임베딩을 받음
- 50%는 B가 실제 A의 다음 문장이고 나머지 50%는 랜덤한 문장, 이는 NSP를 위한 것
- 이 두 문장은 길이가 합쳐서 512 토큰 이하가 되도록 샘플링됨
- 언어모델 마스킹은 WordPiece 토큰화 이후에 적용되며, 15%의 균일한 마스킹 비율이 사용되며 부분 단어 조각에 대한 특별한 고려는 없음

- 256 시퀀스의 배치 사이즈 사용 (256 시퀀스 * 512 토큰 = 128,000 토큰/배치 - 한 배치당 토큰 수), 1,000,000 스텝동안 학습 진행, 전체 데이터셋인 3.3 billion 단어 말뭉치를 약 40 epoch에 걸쳐서 훈련 (약 40번 반복하여 학습)
- Adam 사용, learning rate 1e-4, $\beta_1 = 0.9, \Beta_2 = 0.999, L2$ weight decay of 0.01, learning rate warmup over the first 10,000 steps, linear decay of the learning rate
- dropout probability 0.1 on all layers, gelu activation 사용(OpenAI GPT와 동일)
- training loss는 mean masked LM likelyhood 그리고 mean next sentence prediction likelyhood 사용

- BERT base 모델은4 Cloud TPU in Pod configuration(16 TPU chips total)
- BERT large 모델은 16 Cloud TPUs (64 TPU chips total)
- 각 pre-training은 4일이 걸림

- Longer sequence는 불균형하게 가격이 높음, 왜냐하면 어텐션 매커니즘이 시퀀스 길이에 비례하여 제곱적으로 증가하기 때문
- 따라서 시퀀스가 2배 길어진다고 2배 비싼것이 아니라 4배가 비싸짐
- 실험에서 사전 훈련의 속도를 높이기 위해서, 90%의 pre-training에서는 시퀀스의 길이를 128로 설정하여 진행
    - 모델이 좀 더 빠르게 초기학습을 할 수 있도록 도움
- 그리고 나머지 10%는 시퀀스 길이를 512로 늘려서 위치 임베딩(positional embeddings)을 학습
- 위치 임베딩은 문장 내 단어의 상대적인 위치 정보를 나타냄, 이것은 모델이 긴 시퀀스에서도 문맥을 이해할 수 있도록 도와줌

## ***A.3 Fine-tuning Procedure***

대부분의 모델 하이퍼파라미터는 pre-training에서와 동일함, 배치사이즈, learning rate, training epoch 같은 예외를 제외하면 

- dropout probability는 항상 0.1
- 최적 하이퍼파라미터 값은 task-specific, 그러나 모든 태스크에 걸쳐 잘 작동하는 value range는 다음과 같음
    - Batch size: 16,32
    - Learning rate(Adam): 5e-5, 3e-5, 2e-5
    - Number of epochs: 2,3,4
- 또한 큰 데이터셋(100k+ labeled training example)은 하이퍼 파라미터 선택에 있어 작은 데이터셋보다 덜 민감함
- Fine-tuning은 일반적으로 매우 빠르고, 그래서 앞서 언급한 파라미터들의 가능한 모든 경우를 시도해보고, 개발용 데이터셋에서 가장 좋은 성능을 보이는 모델을 선택하는 것이 합리적

## ***A.4 Comparison of BERT, ELMo, and OpenAI GPT***

여기서는 최근 유명한 representation learning model인 ELMo, OpenAI GPT 그리고 BERT에 대해 알아봄

모델 아키텍처에 대한 시각적인 비교는 *figure 3*에 있음

![*Figure 3.* Pre-training 모델 아키텍처의 차이점. BERT는 bidirectional Transformer이용. OpenAI GPT는 left-to-right Transformer 이용. ELMo는 left-to-right과 right-to-left LSTMs를 각각 훈련시켜 concatenation해서 downstream task에 관한 feature를 생성. 셋 중에서, 오직 BERT representations만이 모든 레이어에서 좌측 및 우측 문맥에 공동으로 의존함, 즉, BERT는 각 레이어에서 양방향 문맥을 모두 고려함. 아키텍처 차이점에 덧붙여서, BERT와 OpenAI GPT는 fine-tuning approach이고, ELMo는 feature-based approach임.](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/0b8ecb26-e7ae-4f7e-afd4-837a1ea86ebb/Untitled.png)

*Figure 3.* Pre-training 모델 아키텍처의 차이점. BERT는 bidirectional Transformer이용. OpenAI GPT는 left-to-right Transformer 이용. ELMo는 left-to-right과 right-to-left LSTMs를 각각 훈련시켜 concatenation해서 downstream task에 관한 feature를 생성. 셋 중에서, 오직 BERT representations만이 모든 레이어에서 좌측 및 우측 문맥에 공동으로 의존함, 즉, BERT는 각 레이어에서 양방향 문맥을 모두 고려함. 아키텍처 차이점에 덧붙여서, BERT와 OpenAI GPT는 fine-tuning approach이고, ELMo는 feature-based approach임.

BERT와 가장 비교할만한 pre-training method는 OpenAI GPT인데, 이는 large corpus에서 학습시킨 left-to-right Transformer LM임

BERT의 많은 디자인 결정이 의도적으로 GPT와 가장 비슷하도록, 그래서 두 방법을 최소한으로 비교할 수 있도록 설계되었음

이 작업의 핵심 주장은 bi-directionality와 section 3.1에서 보여진 두 pre-training 태스크들이 경험적 개선의 대부부을 설명한다는 것

그러나 BERT와 GPT가 훈련된 방법 사이에는 여러 다른 점이 존재

- GPT는 BooksCorpus(800M words)로 훈련됨 ; BERT는 BooksCorpus와 Wikipedia(2500M)로 훈련됨
- GPT는 sentence seperator([SEP])과 classifier token([CLS]) 사용, 이들은 오직 fine-tuning 에만 사용됨; BERT는 [SEP], [CLS], 문장 A/B 임베딩을 pre-training동안 학습함
- GPT는 배치사이즈 32000의 단어들을 1M step동안 학습함; BERT는 1M step동안 배치사이즈 128,000 단어들을 학습
- GPT는 모든 fine-tuning 실험에서 고정된 learning rate 5e-5 사용; BERT는 Dev Set에서 최적 성능을 보여준 task-specific fine-tuning learning rate 사용

이러한 차이점들의 영향을 분리하여 확인하기 위해서, ablation experiments를 $5.1$에서 진행

개선의 대부분이 두 pre-training task와 이를 통해 가능한 양방향성에서 나온 것임을 보여줌

즉, BERT의 주요 향상 요인은 이 두가지 사전 훈련 작업과 양방향성에서 비롯된 것으로 나타남

## ***A.5 Illustrations of Fine-tuning on Different Tasks***

![*Figure 4 .*](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/fdeff297-42e6-4bc6-a0ff-bf71db769fcd/Untitled.png)

*Figure 4 .*

다양한 작업에 대한 fine-tuning 예시는 *Figure 4*에 나타남

- task-specific model들은 BERT를 사용하고 추가 출력 레이어를 하나 더 붙여 사용, 그래서 제로에서부터 학습해야하는 매개변수의 수를 최소화 가능
- (a)와 (b)는 sequence-level 태스크이고, (c)와 (d)는 token-level 태스크
- figure에서, $E$는 input 임베딩을 나타내고, $T_i$ 는 토큰 $i$의 문맥적 respresentation을 나타내며 [CLS]는 classificaiton output을 위한 special symbol, 그리고 [SEP]은 비연속적 token sequence를 분리하기 위한 special symbol

# ***B Detailed Experimental Setup***

## ***B.1 Detailed Descriptions for the GLUE Benchmark Experiments***

*Table 1*의 GLUE 결과는 https://gluebenchmark.com/ 와 [https://blog](https://blog/).[openai.com/language-unsupervised](http://openai.com/language-unsupervised) 에서 볼 수 있음
GLUE 벤치마크는 아래의 데이터셋을 포함, Wang et al.에서 요약된 디스크립션임

### ***MNLI***

Multi-Genre Natural Language Inference는 large scale, crowdsourced entailment classification task임

문장 쌍이 주어지고, 목표는 두번째 문장이 첫 번째 문장에 대해 수반, 모순, 중립 여부를 판단

### ***QQP***

Quora Question Pairs는 이진 분류 태스크이며, 목표는 Quora의 두 질문이 의미적으로 동일한지 판단하는것

### *QNLI*

Question Natual Language Inference는 Stanford Question Answering Dataset의 다른 버전, 이진 분류 태스크로 convert된 것

positive example은 (질문, 문장)  올바른 답변을 포함하는 쌍이고, negative example은 (질문, 문장) 같은 문단에서 나온 답변을 포함하지 않는 쌍임

### ***SST-2***

The Stanford Sentiment Treebank는 사람의 감정에 대한 주석이 있는 영화 리뷰에서 추출된 문장의 이진 단일 문장 분류 태스크

### ***CoLA***

The Corpus of Linguistic Acceptability는 이진 단일 문장 탐색 태스크이며, 목표는 English sentence가 언어적으로 허용되는지 아닌지를 예측하는 것

### ***STS-B***

The Semantic Textual Similarity Benchmark는 뉴스 헤드라인 및 다른 소스에서 가져온 문장 쌍의 모음

의미론적 측면에서 두 문장이 얼마나 유사한지 1부터 5까지 나타낸 주석이 존재

### ***MRPC***

Microsoft Research Paraphrase Corpu는 온라인 뉴스 소스에서 자동으로 추출된 문장 쌍으로 구성됨, 해당 문장 쌍이 의미적으로 동일한지를 구분하는 사람의 주석을 포함

### ***RTC***

Recognizing Textual Entailment는 MNLI와 유사한 이진 수반 태스크, 그러나 더 적은 training data를 가짐

### ***WNLI***

Winograd NLI는 소규모 자연어 추론 데이터셋

GLUE 웹페이지는 이 데이터셋의 구성에 이슈가 있으며, GLUE에 제출된 모든 훈련된 시스템이 다수를 예측하는 baseline 정확도인 65.1 보다 낮은 성능을 보였다고 지적

따라서 OpenAI GPT에 대한 공정성을 위해 이 데이터셋을 제외할 것

GLUE 제출에 대해서는, BERT 모델은 항상 majority class를 예측함

# ***C Additional Abalation Studies***

## ***C.1 Effect of Number of Training Steps***

![*Figure 5.* Ablation over number of training steps. 이는 미리 훈련된 모델 파라미터를 사용하여 k step동안 fine-tuning 한 후의 MNLI 정확도를 나타낸 것. x축은 k의 값, 즉 x축은 미리 훈련된 모델을 몇 step동안 fine-tuning했는지 나타냄, 그림은 fine-tuning steps에 따른 MNLI의 정확도 변화를 보여줌](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/6c634094-a2b9-4051-9ebb-9d6f75de7a8d/Untitled.png)

*Figure 5.* Ablation over number of training steps. 이는 미리 훈련된 모델 파라미터를 사용하여 k step동안 fine-tuning 한 후의 MNLI 정확도를 나타낸 것. x축은 k의 값, 즉 x축은 미리 훈련된 모델을 몇 step동안 fine-tuning했는지 나타냄, 그림은 fine-tuning steps에 따른 MNLI의 정확도 변화를 보여줌

*Figure 5*는 k step동안 pre-train된 체크포인트로부터 fine-tuning 후의 MNLI의 정확도임

이것은 다음 질문에 대한 대답을 가능하게 해줌

1. Question: BERT는 높은 fine-tuning 정확도를 달성하기 위해 정말 (128,000 words/batch * 1,000,000 steps)만큼의 거대한 pre-training이 필수적이냐?
    
    ⇒ Yes. BERT base는 500k steps를 학습했을 때보다 1M steps를 학습했을 때 MNLI에서 거의 1.0%의 추가적인 정확도를 달성
    
2. Question: MLM pre-training은 LTR pre-training보다 느리게 수렴하는데, 모든 단어가 아니라 오직 15%의 단어만이 각 배치에서 예측되기 때문인가?
    
    ⇒ MLM 모델은 LTR 모델에 비해 약간 느리게 수렴함, 그러나 절대적인 정확도는 거의 즉각적으로 LTR을 능가하기 시작 
    
    즉, MLM 모델은 초기 수렴이 조금 더 느리지만, 정확도 면에서는 LTR 모델을 빠르게 능가
    

## ***C.2 Ablation for Different Masking Procedures***

섹션 $3.1$ 에서, BERT가 MLM 목표로 사전훈련 할 때, 대상 토큰을 마스킹 하기 위해 혼합 전략을 사용했다고 언급

아래는 masking strategies의 영향을 평가하기 위한 ablation 스터디 결과임

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/b2cad055-cc1f-40a8-aaa8-14cbf7da7262/Untitled.png)

*Table 8. Ablation over different masking strategies*

masking strategies는 pre-training과 fine-tuning간의 불일치를 줄이는 것을 목적으로 함, [MASK] 심볼이 fine-tuning stage에 등장하지 않기 때문

- 즉, 모델을 실제 작업에 맞게 fine-tuning할 때는 [MASK] 기호를 사용하지 않으므로, 두 단계 간의 일관성을 유지하기 위해 특별한 전략이 필요하다는 것

MNLI와 NER 둘 모두에 대해 Dev 결과를 보고

NER에 관해서는, 모델이 representation에 대해 조정할 기회가 없기 때문에 불일치가 feature-based 모델에서 증폭될 것이라고 보고 fine-tuning과 feature-based approach 둘 모두에 대해 보고함

- 결과는 *Table 8*에 나와있음
- 테이블을 보면, Mask 는 MLM을 위해 [MASK]심볼을 표시한 토큰을 뜻함
- SAME은 타겟 토큰을 그대로 두었다는 것을 뜻함
- RND는 타겟 토큰을 다른 랜덤한 토큰으로 대체했다는 것을 뜻함
- 왼쪽 파트의 숫자들은 MLM pre-training 중 사용되는 특정한 전략의 확률을 의미 (80%, 10%, 10%)
- 오른쪽은 Dev set(검증 세트)의 결과를 의미
- feature-based approach에서는 BERT의 4개 레이어를 연결하여 특성으로 사용했으며, 이것이 섹션 $5.3$에서 최선의 접근방법이라는 것을 보여주었음

- fine-tuning은 다양한 마스킹 전략에 대해 대해 놀랍도록 강건하다는 것을 표에서 확인 가능
- 그러나, 기대한대로, MASK strategy만을 사용하는 것은 feature-based approach를 NER에 적용할 때 문제가 있었음
- 흥미롭게도, using only RND 전략은 우리의 전략보다 훨씬 성능이 나쁨

- 자연어 추론 (Language Inference)
    - 상황을 설명하는 문장이 주어졌을 때 그 상황에 대한 상식적인 추론을 의미
    - ex. 자동차 타이어를 교환하는 상황이 주어지고, 상황에 맞게 이어질 적절한 문장을 추론하는 것

- GLUE
    - 인간의 언어 능력을 인공지능이 얼마나 따라왔는지 정량적 성능지표를 만든 것
    - General Language Understanding Evaluation
    - 여러 태스크 존재
        - Question Answering
        - Sentiment Analysis
        - Textual Entailment
        

- downstream task
    
    실질적으로 해결하고자 하는 문제, 최종적 과제
    
- representation
    
    NLP 관련 논문을 읽으면 빠짐 없이 나오는 용어로, 실제 텍스트를 LM이 연산할 수 있도록 만든 형태
    
    - 카운트 기반으로 나타내는 것과 그렇지 않은 것으로 나눌 수 있음

- **???**

- token attend?
    - 문장이나 텍스트를 처리할 때 어떤 부분에 주의를 기울이는 것을 의미
    - 주로 텍스트를 작은 단위로 쪼개어 처리할 때 사용 == token
    
- shallow concatenation
    - 두 개 이상의 벡터나 기타 표현을 단순히 이어붙이는 것을 의미
    - 다양한 특징이나 정보를 결합하여 하나의 표현을 만들기 위한 간단한 방법
    - 각 표현간에 복잡한 변환 과정이나 계산 없이 단순히 이어붙이기만
    - 얕은 연결을 통해 두 개 이상의 표현을 결합하면, 다양한 특징이나 정보를 포함하는 하나의 새로운 표현을 얻을 수 있음

- shallow concatenation vs bidirectional (BERT)
    
    left-to-right concatenation
    
    - 왼쪽에서 오른쪽으로 진행하며 각 단어를 나란히 이어붙임
    - 각 단어의 임베딩을 순서대로 나열
    - 문맥을 좌측방향으로만 고려
    - shallow concatenation은 단어를 나란히 이어붙이는 간단한 방법이며, 좌측방향, 우측방향, 혹은 양쪽 방향 중 하나만을 고려
    - BERT는 좌우의 모든 문맥을 고려하여 각 단어의 임베딩 생성

- aggregate representation
    - 다양한 특징이나 정보를 종합하여 문장, 문서 또는 단어와 같은 텍스트의 전반적인 의미를 나타내는 하나의 표현을 만드는 과정을 의미
    - 문장 내의 모든 단어의 워드 임베딩을 가지고 있을 때, 이를 종합하여 문장 자체의 의미를 나타내는 벡터 생성 가능
    - 평균을 내거나 각 벡터를 연결하여 하나의 벡터로 만들거나 가중 평균을 사용하여 중요한 정보에 더 가중치를 두는 등의 방법 존재
    
    - random restart
        - 모델을 여러 번 초기화하고 다시 훈련시키는 과정을 의미
        - 이 논문에서는 작은 데이터셋에서 fine-tuning을 할 때 발생할 수 있는 불안정성을 극복하기 위한 전략중 하나
        - 작은 데이터셋에서는 모델이 민감하게 반응할 수 있기 때문에, random restart를 통해 초기화된 모델들 중에서 가장 성능이 좋은 모델 선택
    
    - 내적
        - Ti와 S는 각각 문맥 벡터와 질문 벡터를 나타냄
        - 이 두 벡터간의 내적을 계산
        - 내적은 두 벡터의 유사도를 측정하는 방법중 하나로, 두 벡터가 얼마나 비슷한 방향을 가지고 있는지 나타냄
    
    - log likelyhood
        - 로그를 쓰는 이유
        - 수치 안정성과 계산의 편의성
        - 수치안정성에서 곱셈연산은 여러개의 확률을 곱하는데, 확률은 0과 1사이의 값
        - 따라서 여러 확률을 곱하면 아주 작은 값이 되기 쉬움
        - 이렇게되면 컴퓨터의 부동소수점 연산에서 언더플로우가 발생할 수 있음
        - 로그우도를 사용하면 곱셈이 덧셈으로 변환되어 이러한 문제 방지 가능
        - 계산의 편의성, 로그는 곱셈을 덧셈으로 변환시켜주기 때문에, 계산이 훨씬 빨라짐
        - 특히 딥러닝 모델의 역전파 과정에서 미분을 적용하는데 유용
        - 또한 확률분포의 파라미터를 최적화할 때 자주 사용되는데, 이는 최대 우도 추정(MLE)의 기본 원칙 중 하나
        - 이를 통해 모델이 데이터를 가장 잘 설명하는 파라미터를 찾을 수 있음
        - 
    
- modest data augmentation
    
    데이터 증강을 실시하는 데 있어서 과도하지 않고 적절한 정도로 적용한다는 의미
    
- Abalation Study
    - 모델의 부분을 제거하거나 변경하여 그 부분이 전체 모델의 성능에 미치는 영향을 측정하는 실험
    - 각 구성요소의 상대적인 중요성을 이해하려는 적

- BiLSTM
    - Bidirectional Long SHort-Term Memory 레이어를 추가한다는 것
    
- LTR + RTL
    - LTR과 RTL 모델을 결합한 형태, 각각 좌측 문맥과 우측 문맥을 고려
    - 그러나 각 레이어에서는 해당 레이어의 위치에서의 좌측 문맥만 고려
    - 예를 들어, 두 번째 레이어에서는 해당 레이어의 좌측에서 오는 정보와 우측에서 오는 정보를 모두 고려
    - 그러나 이 두 정보를 다음 레이어로 전달할 때에는 각각 좌측 레이어와 우측 레이어로 분리되어 전달
    - BERT는 각 레이어에서 입력 시퀀스의 모든 토큰간 상호작용이 가능
    - 이는 각 토큰이 과측과 우측 모두의 문맥을 고려할 수 있다는것을 의미
    - BERT는 더 많은 문맥을 고려 가능

- perplexity
    - 모델이 언어를 얼마나 잘 예측하는지를 나타내는 지표
    - 낮을수록 예측이 더욱 정확

- held-out training data
    - 모델을 학습시키는 데 사용되지 않은 데이터를 의미

- computational benefits
    - fine-tuned 방식은 미세 조정된 모델 자체를 사용하여 다운스트림 작업 진행
    - feature-based 방식은 미리 계산된 표현을 이용하여 다양한 저렴한 모델 실험 가능

- NER
    
    개체 명명 인식
    
    - ‘OpenAI에서 GPT가 9월 11일에 커피를 마셨다’
    - 모델 출력
        - 조직: OpenAI
        - 인물: GPT
        - 날짜: 9월 11일
    - 위와 같이 주어진 텍스트에서 특정 유형의 개체를 식별하는 작업
    - 일반적으로 사람, 조직, 날짜, 장소, 시간, 수량 등과 같은 명명된 엔티티
    - 단순히 개체 식별 뿐만 아니라, ‘이 문장에서 날짜는 언제인가요?’라는 질문에 대답하는 데도 사용될 수 있음

- maximul document
    - document context: 문서의 전반적인 맥락, 즉 가능한 한 많은 문서적인 맥락을 포함시킨다는 의미
    

- CARF?
    
    CRF 사용하지 않은 이유 추정
    
    - 계산 복잡성, 이전의 레이블을 고려해 시퀀스 레이블을 예측하므로 추가적인 계싼 필요
    - 피처 엔지니어링, CRF를 사용하려면 feature engineering이 필요할 수 있음, 모델에 입력될 특성을 명시적으로 정의해야할 수 있음
    - 모델 복잡성, BERT는 각 토큰의 표현을 계산할 때 왼쪽 및 오른쪽 문맥을 모두 고려하므로, CRF를 사용하지 않고도 상당한 성능을 얻을 수 있을 것으로 예상
    - task-specific Considerations, 어떤 경우에는 CRF가 유용할 수 있지만, 이 작업에서는 CRF가 없이도 충분한 겨로가를 얻을 수 있다는 판단이 있을 수 있음

- partial word piece
    
    단어가 잘려진 부분
    
    예를 들어 apple이라는 단어가 app과 le로 분리된 경우, 얘네는 부분 단어 조각
    
    그러나 BERT에서는 특별하게 고려되지 않음
    
- epoch vs. step
    - epoch: 에포크는 전체 데이터셋을 한 번 모델에 대입하여 순전파와 역전파를 거쳐 학습시키는 과정을 말함, 에포크가 증가할 수록 모델은 더 많은 학습 데이터를 볼 수 있음
    - step(=iteration): 한 스텝은 모델이 한 번 업데이트 되는 단위, 일반적으로 배치 크기에 따라 정해지는데, 예를들어 만약 배치 크기가 64이면 64개의 데이터 포인트를 모델에 대입하여 업데이트하게 됨 스텝이 증가할 때마다 모델은 조금씩 더 학습됨
    - 일반적으로 에포크는 전체 데이터셋을 한 번 돌리는 과정을 의미하며, 스텝은 한 번의 모델 업데이트를 의미
    - 한 에포크당 스텝수가 100이면 한 에포크동안 데이터셋을 100번 사용하여 모델을 업데이트
- $\beta_1, \beta_2$
    
    Adam 알고리즘의 하이퍼 파라미터 중 하나로, 이전 그래디언트의 지수적 평균을 계산하는 데 사용됨
    
- L2 weight decay
    
    모델의 복잡도를 제어하기 위한 규제 방법 중 하나
    
- learning rate warmup over the first 10,000 steps
    
    처음 10,000 스텝동안 학습률을 조금씩 증가시켜 초기학습 안정화
    
- Linear decay of the learning rate
    
    학습률을 선형적으로 감소시키는 방법
    
- loss 함수
    - mean masked LM likelyhood: 모델이 마스킹된 단어를 예측하는 작업에서 얼마나 정확한지를 나타내는 지표
    - 이 값은 보통 언어모델의 훈련 과정에서 손실함수에 반영됨
    - mean next sentence prediction likelyhood: 모델이 두 문장이 이어지는지 여부를 예측하는 작업에서 얼마나 정확한지를 나타내는 지표
    - 언어모델의 훈련손실에 포함됨

- entailment
    - 첫 번째 문장의 내용에는 두 번째 문장의 내용이 논리적으로 포함
    - 첫 번째 문장이 참이라면 두 번째 문장도 반드시 참이어야 함
    - 모든 사람은 죽을 것이다, 아버지가 노인이다 라는 문장을 생각해보면, 두 번째 문장은 첫 번째 문장을 entail하고 있음
    - 이는 논리적으로 만약 모든 사람은 죽을 것이다가 참이라면 아버지가 노인이다 역시 참이어야한다는것