---
title: 선형대수 정리
date: 2025-03-30 21:10:00 +0900
description: 선형대수 정리
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [LinearAlgebra, Math] # add tag
use_math: true
categories:
  - Study Notes
sidebar:
    nav: "blog"
---

# 1. 기초개념
## 벡터
- 정의: 크기와 방향을 가진 데이터 구조
- 데이터를 N차원 공간에서 표현할 수 있게 해줌

## 행렬
- 정의: 데이터를 2D 구조로 표현한 것
- 다차원 데이터를 효율적으로 처리하게 해줌
- 뉴럴 네트워크에서 입력과 가중치 연산시 사용

## 텐서
- 정의: 다차원 배열(벡터와 행렬의 일반화)
- 고차원 데이터를 표현해줌
- 딥러닝 프레임워크(파이토치, 텐서플로우)에서 데이터 표현에 사용

# 2. 주요 연산
## 행렬 곱셈
- 정의: 두 행렬을 곱해 새로운 행렬 생성
- 뉴럴 네트워크에서 입력과 가중치의 선형변환 계산

## Dot Product
- 정의: 두 벡터의 내적 계산
- 유사성 측정에 필요
- self attention에서 query와 key의 연산에 사용

## 크로네커 곱 (Kronecker Product)
- 정의: 두 행렬 간의 특수 곱셈
- 고차원 텐서 구조 확장 혹은 압축할 때 활용

<br/>

# 3. 고유값과 고유벡터
- 정의: 행렬  A 에 대해  $Av = \lambda v$ 를 만족하는  $\lambda$ 와  $v$ 
- 데이터 분산분석(PCA)에 필요
- 뉴럴네트워크 학습 안정성 분석에 필요
- 응용: PCA에서 고유값 분해를 통해 데이터를 저차원으로 투영

## 선형변환과 벡터
- 행렬은 벡터를 다른 공간으로 변환하는 도구
- 대부분의 벡터는 행렬에 의해 크기와 방향이 모두 바뀌지만, **특정 벡터들은 방향은 유지하면서 크기만 변함**
- 이런 벡터를 고유벡터라하고, 크기 변화 비율을 고유값이라고 함
	- 즉, 행렬 계산 후 방향이 그대로인 벡터가 고유벡터이며 거기서의 크기변화 비율이 고유값

## 고유값을 구하기 위해서는
- 특성방정식을 풀어야 함
- $det(A-\lambda I) = 0$
- 이렇게 고유값을 구한 뒤, $(A-\lambda I)v = 0$으로 고유벡터를 구함

## 쓰임
- PCA에서 고유값과 고유벡터를 구하고, 고유값이 가장 큰 축이 데이터의 변동성이 가장 큰 방향

> [!내가 이해한 고유값과 고유벡터, PCA]
> 즉, 벡터에 행렬계산을 해서 벡터 방향이 바뀌지 않는 벡터가 바로 고유벡터, 여기서 크기가 변한 정도(변화율)가 고유값
> PCA에서 고유값과 고유벡터가 사용됨
> PCA는 데이터에서 변동성(=분산)이 가장 큰 축을 찾아 차원을 축소하는 기법
> 여기서 고유값과 고유벡터를 구하고, 고유값이 가장 큰 축을 찾아 사용자가 정한 개수만큼 남겨 차원축소를 진행함

<br/>

# 4. 선형 변환
- 정의: 데이터 공간에서 선형변환을 수행
- 딥러닝의 각 레이어는 선형변환 후 비선형 활성화함수 적용
- 응용: 고차원 데이터 표현 및 변환

<br/>

# 5. 미분가능한 연산
## 역전파(Backpropagation)
- 정의: 손실함수의 그래디언트를 계산하기 위해 선형대수와 미적분 결합
- 필요성: 모델학습에서 가중치 업데이트

<br/>

# 6. 특수 행렬
## 단위 행렬
- 정의: 대각선에 1이 있고 나머지는 0인 행렬
- 필요성: 선형 변환의 초기 상태 유지, 역행렬 계산

## 역행렬
- 정의:  $AA^{-1} = I$ 를 만족하는 행렬(행렬 A를 곱했을 때 항등행렬 I가 되는 행렬)
- 필요성: 모델에서 역전파나 가중치 업데이트 과정에 사용

## 역행렬 조건
- 정사각 행렬(n* n)
- 행렬식이 0이 아니어야 함 ($det(A) != 0$)

## 쓰임
- 선형방정식 풀이, 양변에 역행렬을 곱해 해를 구할 수 있음
- 최적화, 거리계산
- 행렬 A의 고유값이 $\lambda$라면, 역행렬의 고유값은 $\frac{1}{\lambda}$임
- 역행렬이 없는 경우, 대체 방법으로 의사 역행렬(pseudo-inverse) 사용
	- SVD나 최소제곱 기반 계산

## 특이값 분해(SVD, Singular Value Decomposition)
- 정의: 행렬을  $U \Sigma V^T$ 로 분해 
- 필요성: 차원 축소, 노이즈 제거

<br/>

# 7. 고차원 데이터와 차원 축소
## PCA
- 정의: 데이터의 주요 축을 찾아 차원 축소(분산이 가장 큰 축)
- 필요성: 고차원 데이터 시각화, 데이터 주요 특징 보존
- 고유값과 고유벡터를 찾고, 고유값이 큰 축이 데이터의 변동성(분산)이 가장 큰 방향

## T-SNE, UMAP
- 정의: 고차원 데이터를 저차원으로 임베딩하는 비선형 차원 축소 기법
- 필요성: 데이터 시각화

## t-SNE
- 데이터의 지역적 구조를 유지하며 고차원 데이터를 저차원으로 임베딩
- 유사한 점끼리 가깝게, 다른점은 멀리 배치
- 작동원리: 고차원, 저차원의 확률 분포를 매칭
	- 거리관계를 확률적 분포로 표현, 고차원 공간과 저차원 공간의 분포간 차이를 줄이도록 변환
	- 고차원에서 확률 분포는 각 데이터 포인트 유사도를 가우시안 분포 기반 조건부확률로 계산
	- 저차원에서 확률분포는 각 데이터 포인트 유사도를 t-분포 기반으로 계산
	- 고차원과 저차원 확률분포 간 차이를 KL Divergence를 이용해 최소화
- 장점
	- 고차원 데이터 지역적 구조 잘 유지, 데이터 클러스터링이나 시각화에 적합
- 단점
	- 계산 비용이 높아 대규모 데이터에 비효율적
	- 매번 실행시 결과가 조금씩 달라질 수 있음(랜덤 초기화)
- 선택 가이드
	- 데이터가 작고, 고차원 데이터의 지역적 패턴 시각화가 목표일 때
	- 클러스터 간 경계가 명확하지 않더라도 시각화가 중요한 경우


> **[내가 이해한 T-SNE]**
> - t-SNE는 고차원과 저차원의 데이터 관계를 각각 확률 분포로 표현하고 kl-divergence를 이용해 비교하며 최적화하는 방법
> - 즉, 고차원과 저차원 각각의 데이터 간 유사성을 확률값으로 표현하고 이를 모아 확률 분포 생성 > KL-divergence 사용해 서로 비교하며 최적화
> 	- 고차원 데이터는 가우시안 분포 기반 조건부 확률분포로 표현
> 	- 저차원 데이터는 t-분포로 표현
> 		- _고차원 데이터는 밀도가 불균형하기 때문에 조건부확률에서 $\sigma$를 통해 데이터의 지역 밀도 조정_
> 		  
> **[사용]** 
> - 지역적 구조에 초점(가까운 관계를 시각화하는 데 최적화됨)
> - 데이터 크기가 작을 때 (계산 비용 높음, 메모리 사용량 큼)
> - 클러스터 내 세부적 관계가 중요할 때 (클러스터 내부 분포 분석, 미묘한 차이 강조 등)
> - 시각화가 목적일 때(데이터 시각화에 특화된 기법이라 직관적임)
> - 주의: 클러스터 내부 관계는 잘 표현되나 클러스터 간 거리는 왜곡될 수 있음
> 
> **[my case]**
> - 데이터 크기가 4000개로 소규모임
> - 지역적 관계를 보존하는데 최적화되어 있어 스피커 임베딩 간 미세한 차이 잘 표현 가능
> - 태깅된 텍스트와 임베딩의 상관관계를 확인하기 위해, 클러스터 내부의 디테일한 관계를 잘 표현
> - t-SNE는 클러스터 간 상대적 거리나 전역적인 구조는 왜곡될 수 있지만 시각적 이해를 돕는 직관적인 결과를 생성


## UMAP
- 거리와 연결관계를 기반으로 고차원 데이터의 저차원 임베딩 생성
- T-SNE와 비슷하지만 더 빠르고 효율적
- 지역적과 전역적 구조를 동시에 잘 보존
- 작동 원리: 거리 기반의 그래프 최적화
	- 그래프 이론과 거리 기반 접근 방식 사용하여 데이터의 저차원 표현 생성
	- 고차원 데이터는 그래프 표현: 데이터 포인트를 노드로, 노드간 거리를 엣지로 표현, 유사도 그래프를 생성해 가까운 점은 연결강도가 높고, 먼 점은 약하게 연결
	- 고차원 그래프의 구조를 저차원에서 유지하도록 최적화
	- 고차원 그래프와 저차원 그래프간의 정보 손실 최소화
- 장점
	- t-SNE보다 계산 속도가 빠르고 대규모 데이터 처리에 적합, 지역적 & 전역적 동시 잘 보존
- 단점
	- 파라미터 설정에 따라 결과가 크게 달라지며 시각화 결과가 t-SNE만큼 직관적이지 않을 수 있음
- 선택 가이드
	- 데이터가 크고, 지역적/전역적 패턴 모두를 살리고 싶을 때
	- 계산 속도와 효율성이 중요한 경우


> **[내가 이해한 UMAP]**
> 고차원의 데이터를 저차원 임베딩으로 생성할건데, 그래프 구조를 활용함
> - 고차원 공간의 데이터 포인트를 노드로, 유사성(거리)을 엣지(가중치 사용)로 표현 후 저차원 공간으로 최적화시키며 차이를 최소화함
> 	- 고차원 공간에서 데이터 포인트 간 유사성(거리 계산) > 밀도 기반 확률화를 통해 유사성 정량화 > 이를 기반으로 고차원 그래프 구성
> 	- 초기 저차원 임베딩 무작위 초기화 > 데이터 포인트 간 거리 계산 > cross entropy loss 최소화해 고차원과 저차원 그래프 간 구조 차이를 줄임(gradient descent 사용)
> 		- _고차원 데이터는 본질적으로 밀도가 불균형하기 때문에 밀도 보정 필요_
> 		- 저차원 임베딩에서도 그래프와 거리 계산이 동일하게 적용되기 때문에, 별도의 변환 없이 엣지 가중치와 임베딩 거리를 직접 비교할 수 있음
> 		- 즉, 최적화 목표는 고차원 그래프와 저차원 임베딩 사이의 구조적 일관성을 유지하는 것
> 		  
> **[사용]**
> - 데이터 크기가 클 때 (계산속도 t-sne보다 훨 빠름, 수십만개 이상)
> - 전체적인 관계 파악이 중요할 때(클러스터 간 관계)
> - 속도와 자원이 중요한 경우 (실시간 시각화가 필요한 애플리케이션)

## 이웃수와 iteration에 관하여
- 이웃 수
	- 작게 설정: 지역적 관계 강조
	- 크게 설정: 전역적 관계 강조
- iteration 멈춤 기준
	- 데이터 포인트 이동이 안정화되고, 클러스터가 뚜렷해졌을 때
	- Loss 값(KL Divergence, Cross-Entropy)이 더 이상 감소하지 않을 때

<br/>

# 8. 확률과 선형대수의 결합
## 확률 분포의 표현
- 정의: 벡터나 행렬로 확률 분포를 표현
	- ex. 소프트맥스로 분류확률 계산
- 필요성: 분류모델의 출력확률 계산

## 어텐션 매커니즘과 확률
- 정의: 어텐션 스코어 계산 - softmax로 query-key 유사성 정규화
- 필요성: transformer 모델에서 self-attention의 핵심

<br/>

# 9. 선형대수와 그래프
## 그래프 신경망
- 그래프의 인접행렬(adjacency matrix)와 Laplacian Matrix의 연산

## 컨볼루션 연산
- 정의: CNN에서 지역적 특징 추출을 위한 필터 연산
- 필요성: 이미지 및 음성 데이터 처리


