---
layout: post
title: PCA (주성분 분석)
date: 2023-06-04 22:01:00 +0900
description: 
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [study, statistics] # add tag
use_math: true
---

## 들어가면서 

​	이번 글에서는 차원 축소 방법 중 한 가지인 PCA에 대해 알아보고 정리하겠습니다. 

​                 

---

​      

## PCA (Principal component analysis)

&nbsp; &nbsp;&nbsp;PCA, 즉 주성분 분석이란 고차원의 데이터를 저차원의 데이터로 환원시키는 기법을 말합니다. 모델을 만들 때, 매우 많은 feature들이 사용되면 내부의 파라미터도 매우 복잡해지고, 예측 성능이 떨어질 수 있기 때문에 차원 축소 기법을 쓰곤 합니다. 일례로 카테고리형 변수를 one-hot 인코딩하여 피처의 수가 매우 많아졌을 때 PCA를 활용하여 모델의 차원을 축소시킬 수 있습니다. 또한, 차원 축소를 통해 3차원 이상의 다차원 데이터에서도 데이터의 특성 분포를 시각화하여 살펴볼 수 있습니다. PCA 외에도 다른 차원 축소 기법들이 있지만, 오늘은 PCA에 대해 알아보겠습니다. 



### PCA 단계

PCA를 사용하면 데이터를 좌표계에 사영하여 가장 큰 분산을 가지는 축, 즉 주성분들을 찾아냅니다. 분산이 크다는 것은 데이터의 변동성이 크고, 데이터 간의 차이나 변화가 크다는 것을 의미하며, 이를 이용해 분산이 가장 큰 축을 찾음으로써 주요한 변동성을 포착하여 데이터의 특성을 잘 파악해낼 수 있습니다. 즉, 데이터를 가장 잘 설명하는 축을 찾는 것입니다. 

PCA를 하게 되면 해당 피처 수만큼 주성분들이 생성되는데, 분산이 큰 순서대로 생성됩니다. 만일 처음 3개의 주성분이 80% 이상의 데이터 변동성을 설명할 수 있다면, 3개의 주성분으로 원본 데이터의 변동성을 최대한 보존하면서 차원을 축소시킬 수 있는 것입니다. 이는 다음과 같은 단계로 진행됩니다.

1. **데이터의 표준화**

   첫째로, 데이터를 표준화시켜주어야 합니다. 각 피처마다 스케일이 다르기 때문입니다. 피처의 스케일이 다르면, PCA는 변동성이 큰 피처를 중심으로 주성분을 찾아내게 됩니다. 만일 변수 1의 값이 0~1 사이의 값이고, 2의 값이 100~1000 범위에 있다면 변수2가 더 변동성이 크므로 결과에 더 큰 영향을 줄 수 있습니다. 따라서 표준화를 통해 각 피처의 평균을 0으로, 표준편차를 1로 맞추어주어 스케일에 의한 편향을 없애야 각 피처가  동등한 영향을 미칠 수 있습니다. 

   표준화는 scikit-learn 라이브러리에서 scaler를 사용할 수 있습니다. 아래는 직접 실습해본 내용으로, 캐글의 [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic/data) 데이터를 사용했습니다. 우선 데이터를 불러와줍니다.

   <img width="737" alt="스크린샷 2023-06-04 오후 10 47 01" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/664e6495-7087-40eb-b17c-6de8d8f0b4b1">

     

   해당 데이터에서 카테고리형 변수를 제외하고 추출한 뒤, 인덱스를 PassengerId로 지정해주겠습니다.    

<img width="736" alt="스크린샷 2023-06-04 오후 10 52 20" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/d4477792-6fa2-4897-9b9b-5d8d4857c7b9">

​		  

​		사이킷런의 StandardScaler로 데이터 표준화를 진행해줍니다.  

<img width="729" alt="스크린샷 2023-06-04 오후 10 56 31" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/65c71cf7-d551-4ca1-9e23-3b03e97ac923">

  

  2. **사이킷런 라이브러리로 PCA 수행**

     사이킷런 라이브러리의 PCA 메서드를 임포트해줍니다. n_components는 추출할 주성분의 개수를 의미합니다.

     이 때 추출되는 주성분들은 기존 변수를 조합하여 만들어지는 새로운 변수이므로, 각 행의 인덱스와는 대응하지만 주성분 차원 이름은 기존의 컬럼과 대응하지 않습니다. 따라서 index만 유지해주도록 하겠습니다.

<img width="731" alt="스크린샷 2023-06-04 오후 11 13 55" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/ad4f5680-5afd-4c03-8958-b4428d25ef70">

  

3. **주성분의 설명력 계산**

   pca_explain_variance 메서드로 주성분의 설명력을 알아볼 수 있습니다. 이 메서드는 각 주성분이 설명하는 분산의 비율을 반환합니다. 이를 통해 각 주성분이 전체 분산에서 얼마만큼의 설명력을 가지는지 파악할 수 있습니다. 따라서 주성분을 선택하거나, 축소할 적절한 차원수를 선택하는데 도움이 됩니다. 

   <img width="736" alt="스크린샷 2023-06-04 오후 11 14 04" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/f44a2581-4202-4bcc-8763-40fa979a920b">

   해당 예시에서는 2차원으로 축소한다고 했을 때 원 데이터의 약 57%, 3차원으로 축소한다고 했을 때 약 81%의 설명력을 가진다고 볼 수 있겠습니다. 

