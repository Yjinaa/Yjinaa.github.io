---
layout: post
title:[논문 리뷰] Generalized End-to-End Loss For Speaker Verification 
date: 2024-01-07 21:03:00 +0900
description: 
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [paper] # add tag
use_math: true
---

## **들어가면서**

&#160;&#160;&#160;&#160; 연구를 하면서 필요한 논문들을 공부하는 중에 기록 겸 정리해보았습니다. 이 논문은 구글에서 발표한 논문으로, speaker embedding 생성시 사용되는 loss입니다. 이전의 TE2E보다 더 발전된 형태인 GE2E loss를 소개합니다.

​              

​              

## Abstract

- 이 논문에서는, generalized end-to-end(GE2E)라는 새로운 loss function을 제안
- 이전의 tuple-based end-to-end(TE2E) loss function보다 speaker verification 모델 훈련을 더욱 효과적으로 할 수 있음
- TE2E와는 다르게, GE2E loss function은 각 훈련 프로세스의 각 스텝에서 입증하기 어려운 example들을 강조하는 방향으로 네트워크를 업데이트함
- 추가적으로, GE2E loss는 example selection의 초기 단계가 불필요
- 이러한 것들로, 새 손실 함수를 적용한 모델은 speaker variation EER을 10% 이상 낮추었고, 훈련 시간 또한 60%로 감소시킴
- 또한 MultiReader technique도 도입했는데, 이는 domain adaptaion을 가능하게 함 (training a more accurate model that supports multiple keywords) (ex ‘OK Google and Hey Google) 즉, 다양한 방언을 비롯한 다양한 키워드를 지원

​      

​       

​        

## Introduction

Speaker Varification(SV)는 발화가 특정 화자의 것이 맞는지 화자의 알려진 발언(ex. enrollment utterances)를 기반으로 하여 확인하는 프로세스

Voice Match와 같은 응용프로그램에서 사용

- 등록 및 검증에 사용되는 발화의 제한에 따라, speaker verification 모델은 일반적으로 두 가지 범주 중 하나에 속함
  - text-dependent speaker verification(TD-SV)
  - text-independent speaker verification(TI-SV)
- TD-SV에서는, 등록 및 확인 발화의 대본이 음성으로 제한됨
- TI-SV에서는 등록 또는 확인 발화의 대본에 어떠한 어휘의 제약도 없음, 음성과 발언 지속 시간이 더 큰 다양성에 노출됨 (음성과 발언 지속 시간의 더 큰 변동성이 노출됨)
- 이 작업에서는, TI-SV와 global password TD-SV로 알려진 TD-SV의 특정 subtask(하위작업)에 초점을 맞춤, 여기서 검증은 OK Google과 같은 감지된 키워드를 기반으로 해서 확인
- 이전 연구에서는, i-vector based 시스템이  TD-SV와 TI-SV 응용에 대한 지배적인 접근 방법이었음
- 최근 몇년간, 많은 노력들이 speaker verification에 신경망을 이용하는 연구들이 증가, 가장 성공적인 시스템들은 end-to-end 학습을 사용
- 그러한 시스템들에서는, 신경망의 출력 벡터가 보통 임베딩 벡터(also known as d-vectors)로 불림
- i-vectors 케이스와 비슷하게, 이러한 임베딩은 고정된 차원 공간에서 발화를 represent하는데 사용 가능, 일반적으로 더 간단한 방법을 사용해 화자들간의 모호성을 줄일 수 있음 (= 화자들을 구분 가능)

​                 

​    

### _1.2 Tuple-Based End-to-End Loss_

이전 연구에서 TE2E 모델을 제안, 이는 training에서 runtime 등록과 확인의 2단계 프로세스를 시뮬레이션

- 실험에서, LSTM과 결합된 TE2E 모델은 가장 좋은 성능을 달성
- 각 훈련 스텝마다, 하나의 평가발언(evaluation utterance) $x_{j}$ 그리고 $M$개의 등록 발언 $x_{km} (for~ m=1,...,M)$으로 이루어진 튜플이 LSTM 네트워크{${x_j, (x_{k1}, ..., x{km})}$}에 입력으로 사용됨
- 여기서 $x$는 고정 길이 세그먼트의 특성(log-mel-filterbank energies)을 나타내며, $j$와 $k$는 화자의 발언을 나타내고, $i$는 $j$와 같을수도, 다를 수도 있음
- 튜플은 화자 $j$로부터의 단일 발언과 화자 $k$의 M개의 다른 발언을 포함
- 우리는 $x_j$와 $M$개의 등록 발언이 동일 화자일 경우 ($j = k$) tuple을 positive라고 부르고, 반대일 경우 negative라고 부름
- positive와 negative를 번갈아가며 생성
- 각 입력 튜플에 대해서, LSTM의 $L2$ normalized response를 계산: {$e_j$~ $(e_{k1}, ..., e{kM})$}
- 여기서 각각의 $e$는 LSTM로부터 정의된 sequence-to-vector 매핑의 결과로 나오는 고정된 차원의 임베딩 벡터임
- 튜플 ($e_{k1}, ..., e_{kM}$)는 M개의 발화에서 구성된 voiceprint(음성 특징)를 나타내며 다음과 같이 정의됨

$$
(1)~~  C_k = E_m[e_{km}] = \frac{1}{M}\sum_{m=1}^{M}e_{km}
$$

- 유사도는 코사인 유사도 함수를 이용해 정의됨 (with 학습 가능한 w와 b)

$$
(2)~~s = w\cdot cos(e_{j\sim} , c_k) + b,
$$

- TE2E는 결국 아래와 같이 정의됨

$$
L_T (e_{j\sim}, c_k) = \delta(j,k)(1-\sigma(s)) + (1-\delta(j,k))\sigma(s).
$$

여기서 $\sigma(x) = 1/1(+e^{-x})$ 는 표준 시그모이드 함수이고 $\delta(j,k)$는 $j = k$일 때 1이 되고, 아닐 때 0이 됨

- TE2E 손실함수는 $k=j$일 때 $s$가 더 크도록 장려하고, $k !=j$일 때 $s$값이 더 작도록 장려
- positive와 negative 튜플 모두에 대한 업데이트를 고려할 때, 이 손실함수는 FaceNet의 triplet loss와 매우 유사



### _1.3 Overview_

이 논문에서는, TE2E 아키텍처의 일반화를 소개

이러한 새로운 아키텍처는 다양한 길이의 input sequence로부터 튜플을 더 효율적으로 구성하여 TD-SV와 TI-SV 모두의 성능과 훈련 속도를 동시에 향상시킴

- $2.1$에서는 GE2E 손실함수 소개, $2.2$는 GE2E가 왜 모델 파라미터를 더욱 효율적으로 업데이트하는지에 대한 이론적 정의, $2.3$에서는 이는 여러 단어와 언어를 지원하는 단일 모델을 훈련시킬 수 있는 “MultiReader”로 불리는 신기술 소개,  마지막으로, 실험의 결과를 $3$에서 소개



## 2. *GENERALIZED END-TO-END MODEL*

GE2E 훈련은 많은 발화들을 한꺼번에 처리, 이 발화들은 평균적으로 각 화자당 M개의 발화를 포함하는 N명의 화장로 구성된 배치 형식으로 처리됨

*Figure 1*의 그림과 같음

<img width="718" alt="스크린샷 2024-01-07 오후 9 12 16" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/949c2672-8dea-41d4-a463-ccaffb86aac0">

*Figure 1. system overview. 다른 색들은 각기 다른 화자에게서 온 발화/임베딩을 나타냄*



### _2.1 Training Method_

- 배치를 만들기 위해 $N*M$개의 발화를 가져옴
- 이 발화들은 $N$명의 다른 화자로 구성되어있으며, 각 화자는 $M$개의 발언을 가짐
- 각 특성 벡터 $x_{ji} (1 \leq j \leq N~ and~ 1\leq i \leq M)$ 은 화자 $j$의 발언 $i$에서 추출된 특성들을 나타냄
- 이전의 연구와 유사하게, 각 발언 $x_{ji}$로부터 추출된 feature들을 LSTM 네트워크에 넣음
- 하나의 선형 레이어가 네트워크의 마지막 프레임 response의 추가적인 변환으로 마지막 LSTM 레이어에 연결됨
  - 선형 레이어가 LSTM 네트워크의 마지막 레이어에 연결되어 네트워크의 마지막 프레임 응답을 추가적으로 변환하는 역할을 함
- 전체 신경망의 출력을 $f(x_{ji};w)$라고 나타내고, $w$는 신경망의 모든 파라미터(LSTM 레이어와 선형 레이어 모두)를 나타냄
- 임베딩 벡터(d-vector)는 네트워크의 출력의 L2 normalization로 정의됨, L2 정규화는 벡터의 길이를 1로 만들어줌:

$$
e_{ji} = \frac{f(x_{ji};w)}{\parallel f(x_{ji};w)\parallel}
$$

- $e_{ji}$는 $j$번째 화자의 $i$번째 발화의 임베딩 벡터를 나타냄

- $j$번째 화자의 임베딩 벡터$[e_{j1}, ..., e_{jM}]$의 중심점 $c_j$는 Equation 1을 통해 정의됨

- 유사도 매트릭스 $S_{ji,k}$는 각 임베딩 벡터인 $e_{ji}$와 모든 centroid $c_k(1\leq j,k \leq N,and ~ 1\leq i\leq M)$ 간의 scaled 코사인 유사도를 통해 정의됨:
  $$
  (5)~~S_{ji,k} =  w \cdot cos(e_{ji}, c_k) + b,
  $$

$w$와 $b$는 학습가능한 파라미터

- 코사인 유사도가 클 때 유사도도 크길 원하므로 weight이 positive $w > 0$이 되도록 억제
  - $e_{ji}$는 $j$번째 화자의 $i$번째 발화의 임베딩 벡터를 나타냄
  - $[e_{ji}, ..., e_{jM}]$는 $j$번째 화자의 모든 발화에 대한 임베딩 벡터들을 나타내며, 이들의 평균을 구해 $c_k$로 정의
  - $S_{ji,k}$는 $j$번째 화자의 $i$번째 발화와 $k$번째 화자의 임베딩 벡터들간의 유사도를 나타내는 매트릭스, 이 유사도는 코사인 유사도에 일정한 가중치와 편향을 곱한 값으로 계산됨
  - 코사인 유사도는 두 벡터간의 방향적 유사성 측정, 즉 두 벡터가 얼마나 같은 방향으로 향하고 있는지 나타냄
  - 유사도 값은 -1부터 1까지의 범위를 가지며, 1에 가까울수록 두 벡터는 매우 유사
  - $w$는 유사도 계산에 사용되는 가중치
  - 이 가중치가 양수인 경우, $w * cos(e_{ji}, c_k)$ 의 값이 코사인 유사도 값과 동일한 부호를 가지게 됨
  - 코사인 유사도가 더 큰 경우, 유사도 값 자체도 더 커지게 됨
  - 더 큰 코사인 유사도가 더 큰 유사도 값으로 반영되도록 하는 것
  - w, b는 유사도를 계산할 때 얼마나 중요하게 고려할지를 결정
  - 두 벡터가 유사한 방향으로 학습됨 ⇒ 화자와 발화가 유사한걸 찾는쪽으로 학습됨
  - 즉, 코사인 유사도가 클 때 유사도도 함께 커지길 바라므로 $w$를 양수로 제한
- TE2E와 GE2E의 결정적인 차이는 다음과 같음
  - TE2E의 유사도_(Equation 2)_는 임베딩 벡터 $e_{j\sim}$와 단일 튜플 중심값 $c_k$의 유사도를 정의하는 스칼라값,
  - GE2E는 각 $e_{ji}$와 모든 중심점 $c_k$의 유사도를 계산하는 유사도 매트릭스 (*Equation 5)* 구성
- _Figure 1_은 feature, 각기 다른 색으로 표현된 다양한 화자로부터의 특징, 임베딩 벡터, 유사도 점수와 함께 전체 프로세스 표현
- training에서, 각 발화의 임베딩이 모든 해당 화자의 임베딩의 중심점과 비슷하도록 하면서, 동시에 다른 화자의 임베딩과는 멀어야 함
- _Figure 1_의 유사도 매트릭스에서 본 것과 같이, 색칠된 영역의 유사도값이 큰쪽으로, 회색 영역이 작은 쪽으로 되어야 함<img width="718" alt="스크린샷 2024-01-07 오후 9 22 01" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/6b455532-e084-4ebc-84be-67e47f9cc381">

*Figure 2. GE2E 손실함수가 임베딩을 실제 화자의 중심점쪽으로 밀어주고, 가장 유사한 다른 화자의 중심점에서는 멀어지도록 함*



- _Figure 2_는 동일한 개념을 다른 방식으 설명: 파란 임베딩 벡터가 해당 화자의 중심점(파란 삼각형)에 가깝도록하고, 다른 중심점과는 멀도록함(빨강, 보라 삼각형), 특히 가장 가까운 화자(빨간 삼각형)

- 임베딩 벡터 $e_{ji},$  모든 중심점 $c_k$, 그에 따른 유사도 매트릭스 $S_{ji,k}$ 가 주어지면 이 개념을 구현하는 두 가지 방법 존재:

  - **_softmax_**

    $S_{ji,k}$ $(for~k=1,,...,N)$에 softmax 적용, 이는 $k=j$일 때 출력을 1로, 같지 않을 경우 출력을 0으로 만들어줌

    그래서 각 임베딩 벡터 $e_{ji}$의 손실함수는 다음과 같이 정의될 수 있음

$$
L(e_{ji}) = -S_{ji, j} + log\sum_{k=1}^{N}exp(S_{ji,k})
$$

-  

  - **_contrast_**

    contrast 손실함수는 positive 쌍과 가장 공격적인 negative 쌍으로 정의됨:
    $$
    L(e_{ji}) = 1-\sigma(S_{ji,k} + \max_{1\leq k\leq N} \sigma(S_{ji,k}),~~~k\neq j
    $$
    

    $\sigma(x) = 1/(1+e^{-x})$ 는 시그모이드 함수

    모든 발화에 대해, 정확히 두 구성요소가 손실함수에 더해짐

    (1) a positive component, 임베딩 벡터와 실제 화자의 voiceprint(centroid)간의 positive match와 연결됨

    (2) a hard negative component, 임베딩 벡터와 거짓 화자 중 가장 높은 유사성을 가진 voiceprint(centroid)간의 negative match

    _Figure 2_ 에서, positive term은 $e_{ji}$(파란 원)을 $c_j$(파란 삼각형)쪽으로 push하는 것을 뜻함

    negative term은 $e_{ji}$(파란 원)를 $c_k$(빨간 삼각형)로부터 멀리 pulling하는 것을 뜻함, $c_k$는 $c_{k'}$와 비교했을 때 $e_{ji}$와 더 유사하기 때문

    그래서 contrast loss 는 임베딩 벡터와 negative centroid의 어려운 pairs에 집중할 수 있도록 해줌

    실험에서, 두 수행이 GE2E 손실함수에 모두 유용했음: contrast loss는 TD-SV에, softmax loss는 TI-SV에 살짝 더 잘 작동

    추가적으로, true 화자의 centroid를 계산할 때 $e_{ji}$를 제거하는 것은 훈련을 안정적이게 하고 너무 간단한 해(trivial solution)을 피하는 것을 도와줌

    그래서, negative 유사성($k\neq j)$을 계산할 때 _Equation 1_을 계속 사용하지만, $k=j$일 경우 *Equation 8* 사용
    $$
    (8)~~c_j^{(-i)} = \frac{1}{M-1}\sum_{m=1,m\neq i}^{M}e_{jm}~,
    $$
    
    $$
    (9)~~S_{ji,k} = \begin{cases} w\cdot cos
    (e_{ji},c_j^{(-i)}) + b ~~~~~if~~k=j;\\w\cdot cos(e_{ji}, c_k)+b~~~~~~~~ otherwise. \end{cases}
    $$
    *Equation 4, 6, 7, 9*를 합하면, 최종 GE2E loss인 L_G는 유사성 매트릭스(1\leq j \leq N, ~~and~~ 1\leq i\leq M)의 모든 손실의 합으로 정의됨
    $$
    (10)~~~L_G(x;w) = L_G(S) = \sum_{j,i}L(e_{ji})
    $$
    

    ​     

    ​      

### _2.2 Comparision between TE2E and GE2E_

GE2E 손실함수 업데이트의 단일 배치를 생각해보자: $N$명의 화자, 각 화자의 $M$개의 발화가 있음

- 각 업데이트 스텝은 모든 $N*M$의 임베딩 벡터를 각각 해당하는 중심점으로 밀어넣고, 다른 중심점에서 멀어지도록

- 이는 각 $x_{ji}$에 대한 TE2E 손실함수의 가능한 모든 튜플에 발생하는 상황을 반영

- 화자를 비교할 때 랜덤하게 $j$ 화자에게서 $P$개의 발화를 골랐다고 가정:

  1. Positive tuples: $\{x_{ji}, (x_{j,i_1}, ..., x_{j,i_P})\}$ 이 때 $1\leq i_p \leq M$ 그리고 $p = 1,...,P.$ 여기에는 $\begin{pmatrix} M\\P \end{pmatrix}$개의 positive tuple 존재

  2. Negative tuples: $\{x_{ji}, (x_{j,i_1}, ..., x_{j,i_P})\}$  이 때 $k\neq j$ 그리고 $1\leq i_p\leq M$  , $p = 1,...,P.$

     모든 각 $x_{ji}$에 대해, 다른 $N-1$ 개의 중심점과 비교해야 함, 각각의 비교는 $\begin{pmatrix} M\\P \end{pmatrix}$ 개의 튜플을 포함함

각 positive 튜플은 negative 튜플과 균형을 이룸, 따라서 전체 숫자는 positive와 negative 최대 튜플의 개수에 *2를 한 값임

- 따라서 TE2E 손실함수의 튜플의 총 개수는:
  $$
  (11)~~~2 * max\Big(\begin{pmatrix}M\\P\end{pmatrix}, (N-1) \begin{pmatrix}M\\P\end{pmatrix}\Big)\geq 2(N-1)
  $$
  *Equation 11*의 하한은 $P = M$일 때 발, 따라서 GE2E 손실함수에서의 $x_{ji}$에 대한 각 업데이트는 TE2E 손실함수에서 최소 $2(N-1)$ 개의 스텝과 같음

  위의 분석은 왜 GE2E가 TE2E보다 모델을 더 효율적으로 업데이트하는지를 보여줌, 이는 경험적인 관찰과 연관

  GE2E는 더 짧은 시간 내 더 나은 모델을 만듦



   

   

### _2.3 Training with MultiReader_

다음 케이스를 생각해보자: 작은 Dataset $D_1$을 이용해 모델을 도메인에 적용시키려는 상황

- 동시에, 유사하면서 보다 큰 Dataset $D_2$가 있는데, 같은 도메임은 아님

- $D_2$ $D_2$$D_1$에 잘 동작하는 단일 모델을 훈련시키고 싶음
  $$
  L(D_1, D_2;w)=E_{x\in D_1}[L(x;w)] + \alpha E_{x\in D_2}[L(x;w)]
  $$

- 이는 regularization 기술과 비슷: 보통의 regularization에서는 $\alpha\parallel w\parallel ^2_2$를 사용
- 그러나 여기서는, regularization을 위해  $E_{x\in D_2}[L(x;w)]$ 를 사용
- $D_1$이 충분한 데이터를 갖고있지 않을 때, $D_1$에서 학습시키는 것은 오버피팅을 야기할 수 있음
- 네트워크가 $D_2$에서도 잘 작동하도록 하는 것이 network를 regularize하는 데 도움됨
- 이는 $K$개의 각각의 매우 불균형한 데이터 소스와 결합함으로써 일반화될 수 있음: $D_1, .... D_K$.
  - 각 데이터 소스의 규모나 중요도가 다를 수 있다는 것을 의미
- 각 데이터 소스에 $\alpha_k$ 를 할당하여 데이터 소스의 중요도를 나타냄
- 훈련하는 동안, 각 스텝에서 각 데이터 소스로부터의 발화의 하나의 배치/튜플을 가져오고, 통합된 loss 계산:

$$
L (D_1,...,D_K) = \sum_{k=1}^{K}\alpha_kE_{x_k\in D_k}[L(x_k;w)]
$$





## 3. EXPERIMENTS

실험에서는, feature 추출 프로세스가 6과 같음

- 오디오 신호를 25ms 폭의 프레임으로 나눔
- 각 프레임에 대해 10ms만큼 이동하면서 처리를 진행
- 이후 각 프레임에서 40 차원의 log-mel-filterbank 에너지를 추출 ⇒ 이것이 각 프레임의 feature이 됨

TD-SV application에서는, 같은 특성이 keyword detection과 speaker verification에서 모두 쓰임

- 이 프레임들은 고정된 길이(보통 800ms) 세그먼트를 형성
- TI-SV에서는, Voice Activity Detection(VAD) 이후 주로 랜덤한 고정 길이 세그먼트를 추출, 추론을 위해 슬라이딩 윈도우를 사용 (섹션 $3.2$)

프로덕션 시스템은 3 레이어 LSTM을 사용

- 임베딩 벡터(d-vector) 사이즈는 LSTM projection 사이즈와 같음
- TD-SV에서는 128개의 히든 노드를 사용, 프로젝션 사이즈는 64
- TI-SV에서는 768개의 히든 노드 사용, 프로젝션 사이즈는 256
- GE2E 모델을 훈련할 때, 각 배치는 $N$=64명의 화자와 각 화자마다 $M$=10개의 발화가 담겨있음
- SGD 사용, 초기 learning rate 0.01, 그리고 30M 스텝마다 반으로 줄임
- 경사도의 L2-norm을 3으로 제한
  - L2-norm = 벡터의 길이, 이 값이 3보다 크다면 그레디언트의 방향을 유지한 채로 크기를 3으로 줄임, 이렇게 함으로써 그래디언트가 너무 커지는 것을 방지하며 수렴을 안정화시킴
  - 학습과정에서 그래디언트 폭주(gradient explosion)을 방지하는데 사용됨
- LSTM의 프로젝션 노드에 대한 그래디언트 스케일은 0.5로 설정
- loss 함수의 스케일링 요소 $(w,b)$에 관해, 좋은 초기값은 $(w,b) = (10,-5)$라는 것을 관찰, 그리고 이 값들에 대한 그래디언트 스케일을 0.01으로 낮추는 것이 수렴을 부드럽게 하도록 도와줌

   

   

### _3.1 Text-Dependent Speaker Verification_

존재하는 voice assistant는 보통 단일 키워드만 지원함에도 불구하고, 연구는 유저들이 다양한 키워드를 지원하는 것을 선호한다고 보여줌

구글 홈을 멀티 유저가 사용하는 경우, 두 개의 키워드가 동시에 지원됨: “OK Google”과 “Hey Google”

멀티 키워드에 대한 speaker verification 활성화는 TD-SV와 TI-SV의 사이에 위치, 왜냐하면 대본이 단일 구절로 제한되지도 않지만 완전히 자유로운것도 아니기 때문, 즉 특정 구절에만 국한되지 않고 자유롭게 다양한 문장을 사용할 수 있다는 것을 의미

- TD-SV는 텍스트가 미리 정해진 발화로 제한되어 있는 화자 확인 방법이고, TI-SV는 발화의 내용에 제한이 없어 발음과 발화 지속시간에 더 큰 변동성 존재
- 이에 비해 멀티 키워드에서의 화자 확인은 TD-SV 보다는 자유로우나 TI-SV 보다는 제한이 있는 중간 정도로 볼 수 있음

이러한 문제점을 MultiReader technique으로 해결 (섹션 $2.3$)

MultiReader는 유사 접근 방식과 비교할 때 강력한 장점을 가짐(ex. 여러 데이터 원본을 직접 섞는 것)

이 방법은 다양한 데이터 원본의 크기가 불균형할 때도 처리 가능, 즉 MultiReader는 다양한 데이터 원본의 크기가 다를 때에도 효과적으로 작동할 수 있는 기능을 가짐

- 이 케이스에서는, 훈련할 데이터셋이 두개:

1. “OK Google”이라는 익명의 사용자 쿼리에서 추출된 약 1억 5천만개의 발화와 63만명의 화자로 구성된 훈련 세트
2. 수동으로 수집된 약 120만개의 발화와 약 1만 8천명의 화자로 구성된 “OK/Hey Google” 훈련 세트

첫 번째 데이터 세트는 두 번째 데이터 세트보다 발화 수에서 약 125배, 화자 수에서 약 35배 더 큼

평가를 위해, 2가지 키워드를 등록하고 두 가지 중 하나의 키워드로 검증하는 4가지 사례에서의 Equal Error Rate(EER) 보고

- 모든 평가 데이터셋은 665명의 화자로부터 수동으로 수집되며, 평균 4.5개의 등록 발화와 10개의 평가 발화로 이루어짐

  <img width="710" alt="스크린샷 2024-01-07 오후 9 30 51" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/8ca3eb61-e8e8-40b6-a49e-6f5dd86364af">



   

   

- 결과는 ****\**\*Table 1\*\**\****에 있음
- MultiReader은 모든 4가지의 경우에 대해 상대적으로 약 30%의 향상을 이끌어냄
- 또한 약 83K 명의 각기 다른 화자와 환경 조건에서 수집된 더 큰 데이터셋으로 보다 포괄적인 평가 또한 진행
- 화자마다 평균 7.3개의 등록발화와 5개의 평가발화
- ****\**\*Table 2\*\**\****는 실험에서 사용된 다양한 손실함수에 대한 평균 EER을 MultiReader 설정을 사용하거나 사용하지 않고 훈련한 결과를 정리한 것
- 기준 모델은 512개 노드를 가진 단일 레이어의 LSTM이며 임베딩 벡터 크기는 128
- 둘째 셋째줄의 모델 아키텍처는 3 레이어 LSTM
- 두번째 줄과 세번째줄을 비교해보면, GE2E가 TE2E보다 약 10% 성능이 더 좋은 것을 확인 가능
- ****\**\*Table 1\*\**\****과 유사하게, MultiReader을 사용했을 때 모델이 눈에 띄게 나은 성능을 보이는 것을 알 수 있음
- 표에는 나타나지 않았지만, 언급할 가치가 있는것은 GE2E의 훈련 시간은 TE2E보다 약 60% 덜 걸림

## ***\**\*3.2 Text-Independent Speaker Verification\*\**\***

Ti-SV 훈련에서는, 훈련 발화를 작은 세그먼트로 나누고, partial utterance(부분 발화)로 지칭함

- 모든 부분 발화가 동일한 길이를 갖는 것을 필요로하진 않지만, 같은 배치에 있는 부분 발화는 동일한 길이여야 함
- 그래서 데이터의 각 배치마다, time length $t$를 $[lb, ub]=[140,180]$ 프레임에서 무작위로 정하고, 해당 배치의 모든 발화들이 같은 길이 $t$를 가지도록 함 (_Figure 3_에서와 같이)
  - 각 길이 $t$는 140에서 180 프레임 

<img width="717" alt="스크린샷 2024-01-07 오후 9 31 57" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/a795ddca-3276-4382-ae12-4f12499f66d8">

- 추론 동안, 매 발화마다 고정된 길이 $(lb+ub)/2 = 160$ 프레임의 슬라이딩 윈도우 적용, 50% 겹침 (50%씩 겹쳐서 이동)

- 각 윈도우마다 d-vector 계산

- 최종 발화 단위의 d-vector는 각 윈도우별로 얻은 d-vector을 L2 normalizing 함으로써 일반화됨 (_Figure 4_) ⇒ 벡터의 크기를 1로 만들어줌

- 정규화된 d-vector들은 원소별로 평균냄

  - 최종 발화단위의 d-vector: 화자를 나타내는 특정한 음성 신호를 벡터로 표현한 것, 해당 화자의 음성 특성을 나타냄
  - d-vector는 speaker embedding vector로, 화자의 음성을 고유하게 나타내는 벡터
  - 이 모델에서는 여러 프레임의 d-vector를 결합하여 최종 화자의 특징을 나타내는 단일 벡터를 만들게 됨, 이러한 벡터는 L2 정규화를 거쳐 화자의 음성 정보를 나타냄

- TI-SV 모델들은 18K의 화자들로부터의 약 36M개의 발화로 훈련됨, 익명화된 로그에서 추출됨 (개인정보가 보호되고 익명화된 상태에서 사용됨)

- 평가를 위해, 한 화자당 평균 6.3개의 등록 발화 7.2개의 평가 발화를 가진 1000명의 화자를 추가적으로 사용

- _Table 3_에서 서로 다른 loss 함수에 대한 성능 비교를 보여줌

  <img width="717" alt="스크린샷 2024-01-07 오후 9 33 09" src="https://github.com/Yjinaa/Yjinaa.github.io/assets/71372857/15aba237-6c09-4e9b-98c8-2af81c0bd5ea">



# 4. CONCLUSIONS

- 해당 논문에서, Speaker verification 모델을 더 효율적으로 학습시킬 수 있는 GE2E 손실 함수를 제안
- 이론적 그리고 실험적 결과 둘 다 이러한 손실 함수의 장점을 입증함
- 또한 서로 다른 데이터 소스를 결합하기 위한 MultiReader 기술을 도입, 이는 모델이 멀티 키워드와 멀티 언어를 지원할 수 있도록 해줌
- 이러한 두 기술을 결합함으로써, 더욱 정확한 speaker verification 모델을 만들 수 있음

즉, 각 발성-발성의 거리를 조정하던 기존의 triplet loss를 개선

각 화자의 centroid-발성의 거리를 조정, 동시에 다수의 화자를 고려하도록 개선



​       

​       

​      







