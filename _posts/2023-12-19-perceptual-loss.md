---
title: Perceptual loss 
date: 2023-12-19 18:21:00 +0300
description: Perceptual loss ì¡°ì‚¬
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [model, loss] # add tag
use_math: true
categories:
  - Study Notes
sidebar:
    nav: "blog"
---

# Perceptual loss in Image

- perceptual lossë€ feature reconstruction loss, style reconstruction lossë¥¼ ì •ì˜í•´ ì´ë¯¸ì§€ì˜ styleê³¼ contextë¥¼ ë³´ì¡´í•˜ê³ ì í•˜ëŠ” loss

## **Photo-Realistic Single Image Super Resolution Using a Generative Adversarial Network [[Link]](https://arxiv.org/pdf/1609.04802.pdf)**

- original imageì™€ ë¹„êµí–ˆì„ ë•Œ ê²°ê³¼ë¬¼ì˜ textual detailì´ ë–¨ì–´ì§
- ì—°êµ¬ì§„ë“¤ì€ ì´ ì›ì¸ì´ loss functionì— ìˆë‹¤ê³  ë´„
    - ê¸°ì¡´ pixel-wise MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê°€ë©´ high texture detailì„ ì œëŒ€ë¡œ ì¡ì•„ë‚´ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ì¡´ì¬
    - ì´ì „ ì—°êµ¬ì™€ëŠ” ë‹¤ë¥´ê²Œ VGG networkì˜ high-level feature mapì„ ì´ìš©í•œ perceptual lossë¥¼ ì œì‹œí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°

**Loss function**

- Perceptual loss = Content loss + adversarial loss
- adversarial lossëŠ” ì•Œê³  ìˆëŠ” ê²ƒê³¼ ë¹„ìŠ·, content lossëŠ” [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)ì—ì„œ ì²˜ìŒ ì œì‹œëœ perceptual lossì™€ ê±°ì˜ ë™ì¼
- ê° ì´ë¯¸ì§€ë¥¼ pre-trained CNNëª¨ë¸ì— í†µê³¼ì‹œì¼œ ì–»ì–´ë‚¸ feature mapì„ ë¹„êµí•˜ëŠ” ê²ƒì„ perceptual lossë¼ê³  í•¨
- ë™ì¼í•œ ì´ë¯¸ì§€ì´ë‚˜ í•œ pixelì”© ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë°€ë ¤ìˆëŠ” ë‘ ì´ë¯¸ì§€ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ë©´ lossê°€ 0ì´ì–´ì•¼ê² ì§€ë§Œ pixel wise lossëŠ” 0ì´ ë  ìˆ˜ ì—†ìŒ, illposed problemë•Œë¬¸ì— ì´ëŸ¬í•œ ë‹¨ì ì´ ë” ë¶€ê°ë¨
- illposed problem: ì €í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ê³ í•´ìƒë„ë¡œ ë³µì›í•  ì‹œ, ë³µì› ê°€ëŠ¥í•œ ê³ í•´ìƒë„ì˜ ì´ë¯¸ì§€ê°€ ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ëŠ” ê²ƒ, GANì´ ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ êµ¬í•´ì¤˜ë„ MSE based per pixel lossë¥¼ ì‚¬ìš©í•˜ë©´ possible solutionë“¤ì„ í‰ê· ë‚´ëŠ” ê²°ê³¼ë¥¼ ì·¨í•˜ê²Œ ë˜ë¯€ë¡œ, GANì´ ìƒì„±í•œ ë‹¤ì–‘í•œ high texture detailë“¤ì´ smoothing ë˜ëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜
- ì´ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ GANì´ ìƒì„±í•œ HRì´ë¯¸ì§€ì™€ Original HRì´ë¯¸ì§€ë¥¼ Pretrianed VGG 19ì— í†µê³¼ì‹œì¼œ ì–»ì€ Feature map ì‚¬ì´ì˜ ìœ í´ë¦¬ì•ˆ ê±°ë¦¬ë¥¼ êµ¬í•´ content lossë¥¼ êµ¬í•¨

## Perceptual losses for Real-Time Style Transfer and Super-Resolution [[Link]](https://arxiv.org/pdf/1603.08155.pdf)

- ê°œë³„ í”½ì…€ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‘ ì˜ìƒì„ ë¹„êµí–ˆì„ ë•Œ, perceptually ë˜‘ê°™ì•„ë„ per-pixel loss functionì— ê¸°ì´ˆí•˜ì—¬ ì„œë¡œ ë§¤ìš° ë‹¤ë¥¼ ê²ƒ
- VGGì˜ ë†’ì€ representationì„ ê¸°ë°˜ìœ¼ë¡œ ë‘ ì´ë¯¸ì§€ ë¹„êµ

# For Audio

## 1. **PERCEPTUAL LOSS FUNCTION FOR NEURAL MODELLING OF AUDIO SYSTEMS - ICASSP 2020 [[Link]](https://arxiv.org/pdf/1911.08922.pdf)**

<aside>
ğŸ’¡ **[ìš”ì•½]**
ì‚¬ëŒì˜ ì²­ê° ì¸ì§€ íŠ¹ì„±ì„ ë°˜ì˜í•œ **A-weighting filterë¥¼ ì ìš©**í•´ ì¶”ê°€ì ì¸ ê³„ì‚° ì—†ì´ ì‚¬ëŒ ê·€ì— ë” ë¹„ìŠ·í•˜ê²Œ ë“¤ë¦¬ë„ë¡ íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤ì˜ ì†Œë¦¬ë¥¼ ìƒì„± ê°€ëŠ¥í–ˆë‹¤!

</aside>

### Abstract

ì´ì „ ì—°êµ¬ì—ì„œëŠ” ì›ë³¸ ì‹ í˜¸ì™€ ì‹ ê²½ë§ ì¶œë ¥ ì‹ í˜¸ ì „ë¶€ì— pre-emphasis filterë¥¼ ì ìš©í–ˆìœ¼ë©°, ESR lossë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì›ë³µì„ ì˜í•˜ëŠ”ì§€ ì¸¡ì •í•¨

- ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” ë” perceptually relevantí•œ pre-emphasis filterë¥¼ ê³ ë ¤
    - high frequenciesì—ì„œ lowpass filteringì„ í¬í•¨ 
    â†’ ê³ ì£¼íŒŒ ëŒ€ì—­ì—ì„œ ë‚®ì€ ì£¼íŒŒìˆ˜ ì„±ë¶„ì„ ê°ì†Œ(ê³ ì£¼íŒŒëŒ€ì—­ì˜ ì„¸ë¶€ì ì¸ ì„±ë¶„ì„ ê°ì†Œ)ì‹œí‚´ìœ¼ë¡œì¨ ì¸ê°„ ì²­ê° ì‹œìŠ¤í…œì˜ íŠ¹ì„±ì„ ëª¨ë°©í•˜ê³ , ì¸ê°„ ê·€ë¡œ ë“¤ì„ ë•Œ ìì—°ìŠ¤ëŸ¬ìš´ ì†Œë¦¬ë¥¼ ìƒì„±

### A-weighting filter

- relative loudness of different frequenciesë¥¼ ê³ ë ¤í•˜ë„ë¡ ì„¤ì •ë¨

$$
H(z) = 1+0.85_z^{-1}
$$

- ì´ëŠ” ê³ ì£¼íŒŒ êµ¬ì—­ì˜ emphasisë¥¼ ì¤„ì—¬ì¤Œ (ê³ ì£¼íŒŒ êµ¬ì—­ì€ ìƒëŒ€ì ìœ¼ë¡œ ì—ë„ˆì§€ê°€ ì ê²Œ ì¡´ì¬í•˜ëŠ” ê²½í–¥, ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ê³ ì£¼íŒŒ ì˜ì—­ì„ ê°•ì¡°í•˜ëŠ” ê²ƒì„ ì¤„ì´ê³  ì „ì²´ì ì¸ ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ ê· í˜•ì„ ë§ì¶”ê³  ì²­ì·¨ìì˜ ì²­ê°ì  ê²½í—˜ì„ ê°œì„ )
- pre-emphasis filterëŠ” ì†ì‹¤ì„ ê³„ì‚°í•˜ê¸° ì „ì— ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ê³¼ ëª©í‘œ ì‹ í˜¸ì— ì ìš©ë¨, ì´ëŠ” ì…ë ¥ ì˜¤ë””ì˜¤ì™€ ëª©í‘œ ì˜¤ë””ì˜¤ ëª¨ë‘ì— í•„í„°ë¥¼ ì ìš©í•œ í›„, ì´ í•„í„°ë§ëœ ì‹ í˜¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì†ì‹¤ì„ ê³„ì‚° â†’ ëª¨ë¸ì´ ì†ì‹¤ ê³„ì‚° ì‹œ íŠ¹ì • ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ ì˜¤ë¥˜ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê²Œ í•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ì§€ê°ì ìœ¼ë¡œ ë” ìš°ìˆ˜í•œ ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ë„ë¡ ë„ì›€

**ì‹¤í—˜ í•„í„° ì¢…ë¥˜**

1. First-order highpass filter: ì „í†µì ìœ¼ë¡œ ìŒì„±ì²˜ë¦¬ì—ì„œ ì‚¬ìš©ë˜ì–´ ë‚®ì€ ì£¼íŒŒìˆ˜ë¥¼ ì–µì œ
2. Folded differentiator filter: ë†’ì€ ì£¼íŒŒìˆ˜ì™€ ë‚®ì€ ì£¼íŒŒìˆ˜ë¥¼ ëª¨ë‘ ê°ì‡ ì‹œí‚¤ëŠ” ê²ƒì´ ëª©ì 
3. A-weighting filter: ì¸ê°„ì˜ ì²­ê° ì¸ì‹ì„ ëª¨ë°©í•´ íŠ¹ì • ì£¼íŒŒìˆ˜ ëŒ€ì—­ì„ ê°•ì¡°
ì¤‘ê°„ ì£¼íŒŒìˆ˜ ëŒ€ì—­(ëŒ€ëµ 500Hzì—ì„œ 4kHz ì‚¬ì´)ì—ì„œ ê°€ì¥ ë¯¼ê°í•œ ì¸ê°„ì˜ ì²­ê°ì„ ê³ ë ¤, ì¤‘ê°„ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì„ ê°•ì¡°í•˜ê³  ë§¤ìš° ë‚®ê±°ë‚˜ ë†’ì€ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ ì†Œë¦¬ë¥¼ ê°ì‡ ì‹œí‚´(ì¸ê°„ì˜ ì²­ê°ì¸ì‹ê³¡ì„ ì— ê¸°ë°˜)
- ëª¨ë“  í•„í„°ë“¤ì€ í›ˆë ¨ì¤‘ì—ë§Œ ì‚¬ìš©ë˜ë©°, ì‹¤í–‰ì‹œ ì¶”ê°€ì ì¸ ê³„ì‚° ë¹„ìš© ì—†ì´ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ 

## 2. **Perceptual audio loss function for deep learning [[Link]](https://arxiv.org/pdf/1708.05987.pdf)**

<aside>
ğŸ’¡ **[ìš”ì•½]**
MSEì™€ ê°™ì€ ë©”íŠ¸ë¦­ì€ MOSì™€ ê°™ì€ subjective ë©”íŠ¸ë¦­ê³¼ correlationì´ ì ìŒ
PESQì™€ ê°™ì€ Perceptualí•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€í•˜ê³  ì‹¶ì€ë°, PESQëŠ” íŠ¹ì„±ìƒ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•˜ë©° ìƒ˜í”Œ ë‹¨ìœ„ í‰ê°€ê°€ ë¶ˆê°€í•´ì„œ í™œìš©í•˜ê¸° ì–´ë ¤ì›€
â†’ WaveNetì˜ large receptive fieldë¥¼ ì´ìš©í•´ì„œ PESQ ì•Œê³ ë¦¬ì¦˜ì„ ëª¨ë°©í•´ë³´ì!
â†’ PESQ ê²°ê³¼ë¥¼ ì£¼ê³  WaveNetì´ ì´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµì‹œí‚´
**Conclusion :** 0.25 ê¸¸ì´ì˜ ìƒ˜í”Œì„ í†µí•´ PESQê°’ê³¼ WaveNetì˜ ì˜ˆì¸¡ê°’ì„ ë¹„êµí–ˆì„ ë•Œ, 81%ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„

</aside>

- PESQ ì•Œê³ ë¦¬ì¦˜ ëª¨ë°©
    - ë¹„ì„ í˜•ì´ê³  ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ê³  ìˆì–´ ì˜¤ë””ì˜¤ ìƒ˜í”Œ ì‚¬ì´ì˜ ì¥ê¸°ì ì¸ ì˜ì¡´ì„± ê³ ë ¤í•¨ â†’ MSEì²˜ëŸ¼ ìƒ˜í”Œë‹¨ìœ„ ì•ˆë˜ê³  differentiableí•œ í•¨ìˆ˜ë¡œ í‘œí˜„ì´ í˜ë“¦, ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ ì „ì²´ì ì¸ í’ˆì§ˆì„ í‰ê°€
- ë”°ë¼ì„œ Wavenetì„ í™œìš©, í™•ì¥ëœ í•©ì„±ê³±ì„ ì‚¬ìš©í•´ ë§¤ìš° í° receptive fieldë¥¼ ê°€ì§€ê³ ìˆì–´ ì˜¤ë””ì˜¤ ìƒ˜í”Œ ì‚¬ì´ì˜ ì¥ê¸°ì ì¸ ì˜ì¡´ì„±ì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆìŒ
- Wavenetì„ í›ˆë ¨í•´ clean ì˜¤ë””ì˜¤ì™€ degraded ì˜¤ë””ì˜¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì´ë¥¼ í†µí•´ PESQ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•¨ â‡’ PESQ ê²°ê³¼ë¥¼ ì‚¬ìš©í•´ PESQ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë¨
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/4ea136f3-5df0-4c67-984c-786662dca6ae/Untitled.png)
    
- $P$ = differentianle loss function(trained to learn PESQ mapping)         $\lambda$ = [0,1]
- ì˜¤ë””ì˜¤ ìƒ˜í”Œ ê¸¸ì´ 0.25ì´ˆ, í™”ì ì•„ì´ë´í‹°í‹°ë¥¼ conditionìœ¼ë¡œ ì¤Œ â†’ í™”ìë³„ ë” ì •í™•í•œ PESQ ì ìˆ˜ í•™ìŠµ ê°€ëŠ¥
- ì¦‰ PESQì™€ ê°™ì€ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ë³µì¡í•œ í‰ê°€ ì²™ë„ë¥¼ ëª¨ë°©í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë¯¸ë¶„ ê°€ëŠ¥í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê°œë°œí•¨
- 0.25 ê¸¸ì´ì˜ ìƒ˜í”Œì„ í†µí•´ PESQë¥¼ ì¸¡ì •í–ˆì„ ë•Œ, 81%ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„

## 3. **PERCEPTUAL LOSS BASED SPEECH DENOISING WITH AN ENSEMBLE OF AUDIO PATTERN RECOGNITION AND SELF-SUPERVISED MODELS - ICASSP2021 [[Link]](https://arxiv.org/pdf/2010.11860.pdf)**

<aside>
ğŸ’¡ **[ìš”ì•½]**
Speech denoisingì€ perceptual quality í–¥ìƒì— ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒ â‡’ ê°ê¸° ë‹¤ë¥¸ ìŒì„± íŠ¹ì§•ì— ëŒ€í•œ pre-trained ëª¨ë¸ì˜ ì•™ìƒë¸”ë¡œ perceptual lossë¥¼ ìƒì„±í•´ë³´ì!
â†’ 6ê°œ ì‚¬ìš© = speaker classification, acoustic event model, speaker embedding, emotion classification, 2 self-supervised speech encoders
â†’ lossëŠ” ê° ëª¨ë¸ì˜ loss + STFT feature-domain L1 loss ì´ 7ê°œ ì‚¬ìš©
â†’ ê·¸ëŸ¬ë‚˜ 7ê°œë¥¼ ëª¨ë‘ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜ë¨ì„ ë°œê²¬
â†’ hand-tuningí•˜ì—¬ acoustic event model + L1 lossë¡œ í•´ë‹¹ ì„±ëŠ¥ì„ ë³µì›í•  ìˆ˜ ìˆì—ˆìŒ
â†’ ìµœì‹  MTL ë°©ë²•ë“¤ì´ greedy hand-tuning ê¸°ë°˜ weight ì„ íƒì„ ëŠ¥ê°€í•˜ì§€ ëª»í•œë‹¤!
â†’ PASE+ ëª¨ë¸ê³¼ acoustic event modelì´ ê°€ì¥ íš¨ê³¼ì ì´ì—ˆìŒ

</aside>

- ìŒì„± ë””ë…¸ì´ì§•ì€ ì¸ì§€ì ì¸ í€„ë¦¬í‹°ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŒ
    - Perceptual Ensemble Regularization Loss ì‚¬ìš©
- 6 large-scale pre-trained modelë¥¼ ì´ìš©í•´ ë¶„ì„
    - speaker classification, acoustic model, speaker embedding, emotion classification, 2 self-supervised speech encoders(PASE+, wav2vec2.0)
    - ì¦‰, ë‹¤ì–‘í•œ ìŒì„± íŠ¹ì„±ì— ëŒ€í•œ ì™œê³¡ì„ ìµœì†Œí™”í•¨ìœ¼ë¡œì¨, ì´ëŸ¬í•œ ê°œë³„ ì†ì‹¤ë“¤ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ì „ì²´ì ì¸ perceptual lossê°€ í˜•ì„±
- Conformer Transformer Networksë¥¼ ì´ìš©í•´ ê°•ë ¥í•œ ë² ì´ìŠ¤ë¼ì¸ì„ êµ¬ì¶•
+ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê²°í•©í•´ perceptual loss ê³„ì‚°
- í•œ ë²ˆì— í•˜ë‚˜ì”© ë³´ì¡° ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, acoustic event modelê³¼ self-supervised ëª¨ë¸ PASE+ê°€ íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ ë°í˜€ì§
- ë² ìŠ¤íŠ¸ ëª¨ë¸ì€ PERL-AE, only acoustic event modelë§Œ ì‚¬ìš©
- denoisingì´ full frameworkë¥¼ í™œìš© ê°€ëŠ¥í•˜ëƒë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•´, ëª¨ë“  ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•´ë´¤ì§€ë§Œ 7ê°œì˜ ë¡œìŠ¤ ì‹ì´ Multi-Task Learningì— ì–´ë ¤ì›€ì„ ê²ªì—ˆìŒ
- ì¦‰, sota Multi Task ê°€ì¤‘ì¹˜ í•™ìŠµ ë°©ë²•ì´ hand tuningì„ ëŠ¥ê°€í•˜ì§€ ëª»í•¨, ë„ë©”ì¸ ë¶ˆì¼ì¹˜ì™€ lossë“¤ì˜ ì•½í•œ ìƒí˜¸ë³´ì™„ì„± ë•Œë¬¸ì¼ ìˆ˜ ìˆìŒ
- ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ì™€ ì†ì‹¤ í•¨ìˆ˜ë“¤ì´ ì„œë¡œ ìƒí˜¸ ë³´ì™„ì´ ë˜ì§€ ì•Šê³ , ë‹¤ë¥¸ ë°ì´í„° ë„ë©”ì¸ê°„ì˜ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ìµœì  ì„±ëŠ¥ ë‹¬ì„±ì´ ì–´ë ¤ì›€

- simple l1 ë¡œìŠ¤ì™€ Conformer Transformerë¡œ ê°•ë ¥í•œ ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•
- PERLì´ë¼ëŠ” framework ì œì•ˆ, 6ê°œì˜ ì˜¤í”ˆì†ŒìŠ¤ large-scale pre-trained networksë¥¼ ì‚¬ìš©í•¨
    - perceptual lossë¡œ speech denoising analyze
- speexh tasksì—ì„œ perceptual loss trainingì˜ self-supervised representationsì˜ ìœ ìš©ì„±ì„ ì…ì¦
- l1 enhancement lossì™€ conformer componenetsê°€ sotaë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•¨ì„ ì…ì¦
- PERLì˜ 7ê°œì˜ ë¡œìŠ¤ë¥¼ ì „ë¶€ ì‚¬ìš©í•´ì„œ, sotaì˜ ë©€í‹°íƒœìŠ¤í‚¹ ê°€ì¤‘ì¹˜ í•™ìŠµ ë°©ë²•ì€ hand-tuned ê°€ì¤‘ì¹˜ë¥¼ ëŠ¥ê°€í•  ìˆ˜ ì—†ìŒì„ ì…ì¦

- l1 lossëŠ” denoisingì—ì„œì˜ ì™œê³¡ ë¬¸ì œ í•´ê²° ë¶ˆê°€
- perceptual loss trainingì„ ìœ„í•´ ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì‚¬ì „ í›ˆë ¨ëœ speech modelì˜ ì•™ìƒë¸”ì¸ PERL ì œì•ˆ
    - acoustic event classification, speaker embedding, acoustic model, speech emotion recognition + self-supervised speech encoders
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/9d9da34d-94d6-4314-a7db-1f626b38d480/Untitled.png)
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/cd79dc2f-6b22-43b0-99f4-4f63edc84358/Untitled.png)

- $n_i$ = lossë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ì¸µì˜ ê°œìˆ˜
- ë§ˆì§€ë§‰ í•­ì€ ë‹¨ìˆœí•œ STFT feature-domain L1 ì†ì‹¤ì„ ë‚˜íƒ€ëƒ„, ì‹ í˜¸ì˜ ì£¼íŒŒìˆ˜ ì˜ì—­ íŠ¹ì§•ì„ ë³´ì¡´í•˜ëŠ” ë° ì¤‘ìš”
- ê° ì†ì‹¤ì€ íŠ¹ì •í•œ ìŒì„±ì˜ íŠ¹ì„±ì— ì´ˆì ì„ ë§ì¶”ë©°, ê°ê° ì´ë²¤íŠ¸, ìŒí–¥, í™”ì, ê°ì •, PAE+ ë° wav2vecì— í•´ë‹¹í•˜ëŠ” ì†ì‹¤ì„, ê° ì†ì‹¤ì€ í•´ë‹¹ íŠ¹ì„±ì— ëŒ€í•œ ì™œê³¡ì„ ìµœì†Œí™” í•˜ëŠ” ë° ê¸°ì—¬
- $n_i$ëŠ” perceptual lossë¥¼ ìœ ë„í•˜ëŠ” ë° ì‚¬ìš©ëœ í•´ë‹¹ ë³´ì¡° ë„¤íŠ¸ì›Œí¬ì˜ ë‚´ë¶€ ë ˆì´ì–´ ìˆ˜
- ië²ˆì§¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ íŒŒìƒëœ perceptual lossëŠ” í•©ì‚°ë˜ê³  $n_i$ë¥¼ í†µí•´ ë‚˜ëˆ ì§
- ë³´ì¡° ë„¤íŠ¸ì›Œí¬ë“¤ì€ ê³ ì •ë˜ì–´ìˆìœ¼ë©° ì¶œë ¥ ê³µê°„ì„ ì œí•œí•˜ëŠ” ë° ë„ì›€ì„ ì¤Œ, ì´ëŠ” transfer learning, knoledge distillation, multi-task learningê³¼ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ë°©ì‹
- flexible í•˜ê¸° ë•Œë¬¸ì—, denoising ì´ì™¸ì—ë„ voice conversionë„ ì“¸ ìˆ˜ ìˆê³  domain adaptationë„ ë›°ì–´ë‚¨
    - ê·¼ë° ê°ê¸° ë‹¤ë¥¸ ë„ë©”ì¸ê³¼ featureë¡œ í›ˆë ¨ë˜ì—ˆê¸° ë•Œë¬¸ì— domain mismatchê°€ ìƒë‹¹í•  ìˆ˜ ìˆìŒ â†’ initial layer ì‚¬ìš© ê¶Œì¥
    - í…ŒìŠ¤íŠ¸ì…‹ ë¼ë²¨ì´ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ ì¤€ë¹„ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— evaluationì€ enhancement metricë§Œ ë³´ì—¬ì¤Œ
- í•´ë‹¹ í”„ë ˆì„ì›Œí¬ PERLëŠ” ê°œë³„ì ìœ¼ë¡œ, íŠ¹ì • ì¡°í•©ìœ¼ë¡œë„ íš¨ê³¼ì ì„
- acoustic eventì™€ PASE+ ëª¨ë¸ì´ ê°€ì¥ íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨
- regular feature-domain enhancement lossì˜ combiningì´ ìƒí˜¸ ë³´ì™„ì ì„ì„ ë°œê²¬í•¨
- Ablation studyì—ì„œ, PERLì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì‚¬ìš©í•  ë•Œ, 7ê°€ì§€ ë¡œìŠ¤ í”„ë ˆì„ì›Œí¬ê°€ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì—ì„œëŠ” ì–´ë ¤ì›€ì„ ê²ªê³  ì„±ëŠ¥ ì†ì‹¤ì„ ê²ªëŠ” ê²ƒì„ ë°œê²¬í•¨
- ì´ë¥¼ ìœ„í•´ ì—¬ëŸ¬ MTL ë°©ë²•ë“¤ì„ ì‹¤í—˜í•˜ì—¬, ìµœì‹  MTL ë°©ë²•ë“¤ì´ greedy hand-tuning ê¸°ë°˜ weight ì„ íƒì„ ëŠ¥ê°€í•˜ì§€ ëª»í•˜ì§€ë§Œ, í¥ë¯¸ë¡­ê²Œë„ ì´ ì‹œìŠ¤í…œì€ ìµœì ì˜ ì†ì‹¤ë¡œ (L1 + AcousticEvent)ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì†ì‹¤ëœ ì„±ëŠ¥ì„ ë³µì›í•  ìˆ˜ ìˆìŒì„ ê²°ë¡ ëƒ„
- í–¥í›„ì—ëŠ” PERLì„ ë” richí•œ í…ŒìŠ¤íŠ¸ setupì—ì„œ ë¶„ì„í•´ ë³´í¸ì ì¸ í–¥ìƒì„ ìœ„í•´ ë‚˜ì•„ê°€ê³ 
- SESQAì™€ ê°™ì€ ìë™ ìŒì„± í’ˆì§ˆ ì •ëŸ‰í™” ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•©í•˜ê³ 
- ì£¼ ë„¤íŠ¸ì›Œí¬ì™€ ë³´ì¡° ë„¤íŠ¸ì›Œí¬ ì‚¬ì´ì˜ ë„ë©”ì¸ ë¶ˆì¼ì¹˜ ì™„í™”ë¥¼ ìœ„í•œ bridge/adaptorë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤

## 5. **PERCEPTUAL ANALYSIS OF SPEAKER EMBEDDINGS FOR VOICE DISCRIMINATION BETWEEN MACHINE AND HUMAN LISTENING - ICASSP 2023 [[Link]](https://ieeexplore.ieee.org/document/10094782/authors#authors)**

<aside>
ğŸ’¡ **[ìš”ì•½]**
â†’ (1) ìƒˆë¡œìš´ speaker embedding loss ì‹ì„ ì œì•ˆí•˜ê³ , (2) ì¸ê°„ì˜ ìŒì„± ì¸ì‹ê³¼ ëª¨ë¸ì˜ speaker embedding ê°„ ì°¨ì´ë¥¼ ë¶„ì„

â†’ í˜„ì¬ speaker embeddingì€ latent í‘œí˜„ì´ í›ˆë ¨ ê³¼ì • ì¤‘ ëª…ì‹œì ìœ¼ë¡œ ê°ë… ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, í´ë˜ìŠ¤ ë‚´ ë³€ë™ì„±ìœ¼ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì €í•˜ë  ê°€ëŠ¥ì„± ì¡´ì¬
â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í†µí•©í•´ë³´ì! â†’ latent featuresì™€ í•´ë‹¹ classì˜ centerê°„ì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ì— ë”°ë¼ íŒ¨ë„í‹° ë¶€ì—¬
â†’ í›ˆë ¨ê³¼ì • ì¤‘ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ í†µí•©ë˜ì–´ í•¨ê»˜ ê°ë…ë¨ â‡’ í´ë˜ìŠ¤ ë‚´ ê±°ë¦¬ ìµœì†Œí™”, í´ë˜ìŠ¤ ê°„ ê±°ë¦¬ ìµœëŒ€í™”

ì´í›„ low-level acoustic featuresì™€ ì„ë² ë”©ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•¨
â†’ MFCCsê°€ ê°€ì¥ ë§ì€ ì •ë³´ë¥¼ ê³µìœ , í•˜ì§€ë§Œ ìŠ¤í”¼ì»¤ ì„ë² ë”©ì´ MFCCsë¥¼ í†µí•´ í¬ì°©í•œ ì •ë³´ëŠ” ë§ìœ¼ë‚˜ MFCCsë§Œìœ¼ë¡œëŠ” ì„ë² ë”©ì˜ ë³€ë™ì„±ì„ ì„¤ëª…í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²°ê³¼
â†’ ì´ëŠ” ìŠ¤í”¼ì»¤ ì„ë² ë”©ì´ higher level, compositional representationì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ

â†’ ì‚¬ëŒì—ê²Œ í…ŒìŠ¤íŠ¸ ê²°ê³¼, ì‚¬ëŒì˜ ì˜ˆì¸¡ê³¼ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ 70% ê°€ëŸ‰ ì¼ì¹˜
â†’ ê·¸ëŸ¬ë‚˜ noisyí•œ ìŒì„±ì˜ ê²½ìš° ì‚¬ëŒì˜ ì˜ˆì¸¡ì€ ê±°ì˜ ì¼ê´€ì ì¸ ë°˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ 12% ì €í•˜ë¨
â†’ ìŠ¤í”¼ì»¤ ìŒ ë³„ë¡œ ì§‘ê³„ëœ ì£¼ê´€ì  ìœ ì‚¬ì„± ì ìˆ˜ê°€ ëª¨ë¸ì— ì˜í•´ ì˜ˆì¸¡ëœ ìœ ì‚¬ì„± ì ìˆ˜ì™€ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„ (r=0.6)
â†’ ì´ëŠ” ëª¨ë¸ì´ ë‘ ìŠ¤í”¼ì»¤ë¥¼ í˜¼ë™í•˜ëŠ” ê²½ìš°, ì‚¬ëŒë“¤ ì—­ì‹œ ê·¸ ë‘ ìŠ¤í”¼ì»¤ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ

</aside>

- ì¸ê°„ì˜ ìŒì„±ì¸ì‹ê³¼ ê´€ë ¨í•´ í¬ì°©í•˜ëŠ” ì •ë³´ì— ëŒ€í•´ ì¡°ì‚¬
- ìŠ¤í”¼ì»¤ ì„ë² ë”© ê³µê°„ì—ì„œì˜ ìƒëŒ€ì  ê±°ë¦¬ëŠ” ì¸ê°„ ì²­ì·¨ì°¨ê°€ ì¶”ë¡ í•˜ëŠ” ëª©ì†Œë¦¬ì˜ ìœ ì‚¬ì„±ê³¼ ì ë‹¹íˆ ì¼ì¹˜
    - ê¸°ê³„ì™€ ì¸ê°„ì˜ ì²­ì·¨ê°€ ëª©ì†Œë¦¬ë¥¼ êµ¬ë³„í•  ë•Œ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìˆìŒì„ í™•ì¸
- ìŠ¤í”¼ì»¤ ì„ë² ë”©ì´ ê°•ê±´í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì„ë² ë”© í˜•ì„±ì´ ì§€ê°ì ìœ¼ë¡œë„ ê´€ë ¨ì„±ì´ ìˆë‹¤ê³  ê°„ì£¼ë  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ í™•ì¥ëœ ë¶„ì„ì€ ì§„í–‰ë˜ì§€ ì•Šì•˜ìŒ

- speaker discrimination íƒœìŠ¤í¬ë¥¼ ì¸ê°„ ì²­ì·¨ìì—ê²Œ ë¶€ì—¬í•´ ì–»ì€ speaker voice similarity predictionì„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ë¹„êµ
- ê°ê¸° ë‹¤ë¥¸ speakerì˜ ë°œí™”ë¥¼ 16kHzë¡œ ìƒ˜í”Œë§í•˜ê³  64 mel-scaled friquency bandsë¡œ ë³€í™˜
- CNNì„ í†µí•´ í™”ì ì¸ì‹
    - 5 blocks, 2d conv layer, batch norm, ReLU, MaxPooling + Linear layer to fixed size
    - final projection using linear and softmax layerê°€ classify í•˜ëŠ” ë° ì´ìš©ë¨
- unseen speakerì— ëŒ€í•´ì„œëŠ”, ëª¨ë¸ì€ ë‘ ë°œí™”ì— ëŒ€í•œ latent speaker embeddingì„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•´ ì§ì ‘ì ìœ¼ë¡œ ë¹„êµí•¨
- ì´ì „ì˜ d-vector í˜¹ì€ x-vector ê¸°ë°˜ approachëŠ” ì¶œë ¥ ì¸µì—ì„œ cross entropyë¥¼ í†µí•´ speaker embeddingì„ ì–»ìŒ
    - ì¶”ë¡ ì—ì„œëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ì œê±°ë˜ê³  speaker embeddingì´ ê±°ë¦¬ ë©”íŠ¸ë¦­ì„ ì´ìš©í•´ ë¹„êµë¨(ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°™ì€)
- ê·¸ëŸ¬ë‚˜, latent í‘œí˜„ì´ í›ˆë ¨ ê³¼ì •ì¤‘ì— ëª…ì‹œì ìœ¼ë¡œ supervisedë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, í´ë˜ìŠ¤ ë‚´ ë³€ë™ì„±ì€ ë‚¨ì•„ìˆìœ¼ë©° ì´ëŠ” ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆìŒ
- ë”°ë¼ì„œ ë¶„ë¥˜ ê³¼ì •ê³¼ latent embeddingì„ ê³µë™ìœ¼ë¡œ superviseí•˜ì—¬ cross entropyì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ë™ì‹œì— ìµœëŒ€í™”í•˜ê³ ì í•¨
    - ì¦‰, ë”¥ëŸ¬ë‹ ê³¼ì •ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ëª©ì í•¨ìˆ˜ì— ì§ì ‘ ë„£ì–´ì£¼ê² ë‹¤ëŠ” ê²ƒ
    - latent featuresì™€ í•´ë‹¹ class centerì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ì— ë”°ë¼ íŒ¨ë„í‹°ê°€ ë¶€ì—¬ë¨
    - ë”°ë¼ì„œ í›ˆë ¨ ê³¼ì • ì¤‘ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ì™€ ì„ë² ë”© ìœ ì‚¬ë„ê°€ í•©ì³ì ¸ ê°ë…ë¨
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/e93969f8-621b-4c81-ad33-2f25a4ea577e/Untitled.png)
    
    $m$ = length of the mini-batch     $z_i, y_i$ = latent, final layer outputs for the ith sample    $c_{y_i}$ = center vector of the corresponding speaker
    
    $s_i$ = binary target vector extracting the i-th speaker  $N_s$ = number of speakers  $D$   = embedding layer size
    
    - $c_{y_i}$ëŠ” sgdë¥¼ ì´ìš©í•´ ê° speakerì— ëŒ€í•´ í•™ìŠµë¨(lr = 0.01), ì£¼ë³€ì˜ í´ëŸ¬ìŠ¤í„°ë§ëœ ê°ê°ì˜ í‘œí˜„ìœ¼ë¡œ
    - ì„œë¡œ ë‹¤ë¥¸ ìŠ¤í”¼ì»¤ëŠ” ë¶„ë¦¬í•˜ê³ , ë™ì¼ ìŠ¤í”¼ì»¤ì— ëŒ€í•œ ê±°ë¦¬ëŠ” ìµœì†Œí™”
- speaker êµ¬ë³„ì— ìˆì–´ ëª¨ë¸ì´ ì–´ë–¤ acoustic featureë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ low-level íŠ¹ì§•ê³¼ì˜ ê´€ê³„ì— ë”°ë¼ ë¶„ì„

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/d1d515cc-4c3e-4f02-93ff-c184d4dac7d0/Untitled.png)

- ì´ëŸ¬í•œ íŠ¹ì§•ê³¼ ìŠ¤í”¼ì»¤ ì„ë² ë”©ê°„ì˜ ê´€ê³„ë¥¼ í‰ê°€í•´ ì„œë¡œë¥¼ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ëŠ” ì¡°í•©ì„ ì‹ë³„
- ì„ í˜•íšŒê·€ ì ‘ê·¼ë²• ì‚¬ìš©
- cepstralê³¼ chroma featureê°€ speaker embeddingê³¼ ê°€ì¥ ë§ì€ ì •ë³´ë¥¼ ê³µìœ 
- ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•ì€ ì„ë² ë”©ì— ì˜í•´ ì˜ ì˜ˆì¸¡ë  ìˆ˜ëŠ” ìˆì—ˆìœ¼ë‚˜ ì„ë² ë”© ë¶„ì‚°ì„ ì„¤ëª…í•˜ì§„ ëª»í–ˆìŒ
- ìŠ¤í”¼ì»¤ ì„ë² ë”©ì´ MFCCsë¥¼ í†µí•´ í¬ì°©í•œ ì •ë³´ì˜ ì–‘ì€ ë§ìœ¼ë‚˜ MFCCsë§Œìœ¼ë¡œ speaker embeddingì˜ ë³€ë™ì„±ì„ ì„¤ëª…í•˜ê¸°ëŠ” ì–´ë µë‹¤ëŠ” ê²ƒ

- ì‚¬ëŒì—ê²Œ ë‘ ìŒì„±ì„ ì£¼ê³  ê°™ì€ ì‚¬ëŒì¸ì§€ ë‹¤ë¥¸ ì‚¬ëŒì¸ì§€ í‰ê°€í•˜ê²Œ í•¨
    - (í´ë¦°í•œ ìŠ¤í”¼ì¹˜ ì¡°ê±´) í‰ê·  70.4%ì˜ ì •í™•ë„ë¡œ ìŠ¤í”¼ì»¤ ê²€ì¦ ì‘ì—…ì„ ìˆ˜í–‰í–ˆê³ , ì´ ì¤‘ 69.2%ê°€ ëª¨ë¸ì— ì˜í•´ ì˜ˆì¸¡ëœ ê²°ê³¼ì™€ ì¼ì¹˜
    - (noisyí•œ ìŠ¤í”¼ì¹˜ ì¡°ê±´) í‰ê·  73.6%ë¡œ ì—¬ì „íˆ robustí–ˆìœ¼ë‚˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ 12% ê°ì†Œí•¨
- í”¼ì–´ìŠ¨ ìƒê´€ë¶„ì„ ê²°ê³¼, clean ì¡°ê±´ì—ì„œëŠ” ê°ê´€ì  ì ìˆ˜ì™€ ì£¼ê´€ì  ì ìˆ˜ ê°„ ë” ê°•í•œ ìƒê´€ê´€ê³„ê°€ ê´€ì°°ë¨(r=0.65)
    - ê·¸ëŸ¬ë‚˜ noisyí•œ ì¡°ê±´ì—ì„œëŠ” r=0.51
- ìŠ¤í”¼ì»¤ ìŒ ë³„ë¡œ ì§‘ê³„ëœ ì£¼ê´€ì  ìœ ì‚¬ì„± ì ìˆ˜ê°€ ëª¨ë¸ì— ì˜í•´ ì˜ˆì¸¡ëœ ìœ ì‚¬ì„± ì ìˆ˜ì™€ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„ (r=0.6)
    - ì´ëŠ” ëª¨ë¸ì´ ë‘ ìŠ¤í”¼ì»¤ë¥¼ í˜¼ë™í•˜ëŠ” ê²½ìš°, ì‚¬ëŒë“¤ ì—­ì‹œ ê·¸ ë‘ ìŠ¤í”¼ì»¤ë¥¼ êµ¬ë³„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ
- ìŠ¤í”¼ì»¤ ìœ ì‚¬ì„±ì€ í•™ìŠµëœ ìŠ¤í”¼ì»¤ ì„ë² ë”©ì˜ ì½”ì‚¬ì¸ ê±°ë¦¬ì— ì˜í•´ objectiveí•˜ê²Œ ê²°ì •ë¨
    - ì œì•ˆëœ ê³µë™ lossëŠ” baseline(x-vector approach)ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ê³¼ ë…¸ì´ì¦ˆ ê°•ê±´ì„±ì„ ë³´ì—¬ì¤Œ
- ê° acoustic feature í•˜ë‚˜ì”©ë§Œìœ¼ë¡œëŠ” í‘œí˜„ë  ìˆ˜ ì—†ìŒ, ì´ëŠ” ìŠ¤í”¼ì»¤ ì„ë² ë”©ì´ higher level, compositional representationì„ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ
- ëª¨ë¸ ì˜ˆì¸¡ê³¼ ì‚¬ëŒì˜ ì¸ì§€ì  ì˜ˆì¸¡ì€ 70%ì˜ ì¼ì¹˜ìœ¨ì„ ë³´ì„, SNR 5dBì—ì„œ ì¸ê°„ ì²­ì·¨ìë“¤ì€ ì˜í–¥ì„ ë°›ì§€ ì•Šì•˜ìœ¼ë‚˜ ëª¨ë¸ì€ 12%ê¹Œì§€ ì„±ëŠ¥ì´ ë–¨ì–´ì§

## 6. **Speech Denoising with Deep Feature Losses [[Link]](https://arxiv.org/pdf/1806.10522.pdf)**

<aside>
ğŸ’¡ **[ìš”ì•½]**
denoisingì— ë°°ê²½ ì†ŒìŒì„ ì œê±°í•˜ê¸°ê°€ í˜ë“¦ â‡’ ì¼ë°˜ì ì¸ L1 loss ì‚¬ìš© ì‹œ perceptually ì¤‘ìš”í•œ low energy speech ì •ë³´ë¥¼ ë¶€ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬
â‡’ auxiliary networkì˜ ê° activationì˜ ì°¨ì´ì— ëŒ€í•´ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” **deep feature loss** ì‚¬ìš©
â‡’ cvì—ì„œëŠ” ImageNetì„ auxiliary networkë¡œ í™œìš©í–ˆëŠ”ë°, ìŒì„±ì—ëŠ” ê·¸ëŸ¬í•œ standardí•œ ëª¨ë¸ ë¶€ì¬
â‡’ ìŒì„± classification ëª¨ë¸ì„ ì§ì ‘ í›ˆë ¨ì‹œì¼œ auxiliary networkë¡œ í™œìš©í•¨

</aside>

- Under-determined case of single-channel speech denoising ì–´ë ¤ì›€
    - ì£¼ì–´ì§„ ì˜¤ë””ì˜¤ ì‹ í˜¸ì—ì„œ ìŒì„±ê³¼ ë…¸ì´ì¦ˆë¥¼ ë¶„ë¦¬í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•œ ìƒí™©ì˜ single channel speech denoising
    - ë‹¨ì¼ ì±„ë„ ì˜¤ë””ì˜¤ ì‹ í˜¸ì—ì„œ ë³µì¡í•˜ê³  ë¶ˆí™•ì‹¤í•œ ë°°ê²½ ì†ŒìŒì„ ì œê±°í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš´ ì‘ì—…ì„
- ë‘ wav ì‚¬ì´ì˜ ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•´, pretrained audio classification metworkë¥¼ ì´ìš©í•˜ì—¬ internal activation patternsë¥¼ ë¹„êµí•¨ â†’ ê°ê¸° ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ ìˆ˜ë§ì€ featuresë¥¼ ë¹„êµ
    
    <aside>
    ğŸ‘‰ content lossë¥¼ ë³¸ë”°ì˜¨ë“¯.. cnnì—ì„œ pretrainedëœ vgg ì´ìš©í•´ì„œ ì´ˆê¸° ë ˆì´ì–´ì—ì„œ ì´ë¯¸ì§€ì˜ low level íŠ¹ì„±ì„ ë½‘ì•„ë‚´ì–´ ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ìœ ì§€ â†’ ì›ë³¸ ì´ë¯¸ì§€ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ ì‚¬ì´ì˜ feature mapì˜ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
    
    </aside>
    
- L1 lossë¥¼ ì‚¬ìš©í–ˆì„ë•Œ, ë‚®ì€ SNRì—ì„œ ì¶œë ¥ í’ˆì§ˆ í˜„ì €íˆ ì €í•˜, ì¦‰ ì§€ê°ì ìœ¼ë¡œ ì¤‘ìš”í•œ low energy speech ì •ë³´ë¥¼ ë¶€ì ì ˆí•˜ê²Œ ì²˜ë¦¬
    - â‡’ pretrained ë„¤íŠ¸ì›Œí¬ì˜ internal activationì˜ ì°¨ì´ì— ëŒ€í•´ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•˜ëŠ” deep feature loss ì‚¬ìš©
    - layered networkì˜ íŠ¹ì„±ìƒ, loss ë„¤íŠ¸ì›Œí¬ì—ì„œ ë‹¤ë¥¸ depthì˜ feature activationì€ ì‹œê·¸ë„ì˜ ë‹¤ë¥¸ íƒ€ì„ ìŠ¤ì¼€ì¼ì— í•´ë‹¹
    - ì´ëŸ¬í•œ activationì—ì„œì˜ ì°¨ì´ì— íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•¨ìœ¼ë¡œì¨, ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ ìŠ¤ì¼€ì¼ì— ëŒ€í•´ ì—¬ëŸ¬ featureë“¤ì„ ë¹„êµí•˜ê²Œ ë¨ â†’ ì¦‰, ë‹¤ì–‘í•œ ë©´ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ!
- ê·¸ëŸ¬ë‚˜ ImageNetê³¼ ê°™ì€ standardí•œ ëª¨ë¸ì´ ìŒì„±ìª½ì—” ì•„ì§ ì—†ê¸° ë•Œë¬¸ì—, ì§ì ‘ í›ˆë ¨ì‹œí‚´
    - 15 conv layers with 3*1 kernels, batch norm, LReLU units, zero padding
    - ë‘ ê°€ì§€ íƒœìŠ¤í¬ (scene type ë§ì¶”ê¸°, event ë§ì¶”ê¸°)
    - xavier ì´ˆê¸°ê°’, adam, lr $10^{-4}$, 2500 epochs (4,680 iterations for 1 epoch)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/2558e51f-b83d-439a-abbe-bdec3cfe3090/Untitled.png)
    
    $\Beta$ = clean signal   $\theta$ = denoising network parameter   $\phi^m$ = mth feature layer of feature loss network $g(x)$ = output of the denoising network
    
    $\lambda_m$ = $\lambda=1$ë¡œ ì„¤ì • í›„ 10 ì—í¬í¬ í›ˆë ¨ í›„ì˜ ì—­ìˆ˜     feature lossëŠ” weighted L1 lossë¡œ ì •ì˜ë¨
    
- perceptual experiments
    - Amazon Mechanical Turkì—ì„œ A/B Test

## 7. FEATURE ENHANCEMENT WITH DEEP FEATURE LOSSES FOR SPEAKER VERIFICATION - ICASSP 2020 [[Link]](https://arxiv.org/pdf/1910.11905.pdf)

<aside>
ğŸ’¡ **[ìš”ì•½]**
SV í•  ë•Œ, ë…¸ì´ì¦ˆê°€ í¬í•¨ë˜ë‹ˆ ì„±ëŠ¥ì´ ë–¨ì–´ì§ â‡’ speech enhancementë¥¼ ì´ìš©í•´ë³´ì! [6. **Speech Denoising with Deep Feature Losses [[Link]](https://arxiv.org/pdf/1806.10522.pdf)**]ì—ì„œ ì“°ì˜€ë˜ Deep Feature Lossë¥¼ ì¨ë³´ì.
â†’ ì´ì „ ì—°êµ¬ì™€ ë‹¤ë¥´ê²Œ auxiliary networkì˜ íƒœìŠ¤í¬ë¥¼ x-vector network taskë¡œ ì§€ì •í•´ ê°™ë„ë¡ í•˜ì.
â†’ log mel-filterbank ì‚¬ìš©í•´ feature domain enhance â†’ MFCC ê¸°ë°˜ auxiliary networkì™€ í˜¸í™˜ì„±ì„ ë†’ì´ì.
â†’ í›¨ì”¬ ë” í° ë°ì´í„°ì…‹ì„ ì¨ë³´ì.
â†’ DFLê³¼ FLì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•´ë³´ê³  ë¹„êµ (DFL ë‹¨ë…, FL ë‹¨ë…, DFL+FL)

**Conclusion:** DFL ë‹¨ë… ì‚¬ìš©ì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ìœ¼ë©°, DFL + FLì€ baselineë³´ë‹¤ë„ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•˜ë‹¤!

</aside>

- SVì— speech enhancementê°€ key ë¼ê³  ì—¬ê¹€, DFLì„ ì´ìš©í•´ enhancementí•´ì„œ SVì˜ ì„±ëŠ¥ì„ ë°œì „ì‹œì¼œë³´ì! (x-vector based SV systems)
- [**Speech Denoising with Deep Feature Losses]** ë¥¼ ì°¸ê³ , ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ì ì€
    1. auxiliary taskë¥¼ x-vector network taskë¡œ ì§€ì •í•´ ê°™ê²Œ í•¨ i.e. speaker classification
    2. feature-domainì„ enhance â†’ log mel-filterbank, MFCC ê¸°ë°˜ auxiliary networkì™€ í˜¸í™˜ì„±ì„ ë†’ì„
    3. í›¨ì”¬ ë” í° ê·œëª¨ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ proof-of-concept. 
    4. ì¶”ë¡  ì¤‘ì—ë§Œ enhancementë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ x-vector network ì¬í›ˆë ¨í•  í•„ìš”ê°€ ì—†ìŒ
- Loss
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/8c077d86-32bc-4f03-8dc3-2a630d985947/Untitled.png)
    
    - $F_n, F_c: F*T$ í–‰ë ¬, nì€ noisy, cëŠ” cleaní•œ ìƒ˜í”Œì„ ë„£ì–´ ì–»ì€ feature í–‰ë ¬ì„ ëœ»í•¨, FëŠ” ì£¼íŒŒìˆ˜ binì˜ ìˆ˜, TëŠ” í”„ë ˆì„ ê°œìˆ˜
    - $e(\cdot)$: enhancement network, $a(\cdot)$: auxiliary network
    - $a_i(\cdot)$: i ë²ˆì§¸ layerì˜ output, L = layerì˜ ê°œìˆ˜
    - $L_{DFL}, L_{FL}$ì˜ ê³„ìˆ˜ëŠ” 1ë¡œ ê³ ì • â†’ coefficient re-weighting schemeë¥¼ ì‹œë„í–ˆì§€ë§Œ ë„ì›€ ë˜ì§€ ì•Šì•˜ìŒ
    - 3 ê°€ì§€ì˜ loss functionì„ ì œì•ˆ, ê²°ë¡ ì ìœ¼ë¡œëŠ” DFL ë‹¨ë… ì‚¬ìš©ì´ ê°€ì¥ ì¢‹ì•˜ê³ , FL+DFLì€ ì˜¤íˆë ¤ baselineë³´ë‹¤ ì¢‹ì§€ ì•Šì€ ì„±ëŠ¥ì„ ë³´ì„

- **speaker embedding**
    - residual network
    auxiliary networkë¡œ ResNet-34-LDEë¥¼ í™œìš©, Angular Softmax loss function í™œìš© (LDE size=64, network has 5.9 parameters)
    - x-vector network
    2ê°œì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‹¤í—˜ â‡’ ETDNNê³¼ FTDNN
    ETDNNì€ TDNNë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ, convolution layers ì‚¬ì´ì—ì„œ interleaving dense layersí•¨ìœ¼ë¡œì¨
    FTDNNì€ convolution layersê°„ì˜ weight matrixë¥¼ two low rank matricesë¡œ product ì‹œí‚´
    ETDNNê³¼ FTDNNì˜ íŒŒë¼ë¯¸í„°ëŠ” ê°ê° 10Mì™€ 17M
- training
    - batchsize 128, 50 epochs, Adam, lr 0.0075 with warmup, 800frames
    - trained with mean-normalized log Mel-filterbank features
    - ì´ëŸ¬í•œ normalization mismatchë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, enhancementì™€ auxiliaryì‚¬ì´ì— online mean normalizationì„ ì‹œí–‰
    - ETDNNê³¼ FTDNNì€ Mean-Variance Normalized 40-D MFCC featuresë¥¼ ì´ìš©í•œ Kaldi scriptsë¡œ í›ˆë ¨ë¨
    â‡’ FTDNNì˜ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/de8cded5-223c-4f3d-a7de-9531293b95a6/Untitled.png)

## 8. PL-EESR: PERCEPTUAL LOSS BASED END-TO-END ROBUST SPEAKER REPRESENTATION EXTRACTION - IEEE ASRU 2021 [[Link]](https://arxiv.org/pdf/2110.00940.pdf)

<aside>
ğŸ’¡ **[ìš”ì•½]**
speech enhancementì˜ ëª©ì ì€ noiseë¥¼ ì¤„ì„ìœ¼ë¡œì¨ ì¸ì§€ì  í€„ë¦¬í‹°ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ
ê·¸ëŸ¬ë‚˜ ì§€ë‚˜ì¹œ supressionì€ ìŒì„±ì˜ ì™œê³¡ì„ ì´ˆë˜í•˜ê³  speaker embedding extractionì˜ ì„±ëŠ¥ê¹Œì§€ ì €í•˜ì‹œí‚´
â‡’ robust speaker representation extractionì„ ìœ„í•œ PL-EESR ì œì•ˆ

[7. FEATURE ENHANCEMENT WITH DEEP FEATURE LOSSES FOR SPEAKER VERIFICATION - ICASSP 2020 [[Link]](https://arxiv.org/pdf/1910.11905.pdf)]ì™€ ìœ ì‚¬í•œ ì ‘ê·¼ ë°©ë²• ì‚¬ìš©(activationì„ ë¹„êµ)í–ˆìœ¼ë‚˜ í•´ë‹¹ ë…¼ë¬¸ì˜ ë‹¨ì  ì¡´ì¬í•¨
â†’ ë”°ë¼ì„œ ì´ ë‹¨ì ë“¤ì„ í•´ê²°í•œ joint training network ì œì•ˆ

ì´ 3ë‹¨ê³„ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ì§ â‡’ pre-training 1, pre-training 2, fine-tuning
â†’ pre-training 1: speaker representation moduleì´ cross entropyë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ë˜ê³  fix ë¨
â†’ pre-training 2: cross entropyì™€ speaker representation module(=auxiliary network)ì„ ì´ìš©í•œ perceptual lossë¡œ enhancement moduleì´ í›ˆë ¨ë¨
â†’ fine-tuning: ë‘ ëª¨ë“ˆì´ í•¨ê»˜ í›ˆë ¨ë¨, initial rate=0.0001ë§Œ ë‹¬ë¼ì§€ê³  ë‚˜ë¨¸ì§€ ì¡°ê±´ì€ pre-training 2ì™€ ê°™ìŒ

</aside>

### Abstract

- speech enhancementì˜ ëª©ì ì€ background noiseë¥¼ ì¤„ì„ìœ¼ë¡œì¨ ì¸ì§€ì  í€„ë¦¬í‹°ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ
- ê·¸ëŸ¬ë‚˜ ì§€ë‚˜ì¹œ supressionì€ ìŒì„±ì˜ ì™œê³¡ì„ ì¼ìœ¼í‚¤ê³  speaker embedding extractionì˜ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¼
- ì´ëŸ¬í•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, end-to-end learning frameworkë¥¼ ì œì•ˆ, PL-EESR (robust speaker representation extraction)
- ì´ í”„ë ˆì„ì›Œí¬ëŠ” speaker identification íƒœìŠ¤í¬ì˜ í”¼ë“œë°±ê³¼ raw ìŒì„± ì‹œê·¸ë„ê³¼ ê·¸ noisy version ê°„ high-level perceptual deviationì— ê¸°ë°˜í•˜ì—¬ ìµœì í™”ë¨
- noisyí™˜ê²½ê³¼ clean í™˜ê²½ ê°ê°ì—ì„œ speaker verification ì§„í–‰

### Introduction

- [7. FEATURE ENHANCEMENT WITH DEEP FEATURE LOSSES FOR SPEAKER VERIFICATION - ICASSP 2020 [[Link]](https://arxiv.org/pdf/1910.11905.pdf)]ì™€ ìœ ì‚¬í•œ approach(auxiliary networkì— í†µê³¼ì‹œì¼œ activationì„ ë¹„êµí•˜ëŠ” ë°©ë²•)ë¥¼ ì‚¬ìš©, ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ ì ì€
    - ìœ„ ì—°êµ¬ì—ì„œì˜ auxiliary networkëŠ” speaker representation networkë‘ì€ ë‹¤ë¦„
    - í† íƒˆ íŒŒë¼ë¯¸í„°ê°€ 26.1Mì¸ë°, ì´ëŠ” standard x-vector networkë³´ë‹¤ í¼ â‡’ ë‘ ë‹¨ê³„ trainingì„ ê±°ì³ì•¼í•˜ê³ , enhancementëª¨ë¸ì´ verification ëª¨ë¸ê³¼ ë…ë¦½ì ì„
    - perceptual lossê°€ clean speechì¸ì§€ ì•„ë‹Œì§€ì— í¬ê²Œ ì˜ì¡´í•¨ (manuallyí•˜ê²Œ ì„ íƒí•´ì•¼ í•¨)
    - ë”°ë¼ì„œ ì´ ì—°êµ¬ì—ì„œëŠ” end-to-end joint training networkë¥¼ ì œì•ˆí•¨, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” 9.8M
        - perceptual lossì™€ cross entropy lossë¡œ ìµœì í™”ë˜ëŠ” ê°•ê±´í•œ end-to-end speaker representation network ì œì•ˆ
        - task-specific enhancement moduleê³¼ spaekr embedding extraction moduleë¡œ êµ¬ì„±ë¨
        - ë¨¼ì € ì„ë² ë”© ëª¨ë¸ì´ speaker identification taskë¡œ í›ˆë ¨ë˜ê³  fixë¨
        - ë‹¤ìŒìœ¼ë¡œ enhancement ëª¨ë“ˆì´ cross entropyì™€ perceptual lossë¡œ í›ˆë ¨ë¨, ì´ ë•Œ auxiliary networkë¡œ speaker embedding extraction ëª¨ë“ˆì„ ì‚¬ìš©
        - ì ì¬ì ì¸ mismatchë¥¼ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ ë‘ ëª¨ë“ˆì€ ë™ì‹œì— fine tuning
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/13edd200-5408-4352-b6b7-b063b3db91c8/Untitled.png)
        

### Perceptual loss

- Problem
    - ì „í†µì ìœ¼ë¡œ, speech enhancement ë„¤íŠ¸ì›Œí¬ëŠ” clean spectrogramê³¼ ground-truth ê°„ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ í›ˆë ¨ë¨
    - ê·¸ëŸ¬ë‚˜ ì´ëŠ” speaker ê´€ë ¨ ì •ë³´ ì†ì‹¤ í˜¹ì€ ì™œê³¡ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ
    - enhanced spectrogramìœ¼ë¡œ embedding taskì— ì§ì ‘ì ìœ¼ë¡œ ì ìš©ì‹œí‚¨ë‹¤ë©´, ì„±ëŠ¥ì´ ì €í•˜ë  ê²ƒì´ë©° íŠ¹íˆ high SNR ì»¨ë””ì…˜ì—ì„œëŠ” ë”ìš± ì‹¬í•´ì§
    - ì—°êµ¬ì§„ì€ ì´ê°€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ê°€ ì˜ˆì¸¡ê°’ê³¼ ground truthê°„ perceptual differenceë¥¼ í¬ì°©í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ë´¤ìŒ
- perceptual loss
    - deep networkëŠ” íˆë“  ë ˆì´ì–´ì— ë”°ë¼ ê°ê¸° ë‹¤ë¥¸ activationsë¥¼ ë³´ì—¬ì£¼ê³ , ì´ëŠ” input featureì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ë„ë¡ í•¨
    - ì´ëŸ¬í•œ ê°€ì •ì— ê¸°ë°˜í•´ì„œ, activationê°„ì˜ ê±°ë¦¬ì¸ perceptual lossê°€ ì œì•ˆë¨
    - perceptual lossì˜ ì›ë¦¬ì— ë”°ë¥´ë©´, auxiliary networkëŠ” ì–´ë–¤ ì •ë³´ë¥¼ keepí• ì§€ ì •í•¨ â‡’ ëª©í‘œëŠ” speaker relative informationì„ ê°•í™”í•˜ëŠ” ê²ƒ
    - ë”°ë¼ì„œ speaker verification networkê°€ ë¨¼ì € í›ˆë ¨ë  ìˆ˜ ìˆê³ , ê·¸ í›„ auxiliary networkë¡œì¨ fixë¨ â‡’ enhancement moduleì„ ìœ„í•´ perceptual lossë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/749115d9-efac-4d87-867c-36e733096b7d/Untitled.png)

- $\theta$ = enhance module   $\phi$ = auxiliary speaker embedding extraction module    $\phi_i$ = ith layer of embedding extraction module

### Method

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/6f627438-018f-4c9a-926a-5ae5196fa1af/Untitled.png)

- (d)ë¥¼ ë³´ë©´, 30ì°¨ì›ì˜ log-MEl spectrogramì´ speech signalë¡œë¶€í„° ì¶”ì¶œë˜ê³  ì´ëŠ” enhancement ëª¨ë“ˆê³¼ representation moduleì—ì„œ ë‹¤ ì“°ì„
- corpus chaanelì˜ ì˜í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•´, channel normalization ìˆ˜í–‰
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/ecd1b987-17c8-43a1-8a19-09e65ac46036/Untitled.png)
    
    $\mu_n, \sigma_n$ = noisy channelì˜ mean, deviation           $k$ = index of Mel-filter        $X(k)$ = feature corresponding to $k_{th}$ MEl-filter
    
- enhanced speechì— inverse normalization ìˆ˜í–‰
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/df59e04c-a3bf-41e2-8895-de18fc094aac/Untitled.png)
    
    $\sigma_c, \mu_c$ = clean versionì˜ mean, deviation
    
- instance normalization ìˆ˜í–‰
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/4ffcfa94-f34f-4ad6-99fc-47939b72666e/Untitled.png)
    
    $\sigma_i, \mu_i$ = ê° $\hat{s}_i$ì˜ mean, deviation
    

**enhancement module**

- temporal context ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•´, 3ê°œì˜ BLSTM(128)ê³¼ í•˜ë‚˜ì˜ fc layerê°€ enhance moduleì— ì¡´ì¬
- sigmoidê°€ fc layerì— ì ìš©ë¨ (0ì—ì„œ 1ê¹Œì§€ì˜ maskë¥¼ ìƒì„±í•˜ê¸° ìœ„í•¨)

**verification module**

- x-vector ì±„íƒ, ì²« 5ê°œì˜ TDNN ë ˆì´ì–´ê°€ frame-level informationì¶”ì¶œ
- 2ê°œì˜ fc layerê°€ utterance-levelì˜ ì •ë³´ ì¶”ì¶œ
- ë‘ ë‹¨ê³„ëŠ” global average pooling layerë¡œ ì—°ê²°ë¨
- ê° ë ˆì´ì–´ì˜ activationì´ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì•Œì§€ ëª»í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë“  5ê°œì˜ TDNN ë ˆì´ì–´ì™€ ì²« ë²ˆì§¸ fc layerë¥¼ trainingì—ì„œì˜ perceptual loss ê³„ì‚°ì— ì‚¬ìš©
- ì¶”ë¡  ì‹œ speaker embeddingì€ ì²« ë²ˆì§¸ fully connected layerì—ì„œ ì¶”ì¶œë¨

**Training Strategy**

- [7. FEATURE ENHANCEMENT WITH DEEP FEATURE LOSSES FOR SPEAKER VERIFICATION - ICASSP 2020 [[Link]](https://arxiv.org/pdf/1910.11905.pdf)]ì˜ speech enhancement networkë¥¼ í›ˆë ¨í•˜ê¸° ìœ„í•´ (1)ì´ ì‚¬ìš©ë¨
- ì´ëŠ” (b)ì— ë¬˜ì‚¬ë˜ì–´ ìˆìŒ
- clean utteranceê°€ targetìœ¼ë¡œ ì‚¬ìš©ë˜ê³ , noisy utteranceê°€ targetì˜ auxiliary networkì˜ ë™ì¼í•œ activationì„ ê°€ì§€ë„ë¡ í›ˆë ¨ë¨
- ê·¸ëŸ¬ë‚˜, (1) ì‹ì˜ gradientëŠ” ì˜¤ì§ noisy utteranceì˜ enhancementì—ë§Œ ê´€ì‹¬ìˆìŒ â‡’ ì´ëŠ” clean utteranceì— ì ì¬ì ìœ¼ë¡œ ì•ˆì¢‹ì€ ì˜í–¥ì„ ë¼ì¹  ìˆ˜ ìˆìŒ
- ê·¸ë˜ì„œ, ì—°êµ¬ì§„ì€ (c)ì™€ (d)ì˜ í›ˆë ¨ ë°©ì‹ì„ ì œì•ˆ
- maskê°€ ìƒì„±ë˜ì–´ clean utteranceì— ì ìš©ë˜ê³ , enhancement moduleì€ ê° noisyì™€ clean utteranceì˜ ë‘ gradientë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ ìµœì í™”ë¨
- enhancement ëª¨ë“ˆì˜ ëª©í‘œëŠ” noisy utteranceì™€ clean utteranceê°€ ì¼ê´€ëœ representationì„ ê°–ë„ë¡ í•˜ëŠ” ê²ƒ
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/854f206c-7255-43f3-8752-bf0e7f2dc9a9/Untitled.png)
    
    - $X, S$ = noisy/clean utterance     $j$ = 6
- ì¢‹ì€ speaker representation networkëŠ” ê°™ì€ í™”ìë¡œë¶€í„°ì˜ utterance distanceëŠ” ìµœì†Œí™”í•˜ê³ , ë‹¤ë¥¸ í™”ìê°„ distanceëŠ” ìµœëŒ€í™”í•˜ëŠ” ê²ƒ
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/f3bb2563-bda6-4961-a68e-f31bd61557e3/Untitled.png)
    
    - ì´ cross entropy lossëŠ” í”í•œ loss ì‹ì„, ì„œë¡œ ë‹¤ë¥¸ í™”ì ê°„ ê±°ë¦¬ë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•œ
    - $\phi(\theta(S))$ = speaker representation moduleì—ì„œ ì˜ˆì¸¡ëœ speaker label    $y$ = ground truth speaker label
    - ì§ê´€ì ìœ¼ë¡œ, (6)ì—ì„œì˜ softmaxëŠ” intra-class discriminationì„ í¬ê²Œ í•˜ì§€ë§Œ inter-class distanceì—ëŠ” ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠìŒ
    - ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, entropy lossì™€ perceptual lossë¥¼ ë™ì‹œì— ì‚¬ìš©í•¨
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/cb2384fe-4dd0-4320-bb10-91d51f7e6ad9/Untitled.png)
    
    - perceptual lossëŠ” inter-class distanceì˜ criterionìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
    - $\lambda$ = intra-classì™€ inter-class distanceì˜ ê· í˜•ì„ ë§ì¶°ì£¼ëŠ” ìƒìˆ˜, ì—¬ê¸°ì„œëŠ” 0.5ë¡œ ì„¤ì •
    
    **4.3. Training**
    
    - Pre-training + finetune
    - pre-training
        - (Pre-training 1) speaker representation module - batch size 512, SGD, lr=0.2 and decreases by half when the loss decrease ratio in each epoch is less than 0.01
        - early stopping (lrì´ ì—°ì†ìœ¼ë¡œ 2ë²ˆ ì¤„ì–´ë“¤ë©´)
        - vox_clean_aug dataset ì‚¬ìš©, ì´ ë‹¨ê³„ì—ì„œëŠ” speaker representation ëª¨ë“ˆì— cross entropy lossë§Œ ì‚¬ìš©ë¨
        - ì´ë ‡ê²Œ í›ˆë ¨ëœ speaker representation moduleì€ ê³ ì •ë˜ê³  auxiliary networkë¡œ ì‚¬ìš©ë¨
        - (Pre-training 2) enhancement moduleì€ preceptual lossì™€ cross entropy lossë¥¼ ë™ì‹œì— ì‚¬ìš©í•´ ìµœì í™”ë¨
        - 3 ìŒì˜ cleanê³¼ noisy utteranceê°€ ì‚¬ìš©ë¨ (vox_noisy/vox_clean, vox_noisy_aug/vox_clean,  vox_clean_aug/vox_noisy_aug)
        - batch size 128 (noisy 64 +clean 64), Adadelta optimizer, initial lr 0.3, decreases and early stoppingì€ pre-training 1ê³¼ ë™ì¼
    - finetune
        - speech enhancement moduleê³¼ speaker representation moduleì´ í•¨ê»˜ finetuneë¨ â‡’ potential misamatchë¥¼ ì¤„ì´ê¸° ìœ„í•´
        - initial rate=0.0001 ì´ì™¸ì—ëŠ” pre-training 2ì™€ ëª¨ë“  ê³¼ì •ì´ ë˜‘ê°™ìŒ
    

### ++ **GE2E**

- LSTM layer + linear layer
- $e_ji$ì™€ ê° ì¤‘ì‹¬ì  $c_k$ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©)
- $S_{ji,k}$ = ìŠ¤í”¼ì»¤ jì˜ ië²ˆì§¸ ë°œí™” ì„ë² ë”© $e_{ji}$ì™€ ëª¨ë“  ìŠ¤í”¼ì»¤ ì¤‘ì‹¬ì  $c_k$ê°„ì˜ ìŠ¤ì¼€ì¼ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/13d9be09-c03c-4519-ade0-7c372fd1cb54/7ed263dc-6126-4898-8c45-242bc9b1c4f9/Untitled.png)

- ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ìœ ì‚¬ë„ í–‰ë ¬ì— ì ìš©ë˜ì–´ ê° ì„ë² ë”©ì´ í•´ë‹¹ ìŠ¤í”¼ì»¤ì˜ ì¤‘ì‹¬ì ì— ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë„ë¡ í•¨, ê° ì„ë² ë”© ë²¡í„°ê°€ ìì‹ ì˜ ìŠ¤í”¼ì»¤ ì¤‘ì‹¬ì ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì†ì‹¤ ê°ì†Œ (k=jì¼ ê²½ìš° ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¶œë ¥ì´ 1ì— ê°€ê¹Œì›Œì§€ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡)

