---
title: SASN: End-to-End Trainable Self-Attentive Shallow Network for Text-Independent Speaker Verification
date: 2024-01-12 17:11:00 +0300
description: spk emb + attention
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [speaker embedding] # add tag
use_math: true
categories:
  - Paper Reading
sidebar:
    nav: "blog"
---
## Abstract

GE2E는 SV 분야에서 널리 쓰이고 있지만, 해당 아키텍처에서 LSTM이 쓰이는데 이 LSTM에는 두 가지 한계가 존재한다. 바로 

1) GE2E 임베딩이 긴 input에 대해서는 gradient vanishing 문제를 겪는다는 점

2) 발화들이 고정된 벡터에 적절하게 represent되지 않는다는 점

따라서 이 논문에서는 해당 문제들을 극복하기 위해 SASN, end-to-end trainable self-attentive shallow network를 제안한다. 이는 TDNN 네트워크와 self-attentive x-vector system에 기반한 self-attentive pooling 매커니즘을 통합한 형태이다. 

저자들은 GE2E보다 SASN이 훨씬 더 효율적이고, speaker verification에 대한 정확도도 더 높다고 말한다. VCTK dataset에 대해 GE2E의 절반 정도 되는 사이즈임에도 불구하고 EER, DCF, AUC 각각에서 GE2E 보다 약 63%, 67%, 85%의 향상된 성능을 보였다고 한다. 특히, input이 길어지면 DCF 스코어가 17배 이상 증가했다고 한다. 

## Introduction

SV는 음성의 특징에 기반에 화자를 확인하는 것을 말한다. SV에는 두 가지 접근법이 존재하는데, text-dependent(TD)와 text-independent(TI) 크게 두 가지가 있다. 

TD-SV에서는, 모든 단계들이 이미 정해진 text에 따라 진행된다. 반면, TI-SV는 화자가 말해야하는 정해진 문장이나 제한이 없다. 이는 TD-SV를 TI-SV보다 더 까다롭게 만드는 이유이다. 일반적으로, SV 과정은 세 단계를 거친다: development, enrollment, evaluation이다. development 단계에서는, 화자의 표현에 대한 백그라운드 모델이 생성된다. enrollment 단계에서는 이 백그라운드 모델을 새로운 유저에 대한 화자 모델을 만들기 위해 사용한다. 마지막으로 evaluation 단계에서는 이전에 생성된 화자 모델로 표현을 비교하여 해당 발화가 정말 그 화자의 것이 맞는지 확인한다.

SV 적용에 대한 강력한 접근법 중 하나는 i-vector 기반 시스템이었다. 이는 speaker representation을 생성하는 encoder 모듈과 speaker embedding vectors를 비교하는 discriminative 모듈의 결합에 기반한다. i-vector 시스템은 인코더와 discriminative 모듈의 독립적인 훈련을 필요로 하며, 시스템의 복잡성의 증가가 불가피하다.

최근 연구들은 SV 시스템을 end-to-end 방법으로 훈련 가능하도록 시도해왔다. 이러한 접근에서는, 스피커 임베딩과 discrimination 파트가 하나의 네트워크로 통합되었고, 이는 최소한의 configuration과 assumption을 필요로 한다. 이러한 단일 네트워크 접근 방식은 각 독립 모듈의 오류 누적을 방지해 테스트 정확도를 향상시킬 수 있다. 특히, GE2E 모델은 큰 적용성을 갖고 잇다. 예를 들어, GE2E는 TD-SV, TI-SV 태스크 전부에 좋은 성능을 보였고, 화자 분할, 다중 화자 TTS, speech-to-speech translation과 voice filter 과제에서도 좋은 성능을 냈다. 

그러나, GE2E에는 한계가 존재한다. 우선 첫째로 LSTM에 기반한 임베딩 구조를 가지고 있는데, 이는 긴 입력값에 대한 처리를 어렵게 만든다. 이는 긴 input sequence를 처리할 때 흔히 발생하는 문제인 vanishing gradient에서 기인된다. 입력값의 길이에 따른 SV 정확도에 대해 분석한 논문에 따르면, 더 긴 문장이 입력값으로 사용될 수록 모델의 SV 정확도가 향상다는 결과가 있다. 

둘째로, GE2E의 인코더는 모든 프레임을 동등하게 취급하기 때문에, 화자 확인에 있어 각 프레임이 전달하는 각기 다른 정보의 양과 형태를 모으는 데 어려움이 있다. 예를 들어, 몇몇 프레임은 화자를 확인하는 데 valuable한 정보들을 담고 있을 수 있는 반면, 몇몇 프레임에는 정보가 거의 없을 수 있다. 이는 모델의 computational power가 별로 중요하지 않은 프레임의 학습에도 사용되고 있음을 뜻하며, computational 효율성과 정확성에 좋지 않은 영향을 끼칠 수 있다.

이러한 문제점들을 해결하기 위해, 해당 논문에서는 SASN(Self-attentive Shallow Network)을 제안한다. SASN은 다음과 같은 장점을 가진다.

첫째, SASN의 임베딩 모듈은 TDNN과 self-attention 매커니즘의 결합에 기반한다. TDNN의 sub-sampling 메서드는  computational load를 줄이는 동시에 wide temporal context의 discriminative patterns를 확인할 수 있도록 한다. 더불어, 병렬 sampling 작업은 RNN의 autoregressively calculated되는 output보다 훈련이 빠르다. 그리고 TDNN의 pooling layer는 self-attention 매커니즘에 기반하는데, 이는 모델이 집중해야할 중요한 time window를 확인할 수 있도록 하며, 임베딩을 효율적으로 할 수 있도록 한다.

둘째, SASN의 단일 end-to-end 아키텍처는 서로 다른 독립 구성 요소간의 오류 누적을 방지할 뿐만 아니라, 하이퍼 파라미터 설정 구성을 더 쉽게 할 수 있다.

마지막으로, SASN은 batch-wise loss를 사용해 효율성과 성능을 입증했다. 이는 모델이 학습 프로세스의 각 단계에서 여러 데이터간의 다양한 관계를 고려할 수 있도록 해주기 때문이다. 결과적으로, VCTK 데이터셋에 대해 EER, ECF, AUC 각각에서 GE2E보다 약 63%, 67%, 85%의 향상을 보여주었을 뿐만 아니라 모델 사이즈도 GE2E에 비해 58%정도밖에 되지 않는다. 또한, 더 긴 입력값에 대한 성능 향상이 인상깊었는데, 이는 self-attentive components 덕분이다. 인풋 길이가 180에서 300 프레임까지 증가했을 때, DCF에 대한 성능 향상이 GE2E의 약 17배에 달했다.

## Related works

### 2.1. Self-Attentive Speaker Embedding SV

self-attentive x-vector 시스템의 과정은 두 단계를 포함한다: 임베딩과 스코어링이다. 임베딩 프로세스에서는, speaker disciminative TDNN이 x-vector라고 불리는 speaker-specific representation을 생성하기 위해 훈련된다. 스코어링 프로세스에서는, PLDA(Probabilistic linear discriminant analysis) 백엔드가 x-vector 쌍을 비교하기 위해 사용된다. 임베딩 단계에서는, 이전의 TDNN 레이어에서 나온  발화 임베딩의 각 프레임이 맥락 정보를 나타낸다. 그러고 나서 self-attentive pooling layer가 TDNN의 최종 레이어 출력에서 frame-level 벡터들에 대해 평균과 표준편차를 도출하여 집계한다. 이 접근방식의 어려운 점은 각 임베딩과 스코어링 모듈이 독립적으로 훈련되어야한다는 것이다. 이는 적절한 train/test 환경과 데이터셋의 설정을 어렵게 만든다. 더불어, 독립적으로 훈련된 모듈들은 잠재적으로 오류를 축적할 수 있다.

이러한 문제를 해결하기 위해, 임베딩 파트와 스코어링 파트를 단일 네트워크로 통합하여 end-to-end 방식으로 훈련될 수 있도록 했다. 이러한 결과로, SASN의 전체 시스템 파라미터는 1940K이며, 이는 x-vector 모델의 임베딩 파트 크기의 반도 되지 않는다. (4240K)

### 2.2 Generalized End-to-End loss For Speaker Verification

GE2E는 LSTM을 이용해 발화를 임베딩 벡터에 매핑한다. 그리고 벡터간 유사성을 계산한다. 각 파트는 독립적인 역할이 있고, 동시에 최적화된다. GE2E의 loss 함수는 배치 훈련을 위해 디자인 되었다. 따라서 많은 수의 발화에 대해 한 번에 학습할 수 있다. 

Subsequently,
the similarity matrix was calculated based on the cosine similarity between each utterance and each speaker. The cost function is optimized in a way that the similarity between each utterance and the corresponding speaker is maximized.
We introduced TDNN and the attention pooling layer to overcome the limitation of LSTM-based embedding methods, as
mentioned in Section 1. Owing to TDNN, along with the attentional pooling layer, the performance of the model is significantly increased without any temporal information loss. The results of the performance analysis are summarized in Table 2 and
Figure 2.

## 3. Proposed framework

SASN은 두 모듈로 구성된다. 인코더는 각 utterance feature에서 representation vector를 생성한다. 스코어링 모듈은 임베딩 벡터를 비교하고 스코어링한다. 이 두 파트들이 동시에 훈련된다.

### 3.1. Context-aware, self-attentive encoder module

인코더의 목적은 utterance embedding을 생성하는것이다. 이는 x-vector baseline system에서 소개된 인코더 아키텍처를 기반으로 한다. [[8,9]](https://www.notion.so/End-to-End-Trainable-Self-Attentive-Shallow-Network-for-Text-Independent-Speaker-Verification-6e8ba96ccb6f48238434d3a0bb8a2e92?pvs=21) frame-level input은, 40차원의 feature vector이며 TDNN을 통과한다. TDNN은 layer 1부터 layer 3까지로 이루어져있다. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/befaeb0b-5ff9-4148-ab68-2f9409a6cf13/Untitled.png)

l2 에서의 t는 l1의 범위를 모두 포함하기 때문에, l2의 컨텍스트 수는 l1의 범위 5개 + 앞뒤로 두 개씩 해서 9개의 컨텍스트를 가지게 된다. 또한 l3에서의 t도 마찬가지로 l2의 범위를 모두 포함하기 때문에 l2의 컨텍스트 수 9개에 앞 뒤로 3개씩 6개를 더해 15개의 컨텍스트를 가진다. TDNN의 출력벡터는 어텐션 레이어를 통과하게 된다. 

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/c5122a34-e0ea-4e49-a2f7-079b27e47585/da14ac77-75b1-4ac3-851e-6af57c576c75/Untitled.png)

어텐션 레이어의 구조는 *Figure 1*에 나타내었다. T 길이의 발화 세그먼트가 $H = {h_1, h_2, ..., h_T}$ 로 나타내어지며 어텐션 헤드의 개수를 $d_r$이라고 하자. 첫 번째 어텐션 레이어에서, $512\times T$ 크기의 $H$가 $512 \times d_r$길이로 압축된다.

$$
A_{single} = softmax(f(H^TW_1)W_2),     ~~(1)
$$

$$
E_{single} = \Vert HA_{single}\Vert_{2'} ~~~(2)
$$

각 term의 크기는 다음과 같다:

$W1: 512\times d_a;~W2:d_a\times d_r;~A_{single}:T\times d_r;$

$f(\cdot)$는 ReLUs 활성화 함수이다. $softmax(\cdot)$는 컬럼 wise로 계산된다. 첫 번째 어텐션 레이어는 각 $h_t$의 가중치를 차등적으로 곱한다. 이는 speaker identification에서 필요한 정보의 양에 따라 진행된다. 결론적으로, 각 컬럼의 $E_{single}$은 다른 발화 특성을 인코딩 한다:

$$
Col_c = a_1h_1 + a_2h_2+...+a_rh_r, ~~~ (3)\\(c:column~index, \sum_{i=1}a_i=1)
$$

더불어, stacked 셀프 어텐션이 성능을 올린다는 사실을 발견했고, 이전의 시연과 일치함

SASN에서는, 추가적인 어텐션 작업이 다음과 같이 진행됨:

$$
A_{double} = softmax(g(E_{single}^TW_3)),~~~(4)
$$

$$
E_{double} = \Vert E_{single} * g(A^T_{double})\Vert_2)
$$

$W3$의 크기는 $512 \times 1$이며, 그래서 $A_{double}^T$의 사이즈는 $1 \times d_r$이다. $g(\cdot)$는 행 축을 기준으로 512번 반복적으로 적용되는 타일 연산이다. 

optional second 어텐션 레이어는 $E_{single}$에 대해 column-wise 연산을 수행한다. 어텐션의 출력 결과 $E(E_{single} ~or ~E_{double})$은 풀링 레이어의 입력으로 들어간다. 최종적으로, 1차원의 스피커 임베딩 벡터 $e$가 시간축에서 $E$의 평균과 표준편차와 연결된다.

### 3.2. Scoring module

스코어링 모듈은 화자 임베딩과 발화 임베딩 사이의 유사성을 정량화하기 위한 모듈이다. 인풋 배치는 인코더로부터 나온 여러 발화 임베딩 벡터로 이루어져있다. 인풋 배치는 $N\times M$ 크기의 행렬이며, 각 $N$ 명의 다른 화자가 $M$ 개의 발화를 가진다. 각 임베딩 벡터는 $j$ 번째 화자의 $i$ 번째 발화  $x_{ji}$로 주어진다. $k$ 번째 화자의 임베딩 벡터의 중심점 $c_k$는 각 발화 임베딩 $[e_{k1}, e_{k2}, ..., e_{kM}]$의 평균값으로부터 얻어진다. 하지만, $k$ 번째 화자의 중심점을 $k$ 번째 화자의 임베딩 $e_{ki}$와 비교할 때, $k$번째 화자의 중심점은 안정적인 훈련과 모호한 해를 피하기 위해 $e_{ki}$를 제외한 평균으로 대체된다. (GE2E에서는 포함하여 계산한다.) 따라서 유사도 행렬 $S$는 모든 임베딩 벡터 $e_{ji}$와 중심점 $c_k$의 쌍 사이의 유사도로 다음과 같이 정량화된다.

$$
S_{ji, k} = w\cdot cos(e_{ji},c_l) +b~(w>0), ~~~(6)
$$

loss 함수는 아래 (7)에 나타난다. 이는 발화 임베딩과 해당 화자 임베딩의 유사도를 1로 만들며, 다른 화자 임베딩과의 유사도는 -1로 만든다. 페널티 term $P$는 행렬 $A_{single}$의 redundancy 오류를 줄이기 위해 설계되었다. 

$$
L(e_{ji}) = -S_{ji,j} + log \sum_{K=1}^Nexp(S_{ji,k})+\alpha P,~~~(7)
$$

SV 모델에서, loss 함수 (7)은 인코더와 스코어링 모듈을 동시에 훈련할 때 사용된다.

## 4. Experiments

### 4.1. Simulation setting

시뮬리에이션은 베이스라인 시스템의 대부분을 따라갔다. feature extraxtion 프로세스에서, raw 오디오 시그널은 10ms의 스텝사이즈로 25ms 프레임의 벡터로 변형된다. 이후 40 차원의 log-mel-filter bank energies를 각 프레임에 대해 계산한다. 인코더 모듈에서는, input 프레임의 길이는 180 이상이다. (180, 300, 600) 네트워크는 SGD를 이용해 학습되고, learning rate는 0.01을 사용한다. clipping gradient descent는 적용하지 않았다. 어텐션 헤드 수인 $d_r$은 5, 10, 20을 사용했다. 

### 4.2 Datasets

첫째로, VCTK 데이터셋은 109명 화자의 음성이 녹음된 영어 데이터셋이다. 각 화자는 각기 다른 뉴스로부터 약 400개의 문장을 읽는다. 이는 총 13100개의 발화를 포함하며, 44시간짜리 데이터이다. 둘째로, TIMIT 데이터셋을 활용하는데, 이는 5.4시간의 약 5300개의 발화가 존재한다. 각 630명 화자의 발화 10개씩 존재하며, 미국의 8개 주요 방언 지역의 화자로 이루어져있다. 세 번째 데이터셋은 20SER이다. 한국어 데이터셋이며, 콘덴서 마이크를 사용해 녹음해 약간의 artifact 노이즈가 존재한다. 4.7시간짜리 데이터로, 20명의 화자의 드라마 스크립트를 읽는 5593개 발화가 녹음되어있다. 이 데이터셋의 화자 당 문장 개수는 9개부터 400개로 다양할뿐만 아니라, 문장 자체의 길이도 0.89초부터 17.02초까지로 다양하다. 평균 길이는 3.03초이다.

### 4.3. Evaluation

3개 시나리오를 활용해 실험했다. 첫 번째 케이스에서는 VCTK 데이터셋이 이용되었다. 트레이닝에는 90명의 화자, 테스트에는 19명의 화자 데이터를 포함한다. 

