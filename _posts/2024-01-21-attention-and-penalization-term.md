---
layout: post
title: Attention 매커니즘과 penalization term
date: 2024-01-21 21:05:00 +0900
description: About redundancy error and penalization term in attention mechanism
img:  # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [paper] # add tag
use_math: true
---

## **들어가면서**

&#160;&#160;&#160;&#160; Attention 매커니즘 사용 시 중복성 문제 해결을 위한 Penalty term에 대해 공부할 겸 정리해보았습니다.               

​              

## Attention Mechanism

​       Attention이란 말 그대로 어디에 더 집중해서 볼 것인가를 가능하게 해 주는 기법이다. 인간의 주의력을 모방해서, 모델이 전체 데이터에서 특정 부분에 더 많은 '주의'를 기울이게 한다. 보통 Query, Key, Value의 세 가지 주요 요소를 사용한다. 쿼리는 현재 모델이 처리하고 있는 데이터 포인트를 나타낸다. 문장을 번역하는 경우 현재 번역하고 있는 단어 또는 구절이 쿼리가 될 수 있다. 즉, 모델이 '주의를 기울여야 할 대상'을 나타낸다. 키는 쿼리가 비교되어야하는 데이터 포인트들을 나타낸다. 전체 문장에서 각 단어 또는 구절이 키가 될 수 있다. 밸류는 키와 연관된 데이터 포인트를 나타낸다. 각 키에는 해당하는 밸류가 있으며, 키와 쿼리의 관련성에 따라 해당 밸류가 얼마나 중요한지 결정된다. 밸류는 최종적으로, 모델이 어떤 정보에 주의를 기울일지를 결정하는 데 사용된다. 즉, 쿼리는 '무엇을' 찾아야하는가, 키는 '어디서' 찾아야하는가, 밸류는 찾았을 때 '어떤 정보'를 얻을 수 있는가를 나타낸다.

쿼리와 각 키 간의 유사도를 계산하여 점수를 부여하고, 이 점수는 쿼리가 해당 키(및 해당 밸류)와 얼마나 관련이 있는지를 나타낸다. 계산된 점수에 소프트맥스 함수를 적용하여, 점수들을 확률 분포로 변환한다. 이렇게 하면 각 키-밸류 쌍에 대한 상대적인 중요도가 결정된다. 

각 밸류에 해당하는 점수를 곱하고, 이들의 가중합을 구한다. 이 결과가 바로 쿼리에 대한 '집중된' 출력으로, 입력 데이터의 중요한 부분에 대한 정보를 담고 있는 것이다.        

   

   

​    

## Redundancy Error

그러나 어텐션 매커니즘을 사용할 때, Redundancy Error가 발생할 수 있다. Redundancy Error는 주로 입력 데이터의 중복된 부분에 대해 모델이 과도하게 주의를 기울이는 문제를 말한다. 이러한 문제는 모델이 입력 데이터의 다양한 부분에 균등하게 주의를 분배하지 못하고, 특정 부분에 지나치게 집중하게 될 때 발생한다. 예를 들어 멀티 헤드 어텐션을 이용해 입력 데이터에 대해 여러 개의 어텐션 계산이 동시에 이루어질 때, 이상적인 상황은 각 헤드가 입력 데이터의 다른 측면에 주목하는 것이다. 그러나 만일 모든 헤드가 유사한 패턴이나 정보에 주목하게 되면 이는 redundancy error로 이어질 수 있다. 즉, 모델이 입력 데이터의 다양성을 충분히 포착하지 못하는 것이다. 

​                 

   

   

## Penalization Term

그렇다면 Redundancy error를 해결하기 위해서는 어떻게 해야할까? 

나는 [논문]['https://arxiv.org/pdf/1703.03130.pdf'] 에서 소개한 것처럼 로스 함수에 Penalty Term을 추가하는 방법을 사용했다. 해당 논문에서, 다양성을 평가하기 위해서는 Kullback Leibler divergence를 사용하는 것이 가장 이상적인 방법이지만 자기들의 케이스에서는 안정적이지 않았다고 한다. 이는 일반적으로 KL divergence 하나를 최소화하는 대신, 여러 KL divergence를 최대화하고 있기 때문에 이러한 상황이 발생한다고 짐작한다고.. 매트릭스에 0값이 너무 많기 때문에 훈련이 unstable 해진다고 한다. 연구진이 원하는 것은 모델이 의미론적 측면에서 각각의 개별 요소에 집중하도록 하는 것인데, KL divergence는 보통 두 확률 분포간의 차이를 최소화 하는 데 사용되지만 이 경우에는 개별 요소의 집중도를 높이는 데는 적합하지 않다고 판단한 것. 따라서 이러한 문제를 해결하기 위해 새로운 penalization term을 도입했다. 
$$
\begin{align} P = \Vert (AA^T-I\Vert_F^2\end{align}
$$
KL divergence와 비교해, 이 term은 1/3 정도의 계산량만을 필요로한다. A 행렬과 그 전치행렬의 내적에서 단위행렬을 뺀 값을 중복성의 척도로 이용한다. L2 regularization과 유사하게, 이 패널티 항에 계수를 곱해 loss에 더해 함께 최소화하는 방식으로 사용한다. 

Attention의 softmax 때문에, A의 모든 요소가 0과 1 사이의 값이며 각 행의 전체 합은 1이 된다. 이렇게 변환된 벡터는 각 행이 하나의 확률 분포로 해석될 수 있다. 즉, 이산확률 분포로 간주될 수 있으며, 이 분포 내의 각 값은 입력 데이터의 각 부분에 대한 모델의 '주의'를 나타내는 확률 분포로 해석될 수 있다. $AA^T$ 행렬에서 비대각 요소 $a_{ij}$ 는 두 확률 분포 $a_i$ 와 $a_j$ 간의 상호작용을 나타낸다. 따라서 $a_i$ 와 $a_j$ 의 곱의 합은 두 분포가 얼마나 비슷한지, 또는 서로 상호작용하는지를 나타내는 척도가 된다. 이는 두 벡터의 내적과 유사한 연산이다. 

$a_{ik}$ 와 $a_{jk}$ 가 각각 벡터 $a_i$ 와 $a_j$ 의 $k$ 번째 요소라고 하자. 이 요소들은 각각의 벡터 내에서 특정 위치에 해당하는 확률질량을 나타낸다. 만약 두 확률 분포 $a_i$ 와 $a_j$ 사이에 중첩이 없다면, 두 분포가 완전히 독립적이라면, 해당하는 $a_{ij}$ 는 0이 된다. 반대로, 두 분포가 완전히 동일하고 특정 단어에만 집중한다면, $a_{ij}$ 의 값은 최대 1이 된다. 

예를 들어, $a_i$ 가 [0.1, 0.9, 0, 0]이라고 하고 $a_j$ 가 [0, 0, 0.5, 0.5]라고 하자. 그렇다면 두 벡터 모두 각기 다른 위치에서 집중하고 있으므로 내적 값은 0이 나올 것이다. 만일 두 벡터가 4군데 모두에서 값을 가지고 있다면 1에 가까운 값이 나올 것이다. 따라서 중복성이 높을수록 $a_{ij}$의 값은 0에서 멀어지게 된다.

$AA^T$ 행렬에서 I 행렬을 빼면 대각선 요소(자기 자신에 대한 attention)가 0이 되고, 비대각선 요소는 그대로 유지된다. 이는 모델이 각 어텐션 벡터를 통해 다른 요소들에 집중하도록 유도한다. 각 벡터가 자기 자신이 아닌 다른 요소들에 대해 어떻게 상호작용하는지에 초점을 맞추게 하는 것이다.  즉, 각 벡터가 서로 다른 정보에 집중하도록 유도하여 중복을 줄이는 효과를 가진다.





## 마치면서

확실히 Attention이나 Penalty term이나, 이론적으로 알고 있는 것과 직접 적용해보는 것은 정말 다른 것 같다. 직접 적용해보면서 print 해보면 신기하기도하고.. 참 많이 배우는 중이다. 내일 모델 경과가 기대된다..!🙏



​       

​       

​      







